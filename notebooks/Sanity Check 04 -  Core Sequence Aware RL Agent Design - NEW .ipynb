{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "#  Core Sequence-Aware Agent Design v2\n",
    "\n",
    "This experiment explores a transformer-based recurrent PPO agent for financial trading. The environment is sequence-aware and includes both regime-based augmentation and per-episode reward normalization. The agent is evaluated across top 2 stocks in each sector using structured episode sequences to assess learning generalization.\n",
    "\n",
    "---\n",
    "\n",
    "##  Experiment Configuration\n",
    "\n",
    "| Parameter               | Value                         |\n",
    "|-------------------------|-------------------------------|\n",
    "| Agent                   | Recurrent PPO + Transformer   |\n",
    "| Env Wrapper             | RegimeAugmentingWrapper + PerEpisodeRewardNormalizer |\n",
    "| Episode Length          | 100                           |\n",
    "| Episodes                | 20                            |\n",
    "| Eval Episodes           | 3 per iteration               |\n",
    "| Steps per Update        | 800                           |\n",
    "| Batch Size              | 100                           |\n",
    "| Total Timesteps         | 20,000                        |\n",
    "| Learning Rate           | 0.0003                        |\n",
    "| Entropy Coefficient     | 0.005                         |\n",
    "| Value Function Coeff    | 0.5                           |\n",
    "| Max Gradient Norm       | 0.5                           |\n",
    "| Normalize Advantage     | True                          |\n",
    "| Optimizer               | Adam                          |\n",
    "| Transformer d_model     | 64                            |\n",
    "| Heads                  | 4                             |\n",
    "| Layers                 | 2                             |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- **Train Set:** 2023-01-01 → 2023-07-01\n",
    "- **Test Set:** 2023-07-01 → 2024-01-01\n",
    "- **Assets:** Top 2 stocks by sector\n",
    "- **Sequence Split:** 80% train / 20% eval sequences\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent Architecture\n",
    "\n",
    "- **Feature Extractor:** Transformer encoder with causal mask and learnable positional encoding.\n",
    "- **Policy Class:** Custom `TransformerPolicy` extending `RecurrentActorCriticPolicy`.\n",
    "- **Reward Normalization:** Online normalization within episodes.\n",
    "- **Regime Augmentation:** Appends one-hot encoded market regime to each timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Summary (Selected Stats)\n",
    "\n",
    "| Timesteps | Ep Rew Mean | Explained Variance | Value Loss | Policy Grad Loss |\n",
    "|-----------|-------------|--------------------|------------|------------------|\n",
    "|  8000     | 3.21        | 0.15               | 4.68       | -0.00277         |\n",
    "| 14400     | 2.90        | 0.645              | 1.71       |  0.00163         |\n",
    "| 20000     | **3.79**    | **0.751**          | 1.59       | -0.00012         |\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Snapshots\n",
    "\n",
    "| Timestep | Mean Reward | Std Dev | Eval Length |\n",
    "|----------|-------------|---------|-------------|\n",
    "| 5000     | -8.17       | ±8.72   | 102         |\n",
    "| 10000    | -4.43       | ±7.59   | 102         |\n",
    "| 15000    | -6.17       | ±4.92   | 102         |\n",
    "| 20000    | **0.24**    | ±2.86   | 102         |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "### Paired t-test and Mann-Whitney U-test\n",
    "\n",
    "| Metric       | t-test p-value | Mann-Whitney p-value |\n",
    "|--------------|----------------|-----------------------|\n",
    "| `total_reward` | 0.0300         | 0.0075                |\n",
    "| `calmar`       | 0.0132         | 0.0075                |\n",
    "\n",
    "✅ Both `total_reward` and `calmar ratio` show **statistically significant** improvements compared to the baseline.  \n",
    "Especially, Calmar implies **more stable and risk-adjusted returns**.\n",
    "\n",
    "> Note: `sharpe`, `sortino`, and `final_wealth` were skipped due to empty values in the evaluation logs. Ensure metrics are logged and valid across all test episodes to include them.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fix missing metrics** (`sharpe`, `sortino`, etc.) in the logging pipeline.\n",
    "2. **Plot distribution comparisons** (boxplots, histograms) for rewards and risk-adjusted returns.\n",
    "3. **Run ablation**:\n",
    "   - Without regime augmentation\n",
    "   - Without reward normalization\n",
    "   - With simpler agents (e.g., MLP or LSTM)\n",
    "4. **Test in unseen market conditions** or during volatility spikes to check robustness.\n",
    "\n",
    "---\n",
    "\n",
    "_Logged using `ExperimentTracker` — Run Hash: `${experiment_tracker.run_hash}`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510f9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "from src.env.step_rewards import reward_sharpe,reward_sortino,reward_drawdown,reward_alpha,reward_cumulative,reward_calmar,reward_hybrid\n",
    "\n",
    "class BaseSequenceAwareTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Flexible RL Trading Env with windowed sequence obs (Transformer/LSTM/MLP-ready).\n",
    "    - Set `return_sequences=True` for (window_length, obs_dim) obs (for transformers).\n",
    "    - Set `return_sequences=False` for flat obs (classic RL, SB3 LSTM/MLP).\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, df, feature_cols=None, reward_fn=None, internal_features=None,\n",
    "        episode_length=100, transaction_cost=0.0001, seed=314, window_length=10, return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols or []\n",
    "        self.internal_features = internal_features or [\n",
    "            \"position\", \"holding_period\", \"cumulative_reward\", \"pct_time\",\n",
    "            \"drawdown\", \"rel_perf\", \"unrealized_pnl\", \"entry_price\", \"time_in_position\"\n",
    "        ]\n",
    "        self.obs_dim = len(self.feature_cols) + len(self.internal_features)\n",
    "        self.episode_length = episode_length +2\n",
    "        self.window_length = max(1, window_length)\n",
    "        self.return_sequences = return_sequences  # True: (window, obs_dim), False: flat\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.seed = seed\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        counts = df['symbol'].value_counts()\n",
    "        eligible = counts[counts >= episode_length].index\n",
    "        self.stocks = df[df['symbol'].isin(eligible)]['symbol'].unique()\n",
    "        self.episode_df = df.copy()\n",
    "        self.reward_fn = reward_fn or self.default_reward_fn\n",
    "\n",
    "        # Set observation space\n",
    "        if self.return_sequences:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length, self.obs_dim), dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length * self.obs_dim,), dtype=np.float32\n",
    "            )\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "    \n",
    "    def get_current_regime(self):\n",
    "        \"\"\"\n",
    "        Detects current regime based on recent return volatility and trend.\n",
    "        Returns:\n",
    "            0 = Bull, 1 = Bear, 2 = Sideways\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_length:\n",
    "            return 2  # Not enough data, assume sideways\n",
    "\n",
    "        # Use recent price changes to detect regime\n",
    "        returns = self.episode_df['return_1d'].iloc[self.current_step - self.window_length:self.current_step].values\n",
    "        mean_return = returns.mean()\n",
    "        std_return = returns.std()\n",
    "\n",
    "        # Thresholds can be tuned\n",
    "        if mean_return > 0.001 and std_return < 0.01:\n",
    "            return 0  # Bull\n",
    "        elif mean_return < -0.001 and std_return < 0.01:\n",
    "            return 1  # Bear\n",
    "        else:\n",
    "            return 2  # Sideways\n",
    "        \n",
    "    def default_reward_fn(self, position, price_change, **kwargs):\n",
    "        return position * price_change\n",
    "\n",
    "    def set_episode_sequence(self, sequence):\n",
    "        self.episode_sequence = sequence\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def generate_episode_sequences_v1(self, train_steps=10000):\n",
    "        dataset_length = len(self.df)\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        ticker = self.df['symbol'].unique()[0]\n",
    "        min_start = 0\n",
    "        max_start = dataset_length - self.episode_length - 2\n",
    "        for i in range(episodes):\n",
    "            episode_sequences.append((ticker, np.random.randint(0, max_start)))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def generate_episode_sequences(self, train_steps=10000):\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        for _ in range(episodes):\n",
    "            ticker = np.random.choice(self.stocks)\n",
    "            stock_df = self.df[self.df['symbol'] == ticker].reset_index(drop=True)\n",
    "            max_start = len(stock_df) - self.episode_length - 2\n",
    "            if max_start <= 0:\n",
    "                continue  # skip if not enough data\n",
    "            start = np.random.randint(0, max_start)\n",
    "            episode_sequences.append((ticker, start))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def reset(self, seed=None, options=None, start_index=None):\n",
    "        self.entry_step = None\n",
    "        self.unrealized_pnl = 0\n",
    "        self.relative_perf = 0\n",
    "        self.drawdown = 0\n",
    "        self.time_in_position = 0\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "            \n",
    "        symbol, start_idx = self.episode_sequence[self.episode_counter]\n",
    "        #print(symbol,start_idx,self.df['symbol'].unique())\n",
    "        symbol_df = self.df[self.df['symbol'] == symbol].reset_index(drop=True)\n",
    "        #print(len(symbol_df))\n",
    "        \n",
    "        if start_idx + self.episode_length > len(symbol_df):\n",
    "            print(f\"[WARN] Episode too short for {symbol} at {start_idx}, skipping...\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()  # tenta o próximo episódio\n",
    "\n",
    "        # ✅ Extração segura\n",
    "        #self.episode_df = symbol_df.iloc[start_idx : start_idx + self.episode_length].copy()\n",
    "        end = start_idx + self.episode_length + 1\n",
    "        if end > len(symbol_df):\n",
    "            print(f\"[WARN] Not enough data for {symbol} from {start_idx}, skipping.\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()\n",
    "        self.episode_df = symbol_df.iloc[start_idx:end].copy()\n",
    "        \n",
    "\n",
    "        # Move to next episode (with wrap-around)\n",
    "        self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "        \"\"\"\n",
    "        for _ in range(10):  # Try up to 10 times to get a valid episode\n",
    "            stock = self.stocks[0]\n",
    "            if hasattr(self, \"episode_sequence\"):\n",
    "                if self.episode_counter >= len(self.episode_sequence):\n",
    "                    self.episode_counter = 0\n",
    "                _, start = self.episode_sequence[self.episode_counter]\n",
    "                self.episode_counter += 1\n",
    "            else:\n",
    "                stock = np.random.choice(self.stocks)\n",
    "                stock_df = self.df[self.df['symbol'] == stock].reset_index(drop=True)\n",
    "                max_start = len(stock_df) - self.episode_length\n",
    "                if max_start <= 0:\n",
    "                    continue  # Try another stock\n",
    "                start = np.random.randint(0, max_start + 1)\n",
    "\n",
    "            self.stock = stock\n",
    "            stock_df = self.df[self.df['symbol'] == self.stock].reset_index(drop=True)\n",
    "            self.episode_df = stock_df.iloc[int(start):int(start) + int(self.episode_length + 2)].reset_index(drop=True)\n",
    "\n",
    "            if len(self.episode_df) >= self.window_length:\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"Failed to sample a valid episode with sufficient data.\")\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.entry_price = None\n",
    "        self.position = 0\n",
    "        self.holding_period = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.returns_history = []\n",
    "        self.reward_history = []\n",
    "        self.episode_pct_changes = self.episode_df['return_1d'].values\n",
    "        self.max_possible_reward = np.sum(np.abs(self.episode_pct_changes))\n",
    "        self.current_wealth = 1.0\n",
    "        self.peak_wealth = 1.0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Returns a rolling window of observations (2D or flattened)\n",
    "        obs_list = []\n",
    "        #for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "        #    idx = max(i, 0)  # pad with earliest available step\n",
    "        #    features = self.episode_df.iloc[idx][self.feature_cols].values.astype(np.float32)\n",
    "        for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "            if 0 <= i < len(self.episode_df):\n",
    "                features = self.episode_df.iloc[i][self.feature_cols].values.astype(np.float32)\n",
    "            else:\n",
    "                features = np.zeros(len(self.feature_cols), dtype=np.float32)  # zero padding\n",
    "            internal_state = {\n",
    "                \"position\": self.position,\n",
    "                \"holding_period\": self.holding_period,\n",
    "                \"cumulative_reward\": self.cumulative_reward,\n",
    "                \"pct_time\": self.current_step / self.episode_length,\n",
    "                \"drawdown\": self.drawdown,\n",
    "                \"rel_perf\": self.relative_perf,\n",
    "                \"unrealized_pnl\": self.unrealized_pnl,\n",
    "                \"entry_price\": self.entry_price if self.entry_price is not None else 0.0,\n",
    "                \"time_in_position\": self.time_in_position,\n",
    "            }\n",
    "            internal = np.array([internal_state[name] for name in self.internal_features], dtype=np.float32)\n",
    "            obs = np.concatenate([features, internal])\n",
    "            obs_list.append(obs)\n",
    "        obs_window = np.stack(obs_list)  # shape: (window_length, obs_dim)\n",
    "        if self.return_sequences:\n",
    "            return obs_window  # shape: (window_length, obs_dim)\n",
    "        else:\n",
    "            return obs_window.flatten()  # shape: (window_length * obs_dim,)\n",
    "        \n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        #print(self.current_step,self.episode_length,len(self.episode_df))\n",
    "        done = self.current_step >= self.episode_length - 1\n",
    "        current_row = self.episode_df.iloc[self.current_step]\n",
    "\n",
    "        # Protege contra acesso fora dos limites\n",
    "        if self.current_step + 1 < len(self.episode_df):\n",
    "            next_row = self.episode_df.iloc[self.current_step + 1]\n",
    "        else:\n",
    "            next_row = current_row.copy()  # fallback seguro\n",
    "\n",
    "        price_change = next_row['return_1d']\n",
    "        prev_position = self.position\n",
    "        reward = 0\n",
    "        cost = 0\n",
    "\n",
    "        self.action_counts[action] += 1\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if self.position != 1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = 1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if self.position != -1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = -1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        if self.position != 0:\n",
    "            self.holding_period += 1\n",
    "\n",
    "        step_return = self.position * price_change\n",
    "        self.returns_history.append(step_return)\n",
    "        self.current_wealth *= (1 + step_return)\n",
    "        if self.current_wealth > self.peak_wealth:\n",
    "            self.peak_wealth = self.current_wealth\n",
    "        self.drawdown = 1 - self.current_wealth / self.peak_wealth\n",
    "\n",
    "        if self.position != 0 and self.entry_price is not None:\n",
    "            current_price = next_row['close']\n",
    "            self.unrealized_pnl = (current_price - self.entry_price) * self.position / self.entry_price\n",
    "            self.time_in_position = self.current_step - self.entry_step\n",
    "        else:\n",
    "            self.unrealized_pnl = 0\n",
    "            self.time_in_position = 0\n",
    "\n",
    "        if 'market_return_1d' in self.episode_df.columns:\n",
    "            self.relative_perf = price_change - next_row['market_return_1d']\n",
    "        else:\n",
    "            self.relative_perf = 0\n",
    "\n",
    "        reward = self.reward_fn(\n",
    "            position=self.position,\n",
    "            price_change=price_change,\n",
    "            prev_position=prev_position,\n",
    "            env=self\n",
    "        )\n",
    "        reward -= cost\n",
    "        self.reward_history.append(reward)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        self.current_step += 1\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        info[\"regime\"] = self.get_current_regime()\n",
    "\n",
    "        # Calcula métricas no final do episódio\n",
    "        if done:\n",
    "            returns = np.array(self.returns_history)\n",
    "            mean = np.median(returns) if len(returns) > 0 else np.nan\n",
    "            std = returns.std() if len(returns) > 1 else np.nan\n",
    "            downside = returns[returns < 0]\n",
    "            downside_std = downside.std() if len(downside) > 1 else np.nan\n",
    "\n",
    "            sharpe = mean / std if (std is not None and std > 0 and not np.isnan(std)) else np.nan\n",
    "            sortino = mean / downside_std if (downside_std is not None and downside_std > 0 and not np.isnan(downside_std)) else np.nan\n",
    "\n",
    "            wealth_curve = np.cumprod(1 + returns) if len(returns) > 0 else np.array([])\n",
    "            peak_wealth = np.maximum.accumulate(wealth_curve) if len(wealth_curve) > 0 else np.array([])\n",
    "            drawdowns = (wealth_curve - peak_wealth) / (peak_wealth + 1e-8) if len(wealth_curve) > 0 else np.array([])\n",
    "            max_drawdown = np.abs(drawdowns.min()) if len(drawdowns) > 0 else np.nan\n",
    "            calmar = ((wealth_curve[-1] - 1) / max_drawdown) if (len(wealth_curve) > 0 and max_drawdown and not np.isnan(max_drawdown) and max_drawdown > 0) else np.nan\n",
    "            cum_return = wealth_curve[-1] - 1 if len(wealth_curve) > 0 else np.nan\n",
    "            final_wealth = wealth_curve[-1] if len(wealth_curve) > 0 else np.nan\n",
    "\n",
    "            # Trade-level metrics\n",
    "            trades = []\n",
    "            trade_profits = []\n",
    "            prev = 0\n",
    "            for i, ret in enumerate(returns):\n",
    "                if prev == 0 and ret != 0:\n",
    "                    entry_idx = i\n",
    "                    entry_dir = np.sign(ret)\n",
    "                elif prev != 0 and (ret == 0 or np.sign(ret) != np.sign(prev)):\n",
    "                    if 'entry_idx' in locals():\n",
    "                        trade = returns[entry_idx:i+1]\n",
    "                        trade_profits.append(np.sum(trade))\n",
    "                        del entry_idx\n",
    "                prev = ret\n",
    "            win_rate = np.median(np.array(trade_profits) > 0) if trade_profits else np.nan\n",
    "\n",
    "            if 'market_return_1d' in self.episode_df.columns:\n",
    "                market_returns = self.episode_df['market_return_1d'].values[1:self.episode_length]\n",
    "                market_wealth_curve = np.cumprod(1 + market_returns) if len(market_returns) > 0 else np.array([])\n",
    "                market_cum_return = market_wealth_curve[-1] - 1 if len(market_wealth_curve) > 0 else np.nan\n",
    "                alpha = cum_return - market_cum_return if cum_return is not None and not np.isnan(cum_return) and market_cum_return is not None and not np.isnan(market_cum_return) else np.nan\n",
    "            else:\n",
    "                alpha = np.nan\n",
    "\n",
    "            info.update({\n",
    "                \"episode_sharpe\": sharpe,\n",
    "                \"episode_sortino\": sortino,\n",
    "                \"episode_total_reward\": np.sum(self.reward_history) if len(self.reward_history) > 0 else np.nan,\n",
    "                \"cumulative_return\": cum_return,\n",
    "                \"calmar\": calmar,\n",
    "                \"max_drawdown\": max_drawdown,\n",
    "                \"win_rate\": win_rate,\n",
    "                \"alpha\": alpha,\n",
    "                \"returns\": returns,\n",
    "                \"market_returns\": market_returns if 'market_returns' in locals() else [],\n",
    "                \"downside\": downside,\n",
    "                \"regime\": self.get_current_regime(),\n",
    "                \"final_wealth\": final_wealth,\n",
    "                \"action_hold_count\": self.action_counts[0],\n",
    "                \"action_buy_count\": self.action_counts[1],\n",
    "                \"action_sell_count\": self.action_counts[2]\n",
    "            })\n",
    "\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step} | Pos: {self.position} | Hold: {self.holding_period} | CumRew: {self.cumulative_reward:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "class SequenceAwareSharpeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sharpe, **kwargs)\n",
    "\n",
    "class SequenceAwareSortinoTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sortino, **kwargs)\n",
    "\n",
    "class SequenceAwareAlphaTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_alpha, **kwargs)\n",
    "\n",
    "class SequenceAwareDrawdownTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_drawdown, **kwargs)\n",
    "\n",
    "class SequenceAwareCumulativeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_cumulative, **kwargs)\n",
    "\n",
    "class SequenceAwareCalmarTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_calmar, **kwargs)\n",
    "\n",
    "class SequenceAwareHybridTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_hybrid, **kwargs)\n",
    "\n",
    "class SequenceAwareBaselineTradingAgent:\n",
    "    def __init__(self,df,feature_cols=[],\n",
    "            episode_length=100, seed=314,set_episode_sequence=[]):\n",
    "    \n",
    "        self.env = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols,\n",
    "            episode_length=episode_length, seed=seed)\n",
    "        self.env.set_episode_sequence(set_episode_sequence)\n",
    "        \n",
    "    def predict(self,obs,*args,**kwargs):\n",
    "        #print(self.env.stocks,'xxxxxxxxxxx')\n",
    "        return self.env.action_space.sample(),{}\n",
    "    \n",
    "    def set_episode_sequence(self,seq):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # should return 0, 1, or 2\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550c1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8377624099423380081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design_v2\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "WINDOW_LENGTH = 10  \n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 800,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"project_name\":EXPERIENCE_NAME,\n",
    "    \"environment\": \"SequenceAwareCumulativeTradingEnv\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data ==================================\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# Experiment tracker ========================= \n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "experiment_tracker.set_hash(CONFIG)\n",
    "\n",
    "# Files ======================================\n",
    "checkpoint_path = \"data/checkpoint\" \n",
    "checkpoint_name = \"-8377624099423380081\"#experiment_tracker.run_hash\n",
    "checkpoint_preffix = f\"{checkpoint_name}--checkpoint\"\n",
    "checkpoint_best_model=f\"{checkpoint_path}/{checkpoint_name}--best_model\"\n",
    "log_path=\"data/logs\"\n",
    "save_path= f\"{checkpoint_path}/{checkpoint_name}--final\"\n",
    "print(checkpoint_name)\n",
    "#data/checkpoint/-8377624099423380081--final\n",
    "#data/checkpoint/-3848392742194634112--best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7102f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 100\n"
     ]
    }
   ],
   "source": [
    "EPISODE_LENGTH = 100\n",
    "MAX_LENGTH = 200\n",
    "SAVE_FREQ=10000\n",
    "EVAL_FREQ=5000\n",
    "TOTAL_TIMESTEPS=200000\n",
    "#TOTAL_TIMESTEPS=1000\n",
    "EPISODES_PER_UPDATE = 8          # ~how many episodes before PPO updates\n",
    "EPISODES_PER_BATCH = 1           # number of full episodes per batch\n",
    "\n",
    "# === Auto-derive PPO settings ===\n",
    "N_STEPS = EPISODE_LENGTH * EPISODES_PER_UPDATE\n",
    "BATCH_SIZE = EPISODE_LENGTH * EPISODES_PER_BATCH\n",
    "\n",
    "ENV_CLASS = SequenceAwareCumulativeTradingEnv\n",
    "\n",
    "n = Notify(experiment_tracker.project)\n",
    "n.info('START')\n",
    "print(N_STEPS,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e51121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1961 different episodes accross the top 2 stocks for each sector\n",
      "Testing on 22 different episodes accross the top 2 stocks for each sector\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 10.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 800      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 4.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 1600        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008689321 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.000169    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 7.57        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 4.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 2400        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013494816 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0561      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 7.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 3.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 3200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012913847 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0792      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0018     |\n",
      "|    value_loss           | 7           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 2.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011778145 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0746      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0098     |\n",
      "|    value_loss           | 5.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 2.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 4800        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008499797 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | -0.0266     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.3         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 7.02        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-6.12 +/- 4.80\n",
      "Episode length: 102.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 102         |\n",
      "|    mean_reward          | -6.12       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013199283 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | -0.00122    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.4         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000313   |\n",
      "|    value_loss           | 9.23        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 3        |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 5600     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | 2.47         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 6400         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046049524 |\n",
      "|    clip_fraction        | 0.0721       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.861       |\n",
      "|    explained_variance   | 0.00843      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.64         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    value_loss           | 6.47         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 7200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005759397 |\n",
      "|    clip_fraction        | 0.0264      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.857      |\n",
      "|    explained_variance   | 0.0644      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.28        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005345338 |\n",
      "|    clip_fraction        | 0.00712     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.867      |\n",
      "|    explained_variance   | -0.00672    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.000929   |\n",
      "|    value_loss           | 6.53        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | 1.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 422          |\n",
      "|    total_timesteps      | 8800         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112262815 |\n",
      "|    clip_fraction        | 0.0854       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | 0.0497       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.836        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 9600        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025309598 |\n",
      "|    clip_fraction        | 0.0442      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    value_loss           | 4.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=5.54 +/- 5.90\n",
      "Episode length: 102.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 102         |\n",
      "|    mean_reward          | 5.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015733771 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.834      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.14        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 7.68        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 0.811    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 516      |\n",
      "|    total_timesteps | 10400    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 0.553       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 556         |\n",
      "|    total_timesteps      | 11200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016949331 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.785      |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00937     |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 0.404       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 615         |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018225886 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.766      |\n",
      "|    explained_variance   | 0.0226      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 0.101       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 12800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019670397 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.778      |\n",
      "|    explained_variance   | -0.0415     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.929       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    value_loss           | 6.09        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 0.0336     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 716        |\n",
      "|    total_timesteps      | 13600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03528353 |\n",
      "|    clip_fraction        | 0.0442     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.0293     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.83       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | 0.00695    |\n",
      "|    value_loss           | 4.47       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 0.512       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 14400       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003924586 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | -0.0269     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.000665    |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=4.31 +/- 9.31\n",
      "Episode length: 102.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 102        |\n",
      "|    mean_reward          | 4.31       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915872 |\n",
      "|    clip_fraction        | 0.0461     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.806     |\n",
      "|    explained_variance   | -0.033     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.89       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.00245    |\n",
      "|    value_loss           | 10.6       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 0.733    |\n",
      "| time/              |          |\n",
      "|    fps             | 18       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 812      |\n",
      "|    total_timesteps | 15200    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 855         |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008025272 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.887      |\n",
      "|    explained_variance   | 0.00924     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 6.44        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 900         |\n",
      "|    total_timesteps      | 16800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014848726 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 0.000484    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 7.76        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 1.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 948        |\n",
      "|    total_timesteps      | 17600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04320553 |\n",
      "|    clip_fraction        | 0.0831     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.805     |\n",
      "|    explained_variance   | 0.00394    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.31       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0009    |\n",
      "|    value_loss           | 7.22       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 998        |\n",
      "|    total_timesteps      | 18400      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01505216 |\n",
      "|    clip_fraction        | 0.0392     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.789     |\n",
      "|    explained_variance   | -0.00543   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.27       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | 0.000948   |\n",
      "|    value_loss           | 7.32       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 1.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 1045       |\n",
      "|    total_timesteps      | 19200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01626652 |\n",
      "|    clip_fraction        | 0.0436     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.698     |\n",
      "|    explained_variance   | -0.0263    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.6        |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.000907  |\n",
      "|    value_loss           | 6          |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-1.67 +/- 10.97\n",
      "Episode length: 102.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 102         |\n",
      "|    mean_reward          | -1.67       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003096817 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | -0.0195     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.92        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.000494    |\n",
      "|    value_loss           | 7.13        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 2.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 18       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1090     |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 2.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 1136       |\n",
      "|    total_timesteps      | 20800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06838205 |\n",
      "|    clip_fraction        | 0.0469     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.578     |\n",
      "|    explained_variance   | 0.0132     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.58       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | 0.00818    |\n",
      "|    value_loss           | 7.09       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 2.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1180        |\n",
      "|    total_timesteps      | 21600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015136119 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.0161      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 6.98        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 3.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1225        |\n",
      "|    total_timesteps      | 22400       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023152202 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 0.0145      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.59        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.000915    |\n",
      "|    value_loss           | 5.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 3.04       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 1271       |\n",
      "|    total_timesteps      | 23200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12309171 |\n",
      "|    clip_fraction        | 0.0996     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.689     |\n",
      "|    explained_variance   | -0.106     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.21       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | 0.0238     |\n",
      "|    value_loss           | 4.92       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 3.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1316        |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025562687 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | -0.168      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000949   |\n",
      "|    value_loss           | 5.62        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 2.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1357        |\n",
      "|    total_timesteps      | 24800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019257916 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | -0.0784     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 7.73        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-9.31 +/- 7.57\n",
      "Episode length: 102.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 102        |\n",
      "|    mean_reward          | -9.31      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00946239 |\n",
      "|    clip_fraction        | 0.0529     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.701     |\n",
      "|    explained_variance   | 0.0306     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.09       |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00155   |\n",
      "|    value_loss           | 5.12       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 2.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 18       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 1402     |\n",
      "|    total_timesteps | 25600    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 1.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1448        |\n",
      "|    total_timesteps      | 26400       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012360521 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | -0.0211     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.49        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    value_loss           | 7.23        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | 2.32         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 1502         |\n",
      "|    total_timesteps      | 27200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075347004 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.00736      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 6.2          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 3           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1564        |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016656954 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.062       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 3.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1618        |\n",
      "|    total_timesteps      | 28800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009127669 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.736      |\n",
      "|    explained_variance   | -0.0722     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.08        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -1.92e-05   |\n",
      "|    value_loss           | 4.65        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 102        |\n",
      "|    ep_rew_mean          | 3.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 1681       |\n",
      "|    total_timesteps      | 29600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02988961 |\n",
      "|    clip_fraction        | 0.0917     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.624     |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.82       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.000824   |\n",
      "|    value_loss           | 6.68       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=30000, episode_reward=0.86 +/- 6.37\n",
      "Episode length: 102.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 102          |\n",
      "|    mean_reward          | 0.858        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076282956 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.636       |\n",
      "|    explained_variance   | 0.0547       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | 0.000414     |\n",
      "|    value_loss           | 8.35         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 3.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 17       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1744     |\n",
      "|    total_timesteps | 30400    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "# Causal Mask Function ============================\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# Transformer Feature Extractor ===================\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=64, n_heads=4, n_layers=2, max_len=MAX_LENGTH):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "        input_dim = observation_space.shape[-1]\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward_v1(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "        return x[:, -1]  # return the last token output\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        #print(\">>> [Transformer] Input shape:\", obs.shape)\n",
    "\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "\n",
    "        pooled_output = x[:, -1]\n",
    "        #print(\">>> [Transformer] Pooled output mean/std:\", pooled_output.mean().item(), pooled_output.std().item())\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "# Transformer Policy ===================================\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs,\n",
    "                         features_extractor_class=TransformerFeatureExtractor,\n",
    "                         features_extractor_kwargs=dict(\n",
    "                             d_model=64, n_heads=4, n_layers=2, max_len=32\n",
    "                         ))\n",
    "        #self._build(self.lr_schedule)\n",
    "\n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)\n",
    "\n",
    "class PerEpisodeRewardNormalizer(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_rewards = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean = np.mean(self.episode_rewards)\n",
    "        std = np.std(self.episode_rewards) + 1e-8\n",
    "        norm_reward = (reward - mean) / std\n",
    "        return obs, norm_reward, done, truncated, info\n",
    "# Training =============================================================\n",
    "train_df = ohlcv_df[(ohlcv_df['date']>=\"2023-01-01\") & (ohlcv_df['date']<\"2023-07-01\")]\n",
    "test_df = ohlcv_df[(ohlcv_df['date']>=\"2023-07-01\") & (ohlcv_df['date']<\"2024-01-01\")]\n",
    "train_df = train_df[train_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "test_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "# Train on few episodes to prove a point only\n",
    "train_seq = train_env.generate_episode_sequences(TOTAL_TIMESTEPS)\n",
    "_test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS))\n",
    "episodes = _test_seq\n",
    "unique_episodes = {}\n",
    "for ticker, start in episodes:\n",
    "    if ticker not in unique_episodes:\n",
    "        unique_episodes[ticker] = start\n",
    "# Convert back to a list of tuples\n",
    "test_seq = [(ticker, start) for ticker, start in unique_episodes.items()]\n",
    "\n",
    "print(f\"Training on {len(train_seq)} different episodes accross the top 2 stocks for each sector\")\n",
    "print(f\"Testing on {len(test_seq)} different episodes accross the top 2 stocks for each sector\")\n",
    "def train_agent():\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    train_env.set_episode_sequence(train_seq)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    \n",
    "    train_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(train_env))\n",
    "    eval_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env))\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=SAVE_FREQ, save_path=checkpoint_path, name_prefix=checkpoint_preffix\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, best_model_save_path=checkpoint_best_model,\n",
    "        log_path=log_path, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    model = RecurrentPPO(\n",
    "        policy=TransformerPolicy,\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        #tensorboard_log=\"./tensorboard_logs\",\n",
    "        n_steps=N_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.005,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs=dict(share_features_extractor=True)\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "    model.save(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38331a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Notify(experiment_tracker.project)\n",
    "n.info('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd21cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketVersusWalletHistoryTracker:\n",
    "    def __init__(self, initial_wallet=1.0):\n",
    "        self.wallet_value = initial_wallet\n",
    "        self.prev_wallet_value = initial_wallet\n",
    "        self.wallet_locked = False\n",
    "        self.buy_price = None\n",
    "        self.market_entry_price = None\n",
    "        self.last_price = None\n",
    "        self.has_opened_position = False  # NEW: ensure proper update after first buy\n",
    "\n",
    "        self.wallet_history = []\n",
    "        self.market_history = []\n",
    "        self.price_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset(self, initial_price):\n",
    "        self.__init__(initial_wallet=1.0)\n",
    "        self.market_entry_price = initial_price\n",
    "        self.last_price = initial_price\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.market_history.append(1.0)\n",
    "        self.price_history.append(initial_price)\n",
    "        self.action_history.append(0)\n",
    "\n",
    "    def step(self, action, current_price):\n",
    "        self.price_history.append(current_price)\n",
    "        agent_action = 0\n",
    "\n",
    "        # === 1. Update market benchmark ===\n",
    "        market_perf = current_price / self.market_entry_price\n",
    "        self.market_history.append(market_perf)\n",
    "\n",
    "        # === 2. Update wallet value ===\n",
    "        if self.wallet_locked and self.has_opened_position:\n",
    "            self.wallet_value *= current_price / self.last_price\n",
    "\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.prev_wallet_value = self.wallet_value\n",
    "        self.last_price = current_price  # must be set after wallet update!\n",
    "\n",
    "        # === 3. Process Action ===\n",
    "        if action == 1 and not self.wallet_locked:\n",
    "            self.buy_price = current_price\n",
    "            self.wallet_locked = True\n",
    "            self.has_opened_position = True\n",
    "            agent_action = 1\n",
    "\n",
    "        elif action == 2 and self.wallet_locked:\n",
    "            self.wallet_locked = False\n",
    "            self.buy_price = None\n",
    "            self.has_opened_position = False\n",
    "            agent_action = 2\n",
    "\n",
    "        self.action_history.append(agent_action)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"wallet_history\": self.wallet_history,\n",
    "            \"market_history\": self.market_history,\n",
    "            \"market_price_history\": self.price_history,\n",
    "            \"performed_action_history\": self.action_history\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "#from src.env.base_timeseries_trading_env import SequenceAwareCumulativeTradingEnv\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "#from src.env.base_timeseries_trading_env import RegimeAugmentingWrapper\n",
    "\n",
    "ENV_CLASS=SequenceAwareCumulativeTradingEnv\n",
    "# === Config ===\n",
    "N_EVAL_EPISODES = 5\n",
    "MODEL_PATH = save_path\n",
    "\n",
    "# === Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "test_df = ohlcv_df[(ohlcv_df['date'] >= \"2023-07-01\") & (ohlcv_df['date'] < \"2024-01-01\")]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "\n",
    "# === Evaluation Logic ===\n",
    "def evaluate_agent(agent, env, n_episodes=22):\n",
    "    episode_metrics = []\n",
    "    episode_infos = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating Agent\"):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        # === Tracker setup ===\n",
    "        tracker = MarketVersusWalletHistoryTracker()\n",
    "        initial_price = env.env.env.episode_df.iloc[0]['close']\n",
    "        tracker.reset(initial_price)\n",
    "\n",
    "        while not done:\n",
    "            action, state = agent.predict(obs, state=state, deterministic=True)\n",
    "            action = int(action)\n",
    "            current_price = env.env.env.episode_df.iloc[env.env.env.current_step]['close']\n",
    "\n",
    "            # Step the wallet tracker\n",
    "            tracker.step(action, current_price)\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        # === Episode summary ===\n",
    "        _env = env.env.env\n",
    "        agent_wealth = infos[-1].get(\"final_wealth\", np.nan)\n",
    "        market_wealth = np.prod(1 + _env.episode_df['market_return_1d'].values)\n",
    "        alpha = agent_wealth - market_wealth\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": agent_wealth,\n",
    "            \"market_wealth\": market_wealth,\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "            \"alpha\": alpha,\n",
    "            \"episode_id\": _env.episode_counter,\n",
    "            \"regime\": infos[-1].get(\"regime\", np.nan)\n",
    "        }\n",
    "\n",
    "        tracker_data = tracker.export()\n",
    "        info[\"ticker\"] = _env.episode_df.iloc[0]['symbol']\n",
    "        info[\"wallet_history\"] = tracker_data[\"wallet_history\"]\n",
    "        info[\"market_history\"] = tracker_data[\"market_history\"]\n",
    "        info[\"market_price_history\"] = tracker_data[\"market_price_history\"]\n",
    "        info[\"performed_action_history\"] = tracker_data[\"performed_action_history\"]\n",
    "        episode_infos.append(info)\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics), episode_infos\n",
    "\n",
    "def evaluate_random_agent( env, n_episodes=22):\n",
    "    episode_metrics = []\n",
    "    episode_infos = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating Agent\"):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        # === Tracker setup ===\n",
    "        tracker = MarketVersusWalletHistoryTracker()\n",
    "        initial_price = env.env.env.episode_df.iloc[0]['close']\n",
    "        tracker.reset(initial_price)\n",
    "        initial_symbol =env.env.env.episode_df.iloc[0]['symbol']\n",
    "        #print(initial_symbol,env.env.env.episode_df.iloc[0]['date'],len(env.env.env.episode_df),env.env.env.episode_counter,env.env.env.episode_sequence)\n",
    "        while not done:\n",
    "            action=  env.action_space.sample()\n",
    "            action = int(action)\n",
    "            current_price = env.env.env.episode_df.iloc[env.env.env.current_step]['close']\n",
    "            current_symbol = env.env.env.episode_df.iloc[env.env.env.current_step]['symbol']\n",
    "            if(current_symbol != initial_symbol):\n",
    "                print('EPISODE SWITCHED', initial_symbol,current_symbol)\n",
    "                initial_symbol = current_symbol\n",
    "            # Step the wallet tracker\n",
    "            tracker.step(action, current_price)\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "        #print(current_symbol)\n",
    "        # === Episode summary ===\n",
    "        _env = env.env.env\n",
    "        agent_wealth = infos[-1].get(\"final_wealth\", np.nan)\n",
    "        market_wealth = np.prod(1 + _env.episode_df['market_return_1d'].values)\n",
    "        alpha = agent_wealth - market_wealth\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": agent_wealth,\n",
    "            \"market_wealth\": market_wealth,\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "            \"alpha\": alpha,\n",
    "            \"episode_id\": _env.episode_counter,\n",
    "            \"regime\": infos[-1].get(\"regime\", np.nan)\n",
    "        }\n",
    "\n",
    "        tracker_data = tracker.export()\n",
    "        info[\"ticker\"] = _env.episode_df.iloc[0]['symbol']\n",
    "        info[\"wallet_history\"] = tracker_data[\"wallet_history\"]\n",
    "        info[\"market_history\"] = tracker_data[\"market_history\"]\n",
    "        info[\"market_price_history\"] = tracker_data[\"market_price_history\"]\n",
    "        info[\"performed_action_history\"] = tracker_data[\"performed_action_history\"]\n",
    "        episode_infos.append(info)\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics), episode_infos\n",
    "\n",
    "\n",
    "# === Run Evaluation ===\n",
    "\n",
    "model = RecurrentPPO.load(MODEL_PATH)\n",
    "\n",
    "def make_test_env():\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    return PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env)) #RegimeAugmentingWrapper(ENV_CLASS(test_df, episode_length=EPISODE_LENGTH,feature_cols=FEATURE_COLS))\n",
    "\n",
    "\n",
    "ppo_agent_df, ppo_agent_infos = evaluate_agent(model, make_test_env(), n_episodes=22)\n",
    "random_agent_df, random_agent_infos = evaluate_random_agent(make_test_env(), n_episodes=22)\n",
    "\n",
    "ppo_agent_df[\"agent\"] = \"recurrent_ppo\"\n",
    "random_agent_df[\"agent\"] = \"random\"\n",
    "results_df = pd.concat([ppo_agent_df, random_agent_df])\n",
    "\n",
    "\n",
    "# === Plotting ===\n",
    "melted = results_df.melt(id_vars=\"agent\", var_name=\"metric\", value_name=\"value\")\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=melted, x=\"metric\", y=\"value\", hue=\"agent\")\n",
    "plt.title(\"Agent Performance Comparison (Random vs Recurrent PPO)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Statistical Tests ===\n",
    "comparison_results = []\n",
    "\n",
    "for metric in ppo_agent_df.columns[:-1]:  # exclude 'agent'\n",
    "    a = ppo_agent_df[metric].dropna()\n",
    "    b = random_agent_df[metric].dropna()\n",
    "\n",
    "    # Skip if either is empty\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        print(f\"Skipping metric {metric}: empty values\")\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val_t = ttest_ind(a, b)\n",
    "    u_stat, p_val_u = mannwhitneyu(a, b, alternative='two-sided')\n",
    "    comparison_results.append({\n",
    "        \"metric\": metric,\n",
    "        \"t-test p-value\": p_val_t,\n",
    "        \"mann-whitney p-value\": p_val_u\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71fc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_test_env().env.env.episode_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=ppo_agent_df, x='market_wealth', y='alpha', hue='regime', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.scatterplot(data=ppo_agent_df, x='market_wealth', y='alpha', hue='agent')\n",
    "plt.axhline(0, linestyle='--', color='red', label='Alpha = 0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent_df['alpha_ratio'] = ppo_agent_df['alpha'] / np.abs(ppo_agent_df['market_wealth'] + 1e-8)\n",
    "sns.boxplot(data=ppo_agent_df, x='agent', y='alpha_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa252f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episode_complete_results = []\n",
    "for i in range(len(ppo_agent_infos)):# or select based on alpha, reward, etc.\n",
    "#for i in range(5):# or select based on alpha, reward, etc.\n",
    "    info = ppo_agent_infos[i]\n",
    "    info_ = random_agent_infos[i]\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(info['market_history'], label='Market')\n",
    "    plt.plot(info['wallet_history'], label='Wallet (Agent)')\n",
    "    plt.plot(info_['wallet_history'], label='Wallet (Random)')\n",
    "    plt.title(f\"Episode {info.get('episode_id')} - {info.get('ticker')}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    episode_complete_results.append({\n",
    "        \"agent_wallet\":info['wallet_history'][-1],\n",
    "        \"random_wallet\":info_['wallet_history'][-1],\n",
    "        \"market_wallet\":info_['market_history'][-1],\n",
    "                                   })\n",
    "episode_complete_results_df = pd.DataFrame(episode_complete_results)\n",
    "episode_complete_results_df['agent>random']=episode_complete_results_df['agent_wallet']>episode_complete_results_df['random_wallet']\n",
    "episode_complete_results_df['agent>market']=episode_complete_results_df['agent_wallet']>episode_complete_results_df['market_wallet']\n",
    "print(\"AGENT > RANDOM value counts\",episode_complete_results_df['agent>random'].value_counts())\n",
    "print(\"AGENT > MARKET value counts\",episode_complete_results_df['agent>market'].value_counts())\n",
    "episode_complete_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866af689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test\n",
    "\n",
    "n_wins = episode_complete_results_df['agent>random'].sum()\n",
    "n_trials = len(episode_complete_results_df)\n",
    "\n",
    "p_val = binom_test(n_wins, n_trials, p=0.5, alternative='greater')\n",
    "print(f\"PPO wins over Random: {n_wins}/{n_trials}, binomial p-value = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p_df = pd.DataFrame({\"wallet\":ppo_agent_infos[0][\"wallet_history\"],\"price\":ppo_agent_infos[0][\"market_price_history\"],\"performed_action\":ppo_agent_infos[0][\"performed_action_history\"]})\n",
    "for i in range(len(w_p_df)):\n",
    "    print({\"i\":i,\"wallet\":w_p_df.iloc[i][\"wallet\"], \"price\":w_p_df.iloc[i][\"price\"], \"performed_action\":w_p_df.iloc[i][\"performed_action\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppo_agent_infos[0][\"performed_action_history\"]),len(ppo_agent_infos[0][\"market_price_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWARD ANALYSIS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick one episode\n",
    "info = ppo_agent_infos[0]  # or any i\n",
    "rewards = info[\"episode_total_reward\"]\n",
    "returns = info[\"returns\"]\n",
    "cum_rewards = np.cumsum(info[\"returns\"] * info[\"performed_action_history\"])  # or info['reward_history'] if saved\n",
    "cum_returns = np.cumprod(1 + np.array(returns)) - 1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cum_returns, label=\"Cumulative Return\")\n",
    "plt.plot(np.cumsum(info[\"reward_history\"]), label=\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Return vs Reward\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.cumsum(info[\"reward_history\"]), cum_returns)\n",
    "plt.xlabel(\"Cumulative Reward\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.title(\"Reward vs Return Correlation\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "corr(reward, return) ≥ 0.7 ⇒ strong alignment\n",
    "\n",
    "corr(reward, alpha) ≥ 0.6 ⇒ good relative performance\n",
    "\n",
    "If < 0.3 → your agent may be overfitting to reward noise or not capturing actual alpha\n",
    "\"\"\"\n",
    "reward_list = []\n",
    "return_list = []\n",
    "alpha_list = []\n",
    "\n",
    "for info in ppo_agent_infos:\n",
    "    reward_list.append(np.sum(info[\"reward_history\"]))\n",
    "    return_list.append(info[\"final_wealth\"] - 1)\n",
    "    alpha_list.append(info[\"alpha\"])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"reward\": reward_list,\n",
    "    \"return\": return_list,\n",
    "    \"alpha\": alpha_list\n",
    "})\n",
    "\n",
    "print(df.corr())\n",
    "sns.pairplot(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
