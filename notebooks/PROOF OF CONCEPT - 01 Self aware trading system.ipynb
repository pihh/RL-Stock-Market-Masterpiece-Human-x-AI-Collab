{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c29e00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Self-Aware Transformer Agent for Trading\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional RL trading agents rely heavily on market signals. But the market is noisy, volatile, and unpredictable — especially in short windows. Instead of chasing external patterns, we **shift the focus inward**:\n",
    "\n",
    "> Teach the agent to interpret itself, its context, and act based on internal understanding and consistent behavior.\n",
    "\n",
    "This system mimics how **career traders** operate: not by predicting price ticks, but by reacting intelligently to their strategy performance, risk levels, and regime shifts.\n",
    "\n",
    "---\n",
    "\n",
    "## Agent Design Philosophy\n",
    "\n",
    "| Principle                | Implementation                                                         |\n",
    "| ------------------------ | ---------------------------------------------------------------------- |\n",
    "| Internal State Awareness | Agent tracks position, time held, drawdown, unrealized PnL             |\n",
    "| Compact, Dense Features  | Handcrafted candlestick + PCA-compressed signals                       |\n",
    "| Regime-Driven Training   | Train using rolling market windows with real history                   |\n",
    "| Transformer Context Use  | Use long context (2 months) to act intelligently in the next (1 month) |\n",
    "| Reward Discipline        | Combine PnL, drawdown penalty, and Sharpe-style consistency bonus      |\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering: `FeatureCompressor`\n",
    "\n",
    "| Feature Group        | Description                                          |\n",
    "| -------------------- | ---------------------------------------------------- |\n",
    "| Candlestick Shape    | `body`, `upper_shadow`, `lower_shadow`, `body_ratio` |\n",
    "| Relative Behavior    | `z_close`, `rel_volume`, `price_vs_range`            |\n",
    "| Signal Strength      | `rolling_sharpe`, `entropy`                          |\n",
    "| Historical Embedding | `pca_1`, `pca_2` from rolling PCA of close/volume    |\n",
    "\n",
    "All features are window-smoothed to reduce noise and expose true structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Custom Environment: `SelfAwareTradingEnv`\n",
    "\n",
    "* Builds the agent's **state** with both market and internal agent features\n",
    "* Internal metrics: position, time held, cumulative reward, drawdown, unrealized PnL, pct episode completed\n",
    "* **Reward Function**:\n",
    "\n",
    "  ```python\n",
    "  shaped_reward = (\n",
    "      0.5 * raw_pnl\n",
    "      + 0.3 * sharpe_bonus\n",
    "      - 0.2 * drawdown_penalty\n",
    "  )\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "* Input: full feature state (market + internal), padded or clipped to 60 steps\n",
    "* Positional encoding: learnable\n",
    "* Backbone: TransformerEncoder (2 layers, 4 heads)\n",
    "* Outputs a context vector per timestep → passed to actor & critic\n",
    "* Integrated with `RecurrentPPO` for temporal memory\n",
    "\n",
    "---\n",
    "\n",
    "## Windowed Training Design\n",
    "\n",
    "Each training episode is built from a **single ticker**:\n",
    "\n",
    "| Step | Description                                                                     |\n",
    "| ---- | ------------------------------------------------------------------------------- |\n",
    "| 1    | Select a ticker and sort its data by date                                       |\n",
    "| 2    | Slice 60 days as `context` and next 30 as `target`                              |\n",
    "| 3    | Use only `target` for training, but allow agent to leverage Transformer context |\n",
    "| 4    | Repeat across all valid rolling windows and tickers                             |\n",
    "\n",
    "This creates hundreds of **coherent, regime-aware episodes** for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Full Pipeline Summary\n",
    "\n",
    "```text\n",
    "load_base_dataframe()\n",
    "↓\n",
    "MarketWindowBuilder → [context_df, target_df] for each ticker\n",
    "↓\n",
    "FeatureCompressor → Compress raw OHLCV into ~10 descriptive features\n",
    "↓\n",
    "SelfAwareTradingEnv → Builds internal + market state, shaped rewards\n",
    "↓\n",
    "RecurrentPPO (TransformerPolicy) → Trains over target month\n",
    "↓\n",
    "EpisodeLoggerCallback → Tracks Sharpe, Win Rate, Drawdown, # Trades\n",
    "↓\n",
    "EvaluationRunner → Loads each agent window and scores deterministically\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation & Benchmarking\n",
    "\n",
    "Each agent is evaluated on the same `target_df` it was trained on, using **deterministic inference**. Logged metrics include:\n",
    "\n",
    "| Metric         | Description                                      |\n",
    "| -------------- | ------------------------------------------------ |\n",
    "| `total_reward` | Sum of all rewards in episode                    |\n",
    "| `avg_reward`   | Mean reward per step                             |\n",
    "| `sharpe_bonus` | Volatility-adjusted consistency score            |\n",
    "| `drawdown`     | Max drawdown during episode                      |\n",
    "| `win_rate`     | % of profitable actions (to be optionally added) |\n",
    "\n",
    "Results are saved in `evaluation_results.csv` and can be compared across:\n",
    "\n",
    "* Tickers\n",
    "* Time periods\n",
    "* Volatility regimes\n",
    "\n",
    "---\n",
    "\n",
    "## Benchmark Plan\n",
    "\n",
    "| Baseline             | Description                                                |\n",
    "| -------------------- | ---------------------------------------------------------- |\n",
    "| Random Policy        | Random action sampling, same environment                   |\n",
    "| Hold-only Agent      | Buy once and hold through episode                          |\n",
    "| Classical PPO w/ MLP | Same pipeline, no memory or context                        |\n",
    "| Market Benchmark     | Cumulative return of passive exposure during target period |\n",
    "\n",
    "We will compute **performance deltas** against these baselines and track:\n",
    "\n",
    "* Sharpe outperformance\n",
    "* Drawdown reduction\n",
    "* Adaptation to volatile periods\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes This Unique?\n",
    "\n",
    "* Agent **doesn’t predict**, it *reacts adaptively*\n",
    "* Context is used **intelligently**, not just stacked\n",
    "* Agent performance is **relative to itself**, not absolute return\n",
    "* Internal state + compressed signals = **compact but expressive space**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "from IPython.display import display\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b78b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class FeatureCompressor:\n",
    "    def __init__(self, window=10):\n",
    "        self.window = window\n",
    "        self.pca = PCA(n_components=2)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        # === CANDLE FEATURES ===\n",
    "        df['body'] = df['close'] - df['open']\n",
    "        df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "        df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "        df['range'] = df['high'] - df['low']\n",
    "        df['body_ratio'] = df['body'] / (df['range'] + 1e-8)\n",
    "\n",
    "        # === RELATIVE CONTEXT FEATURES ===\n",
    "        df['z_close'] = (df['close'] - df['close'].rolling(self.window).mean()) / (df['close'].rolling(self.window).std() + 1e-8)\n",
    "        df['rel_volume'] = df['volume'] / (df['volume'].rolling(self.window).mean() + 1e-8)\n",
    "        df['price_vs_range'] = (df['close'] - df['low'].rolling(self.window).min()) / (\n",
    "            df['high'].rolling(self.window).max() - df['low'].rolling(self.window).min() + 1e-8)\n",
    "\n",
    "        # === SIGNAL QUALITY (EWM) ===\n",
    "        df['return'] = df['close'].pct_change()\n",
    "        df['ewm_return_mean'] = df['return'].ewm(span=self.window).mean()\n",
    "        df['ewm_return_std'] = df['return'].ewm(span=self.window).std()\n",
    "        df['rolling_sharpe'] = df['ewm_return_mean'] / (df['ewm_return_std'] + 1e-8)\n",
    "        df['entropy'] = df['return'].rolling(self.window).apply(self.shannon_entropy, raw=True)\n",
    "\n",
    "        # === PCA EMBEDDING ===\n",
    "        pca_inputs = []\n",
    "        raw = df[['close', 'volume']].fillna(0).values\n",
    "        for i in range(self.window, len(raw)):\n",
    "            window = raw[i - self.window:i].flatten()\n",
    "            pca_inputs.append(window)\n",
    "\n",
    "        if len(pca_inputs) > 0:\n",
    "            compressed = self.pca.fit_transform(np.array(pca_inputs))\n",
    "            df.loc[df.index[-len(compressed):], 'pca_1'] = compressed[:, 0]\n",
    "            df.loc[df.index[-len(compressed):], 'pca_2'] = compressed[:, 1]\n",
    "        else:\n",
    "            df['pca_1'] = 0\n",
    "            df['pca_2'] = 0\n",
    "\n",
    "        # === DATE-TIME FEATURES ===\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_month'] = df['date'].dt.day\n",
    "            df['month'] = df['date'].dt.month\n",
    "        else:\n",
    "            df['day_of_week'] = 0\n",
    "            df['day_of_month'] = 0\n",
    "            df['month'] = 0\n",
    "\n",
    "        # === FINAL FEATURE SET ===\n",
    "        features = [\n",
    "            'body', 'upper_shadow', 'lower_shadow', 'body_ratio',\n",
    "            'z_close', 'rel_volume', 'price_vs_range',\n",
    "            'rolling_sharpe', 'entropy',\n",
    "            'pca_1', 'pca_2',\n",
    "            'day_of_week', 'day_of_month', 'month'\n",
    "        ]\n",
    "\n",
    "        return df[features].dropna().reset_index(drop=True)\n",
    "\n",
    "    def shannon_entropy(self, x):\n",
    "        hist, bins = np.histogram(x, bins=5, density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return -np.sum(hist * np.log(hist + 1e-8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6db6df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_aware_trading_env.py\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pandas as pd\n",
    "#from feature_compressor import FeatureCompressor\n",
    "\n",
    "class SelfAwareTradingEnv(gym.Env):\n",
    "    def __init__(self, ohlcv_df: pd.DataFrame, episode_length=100):\n",
    "        super().__init__()\n",
    "        self.episode_length = episode_length\n",
    "        self.current_step = 0\n",
    "\n",
    "        # === Feature Compression ===\n",
    "        self.compressor = FeatureCompressor(window=10)\n",
    "        self.market_data = self.compressor.transform(ohlcv_df).values\n",
    "\n",
    "        # Agent state\n",
    "        self.current_position = 0  # -1 short, 0 flat, 1 long\n",
    "        self.time_in_position = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.position_entry_price = 0.0\n",
    "        self.equity_curve = []\n",
    "        self.returns_history = []\n",
    "\n",
    "        self.market_dim = self.market_data.shape[1]\n",
    "        self.internal_dim = 6\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.market_dim + self.internal_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = sell, 1 = hold, 2 = buy\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.current_position = 0\n",
    "        self.time_in_position = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.position_entry_price = 0.0\n",
    "        self.equity_curve = [1.0]\n",
    "        self.returns_history = []\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        price = self.market_data[self.current_step, 0]  # assume first feature is close or PCA_1\n",
    "\n",
    "        # Process action\n",
    "        raw_reward = 0.0\n",
    "        if action == 0:  # SELL\n",
    "            raw_reward = self._close_position(price, -1)\n",
    "        elif action == 2:  # BUY\n",
    "            raw_reward = self._close_position(price, 1)\n",
    "        else:\n",
    "            self.time_in_position += 1\n",
    "\n",
    "        # Track and update\n",
    "        self.total_reward += raw_reward\n",
    "        self.returns_history.append(raw_reward)\n",
    "        self.equity_curve.append(self.equity_curve[-1] * (1 + raw_reward))\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_length\n",
    "\n",
    "        # Apply reward shaping\n",
    "        shaped_reward = (\n",
    "            0.5 * raw_reward\n",
    "            + 0.3 * self.sharpe_bonus()\n",
    "            - 0.2 * self.calc_drawdown()\n",
    "        )\n",
    "\n",
    "        return self._get_obs(), shaped_reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        market_features = self.market_data[self.current_step]\n",
    "        internal_features = np.array([\n",
    "            self.current_position,\n",
    "            self.time_in_position,\n",
    "            self.calc_unrealized_pnl(),\n",
    "            self.total_reward,\n",
    "            self.calc_drawdown(),\n",
    "            self.current_step / self.episode_length\n",
    "        ])\n",
    "        return np.concatenate([market_features, internal_features])\n",
    "\n",
    "    def _close_position(self, price, new_position):\n",
    "        reward = 0.0\n",
    "        if self.current_position != 0:\n",
    "            change = price - self.position_entry_price\n",
    "            if self.current_position == -1:\n",
    "                change *= -1\n",
    "            reward = change / self.position_entry_price\n",
    "\n",
    "        self.current_position = new_position\n",
    "        self.position_entry_price = price\n",
    "        self.time_in_position = 1\n",
    "        return reward\n",
    "\n",
    "    def calc_unrealized_pnl(self):\n",
    "        if self.current_position == 0:\n",
    "            return 0.0\n",
    "        current_price = self.market_data[self.current_step, 0]\n",
    "        change = current_price - self.position_entry_price\n",
    "        return (change / self.position_entry_price) * (1 if self.current_position == 1 else -1)\n",
    "\n",
    "    def calc_drawdown(self):\n",
    "        peak = np.max(self.equity_curve)\n",
    "        current = self.equity_curve[-1]\n",
    "        return (peak - current) / peak if peak > 0 else 0.0\n",
    "\n",
    "    def sharpe_bonus(self):\n",
    "        returns = np.array(self.returns_history[-20:])\n",
    "        if len(returns) < 2:\n",
    "            return 0.0\n",
    "        mean = np.mean(returns)\n",
    "        std = np.std(returns)\n",
    "        sharpe = mean / (std + 1e-8)\n",
    "        return sharpe\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        reward = 0.0\n",
    "        reward += self.unrealized_pnl * 0.1   # scaled pnl reward\n",
    "        reward -= self.calc_drawdown() * 0.2  # penalize risk\n",
    "        reward += self.sharpe_bonus()         # consistency bonus\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd76a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "class EpisodeLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_data = []\n",
    "\n",
    "    def _on_step(self):\n",
    "        info = self.locals.get('infos', [{}])[0]\n",
    "        if 'episode' in info:\n",
    "            self.episode_data.append(info['episode'])\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        df = pd.DataFrame(self.episode_data)\n",
    "        df.to_csv('agent_journal.csv', index=False)\n",
    "        print(\"✅ Saved trading journal to agent_journal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e5e3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MarketWindowBuilder:\n",
    "    def __init__(self, df: pd.DataFrame, context_days=40, target_days=20, ticker_col='symbol', date_col='date'):\n",
    "        self.df = df.copy()\n",
    "        self.context_days = context_days\n",
    "        self.target_days = target_days\n",
    "        self.ticker_col = ticker_col\n",
    "        self.date_col = date_col\n",
    "\n",
    "    def generate_windows(self):\n",
    "        windows = []\n",
    "        grouped = self.df.groupby(self.ticker_col)\n",
    "\n",
    "        for symbol, group in grouped:\n",
    "            group = group.sort_values(self.date_col).reset_index(drop=True)\n",
    "            total_days = self.context_days + self.target_days\n",
    "\n",
    "            for start_idx in range(0, len(group) - total_days):\n",
    "                context = group.iloc[start_idx : start_idx + self.context_days].copy()\n",
    "                target = group.iloc[start_idx + self.context_days : start_idx + total_days].copy()\n",
    "\n",
    "                windows.append({\n",
    "                    'symbol': symbol,\n",
    "                    'context': context.reset_index(drop=True),\n",
    "                    'target': target.reset_index(drop=True)\n",
    "                })\n",
    "\n",
    "        return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e1c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from market_window_builder import MarketWindowBuilder\n",
    "#from self_aware_trading_env import SelfAwareTradingEnv\n",
    "import pandas as pd\n",
    "\n",
    "class WindowedTrainingLoader:\n",
    "    def __init__(self, raw_ohlcv_df: pd.DataFrame, context_days=40, target_days=20):\n",
    "        self.builder = MarketWindowBuilder(raw_ohlcv_df, context_days, target_days)\n",
    "        self.windows = self.builder.generate_windows()\n",
    "\n",
    "    def get_training_environments(self):\n",
    "        envs = []\n",
    "        for window in self.windows:\n",
    "            context_df = window['context']  # Can be logged or passed to the agent's memory\n",
    "            target_df = window['target']\n",
    "            env = SelfAwareTradingEnv(ohlcv_df=target_df, episode_length=len(target_df))\n",
    "            envs.append({\n",
    "                'context': context_df,\n",
    "                'target_env': env\n",
    "            })\n",
    "        return envs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b99c6",
   "metadata": {},
   "source": [
    "| Component         | Purpose                                                   | Use Cases / Benefits                                    |\n",
    "| ----------------- | --------------------------------------------------------- | ------------------------------------------------------- |\n",
    "| `advantage_head`  | Predicts how much **edge** the agent expects from a state | Detect market opportunities or stale/noisy environments |\n",
    "| `confidence_head` | Predicts how **confident** the agent is in its decision   | Suppress overtrading in noisy periods                   |\n",
    "| Logging those     | Helps you **analyze** what the agent sees as risky, easy  | Visualize uncertainty over time or by ticker            |\n",
    "\n",
    "### Dev note: \n",
    "Decidi alimentar observation state com confidence_score para o cabrão se autoregular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d4ecd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, seq_len=60, embed_dim=64, n_heads=4, n_layers=2):\n",
    "        super().__init__(observation_space, features_dim=embed_dim + 1)  # +1 for confidence\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.input_dim = observation_space.shape[0]\n",
    "        self.embedding = nn.Linear(self.input_dim, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Auto avaliação\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Confidence between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        B = obs.shape[0]\n",
    "        if obs.dim() == 2:\n",
    "            obs = obs.unsqueeze(1)\n",
    "\n",
    "        x = self.embedding(obs)\n",
    "        if x.shape[1] < self.seq_len:\n",
    "            pad_len = self.seq_len - x.shape[1]\n",
    "            padding = torch.zeros((B, pad_len, self.embed_dim), device=x.device)\n",
    "            x = torch.cat([padding, x], dim=1)\n",
    "        elif x.shape[1] > self.seq_len:\n",
    "            x = x[:, -self.seq_len:, :]\n",
    "\n",
    "        x = x + self.pos_embedding[:, :self.seq_len, :]\n",
    "        x = self.encoder(x)\n",
    "        x_last = x[:, -1]\n",
    "\n",
    "        confidence = self.confidence_head(x_last)\n",
    "        return torch.cat([x_last, confidence], dim=-1)  # shape: (B, embed_dim + 1)\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            features_extractor_class=TransformerFeatureExtractor,\n",
    "            features_extractor_kwargs=dict(seq_len=60, embed_dim=64, n_heads=4, n_layers=2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8770d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "class TrainingRunner:\n",
    "    def __init__(self, raw_ohlcv_df, model_save_path=\"self_aware_agent\", total_timesteps=10_000):\n",
    "        self.raw_df = raw_ohlcv_df\n",
    "        self.model_save_path = model_save_path\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def run(self):\n",
    "        loader = WindowedTrainingLoader(self.raw_df)\n",
    "        episodes = loader.get_training_environments()\n",
    "\n",
    "        # Resume logic\n",
    "        journal_path = \"training_journal.csv\"\n",
    "        if os.path.exists(journal_path):\n",
    "            completed_windows = set(pd.read_csv(journal_path)['window'].unique())\n",
    "            all_metrics = [pd.read_csv(journal_path)]\n",
    "        else:\n",
    "            completed_windows = set()\n",
    "            all_metrics = []\n",
    "\n",
    "        for i, episode in enumerate(tqdm(episodes, desc=\"Training windows\")):\n",
    "            window_id = i + 1\n",
    "            model_path = os.path.join(self.model_save_path, f\"agent_window_{window_id}.zip\")\n",
    "\n",
    "            if window_id in completed_windows or os.path.exists(model_path):\n",
    "                print(f\"⏩ Skipping window {window_id}, already completed.\")\n",
    "                continue\n",
    "\n",
    "            context_df = episode['context']\n",
    "            env = DummyVecEnv([lambda: Monitor(episode['target_env'])])\n",
    "\n",
    "            model = RecurrentPPO(\n",
    "                policy=TransformerPolicy,\n",
    "                env=env,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            print(f\"\\n🧠 Training window {window_id}/{len(episodes)}\")\n",
    "\n",
    "            callback = EpisodeLoggerCallback(log_interval=1)\n",
    "            model.learn(total_timesteps=self.total_timesteps, callback=callback)\n",
    "\n",
    "            os.makedirs(self.model_save_path, exist_ok=True)\n",
    "            model.save(model_path)\n",
    "\n",
    "            # Append and immediately persist metrics for crash resilience\n",
    "            episode_df = pd.DataFrame(callback.episode_data)\n",
    "            episode_df['window'] = window_id\n",
    "            all_metrics.append(episode_df)\n",
    "\n",
    "            pd.concat(all_metrics, ignore_index=True).to_csv(journal_path, index=False)\n",
    "\n",
    "        print(\"\\n✅ Training complete. Journal saved to 'training_journal.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6318ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notification = Notify('Self aware trading agent')\n",
    "notification.success('Lets go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1220407",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_base_dataframe()\n",
    "raw_df = raw_df.sort_values(\"date\").reset_index(drop=True)\n",
    "raw_df = raw_df[(raw_df['date']>=\"2024-06-01\") & (raw_df['date']<'2025-06-01')]\n",
    "# Replace with your actual data loading pipeline\n",
    "#raw_df = pd.read_csv(\"data/sp500_ohlcv.csv\", parse_dates=[\"date\"])\n",
    "#raw_df = raw_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# === Run the self-aware transformer agent training ===\n",
    "runner = TrainingRunner(\n",
    "    raw_ohlcv_df=raw_df,\n",
    "    model_save_path=\"self_aware_transformer\",\n",
    "    total_timesteps=10_000 \n",
    ")\n",
    "notification.info('Train started')\n",
    "runner.run()\n",
    "notification.info('Train complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_base_dataframe()\n",
    "raw_df = raw_df.sort_values(\"date\").reset_index(drop=True)\n",
    "raw_df = raw_df[(raw_df['date']>=\"2024-06-01\") & (raw_df['date']<'2025-06-01')]\n",
    "notification.info('Test started')\n",
    "eval_runner = EvaluationRunner(raw_df)\n",
    "eval_runner.run()\n",
    "notification.info('Test complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368ed7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "484b1c41",
   "metadata": {},
   "source": [
    "Hell yes — we’ve built a killer POC already, but now we enter the fun zone: **tightening screws, pushing limits, and future-proofing**.\n",
    "\n",
    "Here are my **top strategic suggestions** to improve the system:\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 1. **Allow Agent to See the Context Directly**\n",
    "\n",
    "Right now the agent **only trains on `target_df`** — even though the Transformer *could* learn from the `context_df`.\n",
    "\n",
    "### 🔧 Option:\n",
    "\n",
    "Concatenate `context_df + target_df` into one episode:\n",
    "\n",
    "* Train the agent only on rewards from the `target_df` portion (e.g. via masking or zero rewards during context)\n",
    "* Allows the Transformer to **build internal market memory naturally**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 2. **Auxiliary Heads: Meta-Predictions**\n",
    "\n",
    "Add small auxiliary outputs to the Transformer that:\n",
    "\n",
    "* Predict future volatility\n",
    "* Estimate next step reward\n",
    "* Classify regime (trend, chop, revert)\n",
    "\n",
    "Why?\n",
    "\n",
    "> Forces the encoder to learn **richer representations** beyond just immediate actions.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 3. **Regime-Based Curriculum**\n",
    "\n",
    "Instead of sampling all episodes equally:\n",
    "\n",
    "* Stratify by volatility or trend\n",
    "* Train easier episodes first\n",
    "* Slowly introduce difficult / noisy environments\n",
    "\n",
    "You can also **balance the regime mix** so the agent doesn't overfit to \"easy\" windows.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 4. **Relative Advantage Evaluation**\n",
    "\n",
    "Instead of just measuring reward:\n",
    "\n",
    "* Compare to baseline strategies (random, hold)\n",
    "* Define `agent_advantage = reward_agent - reward_baseline`\n",
    "\n",
    "You already built the machinery — just run each `target_df` twice and subtract.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 5. **RL Memory Injection (optional but powerful)**\n",
    "\n",
    "If we move to **RecurrentPPO with LSTM memory**:\n",
    "\n",
    "* Let the agent process `context_df` passively (no reward, no action)\n",
    "* Then start the episode normally with memory *initialized*\n",
    "\n",
    "This mimics how traders study history before acting.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ 6. **Online Deployment Skeleton**\n",
    "\n",
    "Later, you could wire this up for:\n",
    "\n",
    "* Daily retraining using recent data\n",
    "* Live execution of agent policy on top tickers\n",
    "* Continual journaling + self-analysis\n",
    "\n",
    "Use a rolling horizon evaluation loop with `context → target` sliding forward by 1 week or 1 day.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Bonus Tactical Tips\n",
    "\n",
    "| Area              | Suggestion                                                      |\n",
    "| ----------------- | --------------------------------------------------------------- |\n",
    "| Logging           | Add `num_trades`, `avg_holding_time`, `win_rate` to journal     |\n",
    "| Feature Expansion | Add cross-ticker info: sector trend, market index return        |\n",
    "| Reward Design     | Add `position_penalty` to discourage churning                   |\n",
    "| Model Variants    | Try `Transformer + LSTM`, or `Temporal Convolution + Attention` |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Want to act on any of these now?\n",
    "\n",
    "I recommend either:\n",
    "\n",
    "1. Concatenating `context + target` into single episodes (with smart reward masking)\n",
    "2. Adding relative performance benchmarking (agent vs. random/hold)\n",
    "\n",
    "Let me know and I’ll implement it right now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782e4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
