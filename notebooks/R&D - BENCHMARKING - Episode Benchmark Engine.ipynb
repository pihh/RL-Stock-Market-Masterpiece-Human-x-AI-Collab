{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be3f765",
   "metadata": {},
   "source": [
    "This is gold, Pi â€” thank you for laying it out so clearly. What you built is already stronger than many academic meta-RL setups. Now letâ€™s **merge the best of your past success with this current Battleground framework** and push it to the next level.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Current Goal\n",
    "\n",
    "**Predict, with â‰¥75% confidence, whether a given RL training episode is transferable.**\n",
    "\n",
    "To do that, we need a **rich and expressive feature space** that captures **structure, uncertainty, dynamics, and regime information** â€” just like your Stock-Month Predictability Study.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What We Already Have\n",
    "\n",
    "From your current `result_df`, we already extract:\n",
    "\n",
    "| Group                 | Features                                                                 |\n",
    "| --------------------- | ------------------------------------------------------------------------ |\n",
    "| Statistical Moments   | `mean_return`, `std_return`, `skew_return`, `kurtosis_return`, `entropy` |\n",
    "| Price Trend           | `return_trend`, `ewm_mean_return`                                        |\n",
    "| Chaos / Regime        | `hurst`, `adf_stat`, `adf_pval`                                          |\n",
    "| Risk & Reward Metrics | `volatility`, `max_drawdown`, `sharpe`, `sortino`, `calmar`              |\n",
    "| Agent Diagnostics     | `success_trades`, `action_hold_ratio`, `action_long_ratio`               |\n",
    "| Outcome Labels        | `score_train`, `score_test`, `advantage_test`, `transfer_delta`          |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Letâ€™s Extend: Feature Suggestions\n",
    "\n",
    "Below are **10 additional meta-features** we can compute **either now or as next step**, all in PyTorch/Numpy-friendly form:\n",
    "\n",
    "| Feature Name               | Why Add It?                                                                |\n",
    "| -------------------------- | -------------------------------------------------------------------------- |\n",
    "| `resid_std`                | From RF prediction of t+1 returns â†’ measures noise                         |\n",
    "| `resid_skew`, `resid_kurt` | Shape of the error â†’ asymmetry or tails                                    |\n",
    "| `resid_acf1`               | Temporal memory in prediction error                                        |\n",
    "| `ljung_pval`               | Statistical confirmation of noise/randomness                               |\n",
    "| `cv_r2`                    | Proxy for model learnability/predictability                                |\n",
    "| `garch_volatility`         | Conditional volatility â‡’ market stress estimation                          |\n",
    "| `change_point_count`       | Regime switch count (e.g. via ruptures or cusum)                           |\n",
    "| `rolling_adf_pval`         | Stationarity evolution over time                                           |\n",
    "| `forecast_entropy`         | Entropy of predictions from RF or AE                                       |\n",
    "| `price_entropy_peak`       | Local entropy spike detection before regime breaks (good for online usage) |\n",
    "\n",
    "These build on your previous success and aim at:\n",
    "\n",
    "* **Residual structure**\n",
    "* **Volatility structure**\n",
    "* **Forecast structure**\n",
    "* **Regime changes**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Architecture Suggestion (Final Plan)\n",
    "\n",
    "```bash\n",
    "[Episode -> Raw OHLCV]\n",
    "         |\n",
    "         v\n",
    "[Feature Extractor (Meta + Residual + Chaos)]\n",
    "         |\n",
    "         v\n",
    "[Representation Learner (AE, Transformer Encoder, etc)]\n",
    "         |\n",
    "         v\n",
    "[Predictor (Classifier or Ranker)]\n",
    "         |\n",
    "         v\n",
    "[Score: Learnability + Transferability + Difficulty]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Action Plan (Ordered)\n",
    "\n",
    "**Phase 1 â€“ Today**\n",
    "\n",
    "1. âœ… Implement `EpisodeMetaFeatureExtractor` (done or in progress)\n",
    "2. âœ… Implement `TradingEnvironmentBattleground` (done!)\n",
    "3. ðŸ” Add new diagnostics: `resid_std`, `resid_acf1`, `cv_r2`, `ljung_pval`\n",
    "4. ðŸ§  Train `EpisodeTransferabilityPredictor` with new features\n",
    "\n",
    "**Phase 2 â€“ Next**\n",
    "5\\. ðŸ§¬ Add `AutoencoderRepresentation` wrapper\n",
    "6\\. ðŸ”„ Train contrastive ranking model: \"Is A > B?\"\n",
    "7\\. â± Evaluate with out-of-time validation (e.g., train on 2023 Q1, test on Q2)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco SÃ¡\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.defaults import RANDOM_SEEDS\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"trading_environment_development\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "MARKET_FEATURES = ['close']\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df_transfer.csv\"\n",
    "\n",
    "MODEL_PATH = CHECKPOINT_DIR+\"/episode_quality_model.pkl\"\n",
    "MARKET_FEATURES.sort()\n",
    "SEEDS.sort()\n",
    "\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa0446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_NAME = \"stock_universe_predictability_selection__MetaFeatures__MetaRlLabeling\"\n",
    "FEATURES_PATH = f\"../data/cache/features_{EXPERIENCE_NAME}.pkl\"\n",
    "TARGETS_PATH = f\"../data/cache/targets_{EXPERIENCE_NAME}.pkl\"\n",
    "META_PATH = f\"../data/cache/meta_{EXPERIENCE_NAME}.pkl\"\n",
    "RL_LABELS_PATH = \"../data/cache/meta_rl_labels_stock_universe_predictability_selection__MetaFeatures__MetaRlLabeling__6293649262173480064.pkl\"\n",
    "\n",
    "excluded_tickers=['CEG', 'GEHC', 'GEV', 'KVUE', 'SOLV']\n",
    "excluded_tickers.sort()\n",
    "#tickers = TOP2_STOCK_BY_SECTOR\n",
    "\n",
    "config={\n",
    "    \"regressor\":\"RandomForestRegressor\",\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\":314,\n",
    "    \"transaction_cost\":0\n",
    "}\n",
    "run_settings={\n",
    "    \"excluded_tickers\": excluded_tickers,\n",
    "    \"min_samples\": 10,\n",
    "    \"cv_folds\": 3,\n",
    "    \"lags\": 5,\n",
    "    \"start_date\":\"2022-01-01\",\n",
    "    \"end_date\":\"2025-01-01\",\n",
    "    \"seed\":314,\n",
    "    \"episode_length\":18,\n",
    "    \"noise_feature_cols\": [\"return_1d\", \"volume\"]  ,\n",
    "\n",
    "    \"train_steps\": 300,\n",
    "    \"min_ep_len\" : 18\n",
    "}\n",
    "\n",
    "# Config section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47fba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6999fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_benchmarking_engine.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Dict, List\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2\n",
    "\n",
    "# Updated utility functions\n",
    "\n",
    "def generate_lagged_features(df, config):\n",
    "    df = df.copy()\n",
    "    if config.get('return'):\n",
    "        df['return_1d'] = df['close'].pct_change()\n",
    "    if config.get('volume'):\n",
    "        df['volume_1d'] = df['volume'].pct_change()\n",
    "    for lag in range(1, config.get('lags', 0) + 1):\n",
    "        df[f'return_lag_{lag}'] = df['return_1d'].shift(lag)\n",
    "        df[f'volume_lag_{lag}'] = df['volume_1d'].shift(lag)\n",
    "    return df.dropna()\n",
    "\n",
    "def compute_sharpe_ratio(returns):\n",
    "    mean = np.mean(returns)\n",
    "    std = np.std(returns)\n",
    "    return mean / std if std > 0 else np.nan\n",
    "\n",
    "def compute_sortino_ratio(returns):\n",
    "    mean = np.mean(returns)\n",
    "    downside_std = np.std(returns[returns < 0])\n",
    "    return mean / downside_std if downside_std > 0 else np.nan\n",
    "\n",
    "def compute_calmar_ratio(returns):\n",
    "    cum_returns = np.cumprod(1 + returns)\n",
    "    drawdown = np.max(cum_returns) - np.min(cum_returns)\n",
    "    mean_return = np.mean(returns)\n",
    "    return mean_return / drawdown if drawdown > 0 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Utility functions with real logic for agent and oracle evaluation\n",
    "def compute_agent_metrics(model, env, random=False):\n",
    "    obs, _ = env.reset()\n",
    "    done, actions, values = False, [], []\n",
    "    while not done:\n",
    "        action = env.action_space.sample() if random else model.predict(obs, deterministic=True)[0]\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        actions.append(action)\n",
    "\n",
    "    values = np.array(env.env.wallet_progress)\n",
    "    if len(values) < 2:\n",
    "        return {}\n",
    "   \n",
    "    returns = values-1 #np.diff(values) #/ (values[:-1] + 1e-9)\n",
    "    negative_returns = returns[returns < 0] if len(returns[returns < 0]) > 0 else np.array([1e-9])\n",
    "    action_probs = np.bincount(actions, minlength=2) / (len(actions) + 1e-9)\n",
    "    drawdowns = values / np.maximum.accumulate(values)\n",
    "    max_drawdown = drawdowns.min() - 1 if len(drawdowns) > 0 else -1\n",
    "    success_trades = env.env.success_trades = 0\n",
    "    failed_trades = env.env.failed_trades = 0\n",
    "    total_trades = env.env.total_trades = 0\n",
    "  \n",
    "    sharpe = 0\n",
    "    sortino = 0\n",
    "    calmar = 0\n",
    "    if returns.std() !=0:\n",
    "        sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "        sortino = returns.mean() / (negative_returns.std() + 1e-9) * np.sqrt(252)\n",
    "   \n",
    "    return {\n",
    "        \"reward\": values[-1] - values[0],\n",
    "        \"volatility\": returns.std(),\n",
    "        \"entropy\": -np.sum(action_probs * np.log2(action_probs + 1e-9)),\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe, #returns.mean() / (returns.std() + 1e-9) * np.sqrt(252),\n",
    "        \"sortino\": sortino, #returns.mean() / (negative_returns.std() + 1e-9) * np.sqrt(252),\n",
    "        \"calmar\": returns.mean() / abs(max_drawdown + 1e-9),\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(np.array(actions) == 0),\n",
    "        \"action_long_ratio\": np.mean(np.array(actions) == 1),\n",
    "        \"cumulative_return\": values[-1] / values[0] - 1 if values[0] != 0 else 0\n",
    "    }\n",
    "\n",
    "def compute_oracle_metrics(env):\n",
    "    obs, _ = env.reset()\n",
    "    done, actions, values = False, [], []\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        curr_idx = env.env.step_idx\n",
    "        next_idx = min(curr_idx + 1, len(env.env.prices) - 1)\n",
    "        curr_price = env.env.prices[curr_idx]\n",
    "        next_price = env.env.prices[next_idx]\n",
    "        price_diff = next_price - curr_price\n",
    "        action = 1 if price_diff > 0 else 0\n",
    "        #action = episode_df.iloc[env.current]  # oracle always assumes uptrend (long)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        actions.append(action)\n",
    "\n",
    "    values = np.array(env.env.wallet_progress)\n",
    "    if len(values) < 2:\n",
    "        return {}\n",
    "    #print(values,actions)\n",
    "    returns = values #np.diff(values) / (values[:-1] + 1e-9)\n",
    "    negative_returns = returns[returns < 1] if len(returns[returns < 1]) > 0 else np.array([1e-9])\n",
    "    action_probs = np.bincount(actions, minlength=2) / (len(actions) + 1e-9)\n",
    "    drawdowns = values / np.maximum.accumulate(values)\n",
    "    max_drawdown = drawdowns.min() - 1 if len(drawdowns) > 0 else -1\n",
    "\n",
    "    success_trades = 0\n",
    "    fail_trades = 0\n",
    "    total_trades = 0\n",
    "  \n",
    "\n",
    "    return {\n",
    "        \"oracle_volatility\": returns.std(),\n",
    "        \"oracle_entropy\": -np.sum(action_probs * np.log2(action_probs + 1e-9)),\n",
    "        \"oracle_max_drawdown\": max_drawdown,\n",
    "        \"oracle_sharpe\": returns.mean() / (returns.std() + 1e-9) * np.sqrt(252),\n",
    "        \"oracle_sortino\": returns.mean() / (negative_returns.std() + 1e-9) * np.sqrt(252),\n",
    "        \"oracle_calmar\": returns.mean() / abs(max_drawdown + 1e-9),\n",
    "        \"oracle_success_trades\": success_trades,\n",
    "        \"oracle_action_hold_ratio\": np.mean(np.array(actions) == 0),\n",
    "        \"oracle_action_long_ratio\": np.mean(np.array(actions) == 1),\n",
    "        \"oracle_cumulative_return\": values[-1] / values[0] - 1 if values[0] != 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def compute_additional_diagnostics(env):\n",
    "    df = env.env.episode_df\n",
    "    df = generate_lagged_features(df.copy(), {'return': True, 'lags': 5})\n",
    "    df = df.dropna()\n",
    "    X = df[[f'return_lag_{i}' for i in range(1, 6)]].values\n",
    "    y = df['return_1d'].values\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    residuals = y - model.predict(X)\n",
    "    ljung_pval = acorr_ljungbox(residuals, lags=[5], return_df=True).iloc[0]['lb_pvalue']\n",
    "    return {\n",
    "        'resid_std': residuals.std(),\n",
    "        'resid_skew': skew(residuals),\n",
    "        'resid_kurtosis': kurtosis(residuals),\n",
    "        'ljung_pval': ljung_pval\n",
    "    }\n",
    "\n",
    "class EpisodeBenchmarkingEngine:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 experiment_name: str= \"episode_benchmark_engine\",\n",
    "                 agent_classes: List = [PPO, A2C],\n",
    "                 seeds: List[int] = RANDOM_SEEDS,\n",
    "                 train_steps: List[int] = [10_000], #, 50_000, 100_000, 200_000],\n",
    "                 n_timesteps: int = 120,\n",
    "                 lookback: int = 20,\n",
    "                 min_valid_length: int = 100,\n",
    "                 market_features=['close','volume'],\n",
    "                 feature_config: Dict = None):\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.experiment_name = experiment_name\n",
    "        self.agent_classes = agent_classes\n",
    "        self.seeds = seeds\n",
    "        self.train_steps = train_steps\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.lookback = lookback\n",
    "        self.min_valid_length = min_valid_length\n",
    "        self.market_features = market_features\n",
    "        self.feature_config = feature_config or {'return': True, 'volume': True, 'lags': 5}\n",
    "        self.envs = [PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2]\n",
    "        self.base_path = f\"data/experiments/{experiment_name}\"\n",
    "        self.checkpoint_path = os.path.join(self.base_path, \"checkpoints\")\n",
    "        self.result_path = os.path.join(self.base_path, \"meta_df_transfer.csv\")\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "    def _get_config_hash(self, config: Dict) -> str:\n",
    "        raw = json.dumps(config, sort_keys=True)\n",
    "        return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "    def _save_model(self, model, config_hash, agent_name, seed):\n",
    "        model_path = os.path.join(self.checkpoint_path, f\"{agent_name}_{config_hash}_{seed}.zip\")\n",
    "        model.save(model_path)\n",
    "\n",
    "    def _load_seen_hashes(self) -> set:\n",
    "        if os.path.exists(self.result_path):\n",
    "            df = pd.read_csv(self.result_path)\n",
    "            return set(zip(df['config_hash'], df['agent_name'], df['seed']))\n",
    "        return set()\n",
    "\n",
    "    def train(self, ticker: str):\n",
    "        df_ticker = self.df[self.df['symbol'] == ticker].sort_values(\"date\").reset_index(drop=True)\n",
    "        months = df_ticker['date'].dt.to_period(\"M\").unique()[24:]\n",
    "\n",
    "        records = []\n",
    "        seen_hashes = self._load_seen_hashes()\n",
    "        market_features = self.market_features\n",
    "        for m in tqdm(months[1:-1], desc=f\"Benchmarking {ticker}\"):\n",
    "            train_end_date = pd.Timestamp(m.start_time)\n",
    "            train_start_idx = df_ticker[df_ticker['date'] < train_end_date].index.max() - self.n_timesteps\n",
    "            test_start_idx = df_ticker[df_ticker['date'] < train_end_date].index.max() - self.lookback\n",
    "\n",
    "            if train_start_idx - self.lookback < 0 or test_start_idx + self.n_timesteps >= len(df_ticker):\n",
    "                continue\n",
    "            _df = df_ticker.copy() #.iloc[train_start_idx - self.lookback: train_start_idx + self.n_timesteps].copy()\n",
    "            #df_train = df_ticker.iloc[train_start_idx - self.lookback: train_start_idx + self.n_timesteps].copy()\n",
    "            #df_test = df_ticker.iloc[test_start_idx - self.lookback: test_start_idx + self.n_timesteps].copy()\n",
    "\n",
    "            #df_train = generate_lagged_features(df_train, self.feature_config)\n",
    "            #df_test = generate_lagged_features(df_test, self.feature_config)\n",
    "            _df = generate_lagged_features(_df, self.feature_config)\n",
    "            df_train = _df.iloc[: train_start_idx + self.n_timesteps].copy()\n",
    "            df_test = _df.iloc[: test_start_idx + self.n_timesteps].copy()\n",
    "            for agent_cls in self.agent_classes:\n",
    "                for seed in self.seeds:\n",
    "                    for steps in self.train_steps:\n",
    "                        for env_cls in self.envs:\n",
    "                            env_version = 'v'+str(env_cls.__version__)\n",
    "                            config = {\n",
    "                                'ticker': ticker,\n",
    "                                'train_idx': int(train_start_idx),\n",
    "                                'test_idx': int(test_start_idx),\n",
    "                                'agent_policy':'MlpPolicy',\n",
    "                                'agent_name': agent_cls.__name__,\n",
    "                                'env_version':env_version,\n",
    "                                'timesteps': steps,\n",
    "                                'seed': seed,\n",
    "                                'feature_config': self.feature_config,\n",
    "                                'market_features':json.dumps(market_features)\n",
    "                            }\n",
    "                            config_hash = self._get_config_hash(config)\n",
    "\n",
    "                            if (config_hash, agent_cls.__name__, seed) in seen_hashes:\n",
    "                                continue\n",
    "\n",
    "                            env_train = Monitor(PositionTradingEnv(df_train, ticker , market_features=market_features, n_timesteps=self.n_timesteps, seed=seed, start_idx=train_start_idx))\n",
    "                            model = agent_cls(\"MlpPolicy\", env_train, verbose=0, seed=seed)\n",
    "                            model.learn(total_timesteps=steps)\n",
    "                            self._save_model(model, config_hash, agent_cls.__name__, seed)\n",
    "\n",
    "                            train_metrics = compute_agent_metrics(model, env_train)\n",
    "                            rand_metrics = compute_agent_metrics(None, env_train, random=True)\n",
    "\n",
    "                            env_test = Monitor(PositionTradingEnv(df_test, ticker,market_features=market_features, n_timesteps=self.n_timesteps, seed=seed, start_idx=test_start_idx))\n",
    "                            test_metrics = compute_agent_metrics(model, env_test)\n",
    "                            rand_test_metrics = compute_agent_metrics(None, env_test, random=True)\n",
    "\n",
    "                            oracle_train = compute_oracle_metrics(env_train)\n",
    "                            oracle_test = compute_oracle_metrics(env_test)\n",
    "\n",
    "                            record = {\n",
    "                                'ticker': ticker,\n",
    "                                'config_hash': config_hash,\n",
    "                                'agent_name': agent_cls.__name__,\n",
    "                                'env_version':env_version,\n",
    "                                'seed': seed,\n",
    "                                'month': m,\n",
    "                                #'train_reward': train_metrics['reward'],\n",
    "                                #'test_reward': test_metrics['reward'],\n",
    "                                #'train_random': rand_metrics['reward'],\n",
    "                                #'test_random': rand_test_metrics['reward'],\n",
    "                                'advantage_train': train_metrics['reward'] - rand_metrics['reward'],\n",
    "                                'advantage_test': test_metrics['reward'] - rand_test_metrics['reward'],\n",
    "                                'transfer_delta': test_metrics['reward'] - train_metrics['reward'],\n",
    "                                'market_features':json.dumps(market_features),\n",
    "                                **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
    "                                **{f\"test_{k}\": v for k, v in test_metrics.items()},\n",
    "                                **{f\"train_random_{k}\": v for k, v in rand_metrics.items()},\n",
    "                                **{f\"test_random_{k}\": v for k, v in rand_test_metrics.items()},\n",
    "                                **{f\"oracle_train_{k}\": v for k, v in oracle_train.items()},\n",
    "                                **{f\"oracle_test_{k}\": v for k, v in oracle_test.items()},\n",
    "                                #**oracle_train,\n",
    "                                #**oracle_test,\n",
    "                                #**test_metrics,\n",
    "                                **compute_additional_diagnostics(env_test)\n",
    "                            }\n",
    "                            records.append(record)\n",
    "\n",
    "                if records:\n",
    "                    df_new = pd.DataFrame(records)\n",
    "                    if os.path.exists(self.result_path):\n",
    "                        df_existing = pd.read_csv(self.result_path)\n",
    "                        df_all = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                    else:\n",
    "                        df_all = df_new\n",
    "                    df_all.to_csv(self.result_path, index=False)\n",
    "                    print(f\"[âœ“] Results saved to {self.result_path}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        df = pd.read_csv(self.result_path)\n",
    "        print(\"[INFO] Transfer success rate:\", (df['transfer_delta'] > 0).mean())\n",
    "        plt.hist(df['transfer_delta'], bins=50)\n",
    "        plt.title(\"Transfer Delta Distribution\")\n",
    "        plt.show()\n",
    "        return df\n",
    "\n",
    "    def predict(self):\n",
    "        print(\"[TODO] Implement meta-feature-based predictor\")\n",
    "\n",
    "    def report(self):\n",
    "        print(\"[TODO] Implement full report generation with diagnostics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1632ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = EpisodeBenchmarkingEngine(OHLCV_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb254410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking AAPL:   0%|          | 0/16 [06:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ebm\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 249\u001b[0m, in \u001b[0;36mEpisodeBenchmarkingEngine.train\u001b[1;34m(self, ticker)\u001b[0m\n\u001b[0;32m    247\u001b[0m env_train \u001b[38;5;241m=\u001b[39m Monitor(PositionTradingEnv(df_train, ticker , market_features\u001b[38;5;241m=\u001b[39mmarket_features, n_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_timesteps, seed\u001b[38;5;241m=\u001b[39mseed, start_idx\u001b[38;5;241m=\u001b[39mtrain_start_idx))\n\u001b[0;32m    248\u001b[0m model \u001b[38;5;241m=\u001b[39m agent_cls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m--> 249\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39msteps)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_model(model, config_hash, agent_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, seed)\n\u001b[0;32m    252\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m compute_agent_metrics(model, env_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    312\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    313\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    314\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    315\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    316\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    317\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    318\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:278\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     adam(\n\u001b[0;32m    247\u001b[0m         params_with_grad,\n\u001b[0;32m    248\u001b[0m         grads,\n\u001b[0;32m    249\u001b[0m         exp_avgs,\n\u001b[0;32m    250\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    251\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    252\u001b[0m         state_steps,\n\u001b[0;32m    253\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    254\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    255\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    256\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    257\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    258\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    259\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    260\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    261\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    262\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    263\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    264\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    265\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    266\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    267\u001b[0m         decoupled_weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoupled_weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m func(\n\u001b[0;32m    934\u001b[0m     params,\n\u001b[0;32m    935\u001b[0m     grads,\n\u001b[0;32m    936\u001b[0m     exp_avgs,\n\u001b[0;32m    937\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    938\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    939\u001b[0m     state_steps,\n\u001b[0;32m    940\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    941\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    942\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    943\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    944\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    945\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    946\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    947\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    948\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    949\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    950\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    951\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    952\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[0;32m    953\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    523\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    527\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ebm.train('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb991302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d626f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd2764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb713670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50e87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03597b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f723a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
