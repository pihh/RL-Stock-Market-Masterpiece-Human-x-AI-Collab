{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a83a608",
   "metadata": {},
   "source": [
    "# 01 - Environment Reward System Ablation Test\n",
    "\n",
    "1. Compute rolling predictability metrics for each ticker\n",
    "2. Visualize and compare scores across universe and time\n",
    "3. Select top-N most “learnable” tickers for RL agent\n",
    "4. Document all decisions, assumptions, and open questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Ablation: all_features ====\n",
      "\n",
      "==== Ablation: market_only ====\n"
     ]
    }
   ],
   "source": [
    "import jupyter \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from stable_baselines3 import PPO\n",
    "from src.env.base_trading_env import (\n",
    "    CumulativeTradingEnv,\n",
    ")\n",
    "boot()\n",
    "\n",
    "# === 1. Feature sets ===\n",
    "MARKET_FEATURES = [\n",
    "    \"day_of_month\", \"day_of_week\", \"order_flow\", \"candle_body\",\n",
    "    \"upper_shadow\", \"lower_shadow\", \"price_change\", \"candle_change\",\n",
    "    \"order_flow_change\", \"overnight_price_change\", \"volume_change\",\n",
    "    \"vwap_change\", \"trade_count_change\", \"return_1d\", \"vix_norm\", \"market_return_1d\"\n",
    "]\n",
    "INTERNAL_FEATURES = [\n",
    "    \"position\", \"holding_period\", \"cumulative_reward\", \"pct_time\",\n",
    "    \"drawdown\", \"rel_perf\", \"unrealized_pnl\", \"entry_price\", \"time_in_position\"\n",
    "]\n",
    "\n",
    "ablation_variants = [\n",
    "    (\"all_features\", MARKET_FEATURES, INTERNAL_FEATURES),\n",
    "    (\"market_only\", MARKET_FEATURES, []),\n",
    "    (\"internal_only\", [], INTERNAL_FEATURES),\n",
    "]\n",
    "for f in INTERNAL_FEATURES:\n",
    "    ablation_variants.append((\n",
    "        f\"no_{f}\",\n",
    "        MARKET_FEATURES,\n",
    "        [i for i in INTERNAL_FEATURES if i != f]\n",
    "    ))\n",
    "\n",
    "# === 2. Experiment config ===\n",
    "EXPERIMENT_NAME = \"walkforward_ablation_base_env_internal_features\"\n",
    "EXCLUDED_TICKERS = ['CEG', 'GEHC', 'GEV', 'KVUE', 'SOLV']\n",
    "TOTAL_TIMESTEPS = 20000\n",
    "EPISODE_LENGTH = 50\n",
    "TOTAL_TRAIN_EPISODES = int(TOTAL_TIMESTEPS/EPISODE_LENGTH)+1\n",
    "TOTAL_TEST_EPISODES = 5\n",
    "SEED = 314\n",
    "\n",
    "walk_forward_splits = [\n",
    "    (\"2023-01-01\", \"2023-07-01\", \"2023-09-01\", \"2023-12-01\"),\n",
    "    (\"2024-01-01\", \"2024-07-01\", \"2024-09-01\", \"2024-12-01\"),\n",
    "]\n",
    "\n",
    "CONFIG = dict(\n",
    "    total_train_episodes=TOTAL_TRAIN_EPISODES,\n",
    "    total_test_episodes=TOTAL_TEST_EPISODES,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    seed=SEED,\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    agent=\"PPO\"\n",
    ")\n",
    "\n",
    "# === 3. Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# === 4. Walk-forward evaluation function ===\n",
    "def evaluate_feature_set(feature_cols, internal_features, config, walk_forward_splits,\n",
    "                        RL_ENV_CLASS, ohlcv_df, EXCLUDED_TICKERS,\n",
    "                        EPISODE_LENGTH, TOTAL_TRAIN_EPISODES, TOTAL_TEST_EPISODES, SEED):\n",
    "    all_rows = []\n",
    "    for split_idx, (train_start, train_end, test_start, test_end) in enumerate(walk_forward_splits):\n",
    "        # Data splits\n",
    "        df_train = ohlcv_df[(ohlcv_df['date'] >= train_start) & (ohlcv_df['date'] < train_end) & ~ohlcv_df['symbol'].isin(EXCLUDED_TICKERS)].reset_index(drop=True)\n",
    "        df_test = ohlcv_df[(ohlcv_df['date'] >= test_start) & (ohlcv_df['date'] < test_end) & ~ohlcv_df['symbol'].isin(EXCLUDED_TICKERS)].reset_index(drop=True)\n",
    "\n",
    "        def generate_episode_sequences(df, episode_length, n_episodes, excluded_tickers, seed=314):\n",
    "            rng = np.random.default_rng(seed)\n",
    "            eligible_tickers = [t for t in df['symbol'].unique() if t not in excluded_tickers]\n",
    "            sequences = []\n",
    "            for _ in range(n_episodes):\n",
    "                ticker = rng.choice(eligible_tickers)\n",
    "                stock_df = df[df['symbol'] == ticker]\n",
    "                max_start = len(stock_df) - episode_length - 1\n",
    "                if max_start < 1: continue\n",
    "                start_idx = rng.integers(0, max_start)\n",
    "                sequences.append((ticker, int(start_idx)))\n",
    "            return sequences\n",
    "\n",
    "        split_seed = int(pd.Timestamp(test_start).timestamp())\n",
    "        train_seq = generate_episode_sequences(df_train, EPISODE_LENGTH, TOTAL_TRAIN_EPISODES, EXCLUDED_TICKERS, seed=split_seed)\n",
    "        test_seq = generate_episode_sequences(df_test, EPISODE_LENGTH, TOTAL_TEST_EPISODES, EXCLUDED_TICKERS, seed=split_seed + 1)\n",
    "\n",
    "        # RL Agent Training (PPO) -- instantiate env with current features\n",
    "        train_env = RL_ENV_CLASS(df_train, feature_cols=feature_cols, internal_features=internal_features, episode_length=EPISODE_LENGTH, seed=SEED)\n",
    "        train_env.set_episode_sequence(train_seq)\n",
    "        test_env = RL_ENV_CLASS(df_test, feature_cols=feature_cols, internal_features=internal_features, episode_length=EPISODE_LENGTH, seed=SEED)\n",
    "        test_env.set_episode_sequence(test_seq)\n",
    "\n",
    "        agent = PPO(\"MlpPolicy\", train_env, verbose=0, n_steps=EPISODE_LENGTH, seed=SEED, batch_size=EPISODE_LENGTH)\n",
    "        agent.learn(total_timesteps=config['total_timesteps'])\n",
    "\n",
    "        def evaluate_env(env, agent, n_episodes, agent_type):\n",
    "            metrics = []\n",
    "            for _ in range(n_episodes):\n",
    "                obs, _ = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action, _ = agent.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, truncated, info = env.step(action)\n",
    "                info = info.copy()\n",
    "                info['agent'] = agent_type\n",
    "                metrics.append(info)\n",
    "            return metrics\n",
    "\n",
    "        rl_metrics = evaluate_env(test_env, agent, len(test_seq), \"RL\")\n",
    "\n",
    "        def evaluate_random(env, n_episodes):\n",
    "            metrics = []\n",
    "            for _ in range(n_episodes):\n",
    "                obs, _ = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = env.action_space.sample()\n",
    "                    obs, reward, done, truncated, info = env.step(action)\n",
    "                info = info.copy()\n",
    "                info['agent'] = \"Random\"\n",
    "                metrics.append(info)\n",
    "            return metrics\n",
    "\n",
    "        random_metrics = evaluate_random(test_env, len(test_seq))\n",
    "        # Store all results\n",
    "        for row in rl_metrics + random_metrics:\n",
    "            row.update({\"split\": f\"{test_start}--{test_end}\"})\n",
    "            all_rows.append(row)\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "# === 5. Run ablation study ===\n",
    "results_table = []\n",
    "for ablation_name, feature_cols, internal_features in ablation_variants:\n",
    "    print(f\"\\n==== Ablation: {ablation_name} ====\")\n",
    "    ablation_df = evaluate_feature_set(\n",
    "        feature_cols=feature_cols,\n",
    "        internal_features=internal_features,\n",
    "        config=CONFIG,\n",
    "        walk_forward_splits=walk_forward_splits,\n",
    "        RL_ENV_CLASS=CumulativeTradingEnv,\n",
    "        ohlcv_df=ohlcv_df,\n",
    "        EXCLUDED_TICKERS=EXCLUDED_TICKERS,\n",
    "        EPISODE_LENGTH=EPISODE_LENGTH,\n",
    "        TOTAL_TRAIN_EPISODES=TOTAL_TRAIN_EPISODES,\n",
    "        TOTAL_TEST_EPISODES=TOTAL_TEST_EPISODES,\n",
    "        SEED=SEED,\n",
    "    )\n",
    "    ablation_df[\"ablation\"] = ablation_name\n",
    "    results_table.append(ablation_df)\n",
    "\n",
    "# === 6. Combine all results\n",
    "feature_ablation_df = pd.concat(results_table, ignore_index=True)\n",
    "\n",
    "# === 7. Analyze and visualize ===\n",
    "summary = []\n",
    "for ablation, group in feature_ablation_df.groupby(\"ablation\"):\n",
    "    rl_sharpes = group.loc[group['agent']=='RL', 'episode_sharpe'].dropna()\n",
    "    random_sharpes = group.loc[group['agent']=='Random', 'episode_sharpe'].dropna()\n",
    "    mean_rl = rl_sharpes.mean() if len(rl_sharpes) > 0 else np.nan\n",
    "    mean_random = random_sharpes.mean() if len(random_sharpes) > 0 else np.nan\n",
    "    t_p = np.nan\n",
    "    u_p = np.nan\n",
    "    if len(rl_sharpes) > 1 and len(random_sharpes) > 1:\n",
    "        t_stat, t_p = ttest_ind(rl_sharpes, random_sharpes, equal_var=False)\n",
    "        u_stat, u_p = mannwhitneyu(rl_sharpes, random_sharpes, alternative='greater')\n",
    "    summary.append({\n",
    "        \"ablation\": ablation,\n",
    "        \"mean_sharpe_rl\": mean_rl,\n",
    "        \"mean_sharpe_random\": mean_random,\n",
    "        \"t_pvalue\": t_p,\n",
    "        \"u_pvalue\": u_p,\n",
    "        \"RL>Random\": mean_rl > mean_random,\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df = summary_df.sort_values(by=\"mean_sharpe_rl\", ascending=False)\n",
    "\n",
    "# === 8. Visualization ===\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x='mean_sharpe_rl', y='ablation', data=summary_df)\n",
    "plt.title(\"Feature Ablation: Mean RL Sharpe by Feature Set\")\n",
    "plt.xlabel(\"Mean Test Sharpe (RL Agent)\")\n",
    "plt.ylabel(\"Ablation (Feature Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n### Feature Ablation Summary Table:\")\n",
    "print(summary_df.to_markdown(index=False, floatfmt=\".3g\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925be5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
