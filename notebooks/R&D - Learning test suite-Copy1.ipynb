{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from trackers import EpisodeTracker,EnvironmentTracker,AgentTracker\n",
    "from environments import PositionTradingEnvV2\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"core_learnability_test\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5022e46",
   "metadata": {},
   "source": [
    "# Steps of the Learning Test Suite Pipeline\n",
    "\n",
    "1. get monthly episodes\n",
    "2. for each month, get prev episode_length days + lookback + required rollback for things to work.\n",
    "3. the data for the train dataframe should end in the 1st day of a given month\n",
    "4. th data for the test dataframe should start on the 1st day of a given month\n",
    "\n",
    "Having the episodes sampled, let's give them a id and lets train each of the agents with their barebones , to compare what every one of them was able to learn in that timeframe and how usefull it was to th next timeframe\n",
    "\n",
    "Classify episodes by:\n",
    "1. meta features\n",
    "2. next day classifier meta features\n",
    "3. agent learning efficiency\n",
    "4. noise \n",
    "5. financial metrics\n",
    "5. transferability\n",
    "\n",
    "Final goal is to predict if a episode is usefull to be trained at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17c8ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import acf, acovf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5be2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_policy(arr):\n",
    "    # return np.median(arr)\n",
    "    return pd.Series(arr).ewm(span=5).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a3186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45462a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_tracker = EpisodeTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e86f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "EXPERIMENT_NAME = \"core_episode_rank\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "FEATURES_PATH = DEFAULT_PATH+'/features'\n",
    "TARGETS_PATH = DEFAULT_PATH+'/targets'\n",
    "META_PATH = DEFAULT_PATH+'/meta'\n",
    "\n",
    "class EpisodeRanker:\n",
    "    def __init__(self,ticker,episode_length=120):\n",
    "        self.episode_tracker = EpisodeTracker()\n",
    "        self.agent_tracker = AgentTracker()\n",
    "        self.environment_tracker = EnvironmentTracker()\n",
    "        self.ticker = ticker\n",
    "        self.episode_length = episode_length\n",
    "        self.episodes = self.episode_tracker.monthly_episode_walkforward(OHLCV_DF,ticker,episode_length)\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "        self.metadata = []\n",
    "        self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        self.df = OHLCV_DF.copy()\n",
    "        if all([os.path.exists(path) for path in [FEATURES_PATH, TARGETS_PATH, META_PATH]]):\n",
    "            self.features = joblib.load(FEATURES_PATH)\n",
    "            self.targets = joblib.load(TARGETS_PATH)\n",
    "            self.metadata = joblib.load(META_PATH)\n",
    "            print(\"Loaded cached feature/target/meta lists.\")\n",
    "\n",
    "    def evaluate(self,env_cls,agent_cls,env_kwargs={},agent_kwargs={}):\n",
    "        episodes = self.episodes\n",
    "        environment = env_cls(self.df,self.ticker,**env_kwargs)\n",
    "        \n",
    "        agent = agent_cls.init(environment)\n",
    "        \n",
    "        evaluation_entry = {\n",
    "            \"env_id\": self.environment_tracker.findOrCreate(env_cls,env_kwargs),\n",
    "            \"agent_id\": self.agent_tracker.findOrCreate(agent_cls,agent_kwargs)\n",
    "        }\n",
    "        for ep in episodes:\n",
    "            evaluation_entry['episode_id']= ep[\"episode_id\"]\n",
    "            df_t1= ep[\"train_episode\"].copy()\n",
    "            r1d =  ep[\"train_episode\"]['return_1d'].astype(float).values\n",
    "            v = ep['train_episode']['volume'].astype(float).values\n",
    "            feat = {\n",
    "                'symbol': self.ticker,\n",
    "                'month_str': str(ep['date']),\n",
    "                'mean_return': mean_policy(r1d),\n",
    "                'std_return': r1d.std(),\n",
    "                'skew': skew(r1d),\n",
    "                'kurtosis': kurtosis(r1d),\n",
    "                'entropy': entropy(np.histogram(r1d, bins=10, density=True)[0] + 1e-8),\n",
    "                'vol_mean': mean_policy(v),\n",
    "                'vol_std': v.std()\n",
    "            }\n",
    "            df_lag = df_t1.copy()\n",
    "            for lag in range(1, 5 + 1):\n",
    "                df_lag[f'return_lag_{lag}'] = df_lag['return_1d'].shift(lag)\n",
    "            df_lag = df_lag.dropna()\n",
    "          \n",
    "            if len(df_lag) < 5:\n",
    "                continue\n",
    "            X = df_lag[[f'return_lag_{i}' for i in range(1, 5+ 1)]].values\n",
    "            y = df_lag['return_1d'].values\n",
    "            model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            # Meta-diagnostics\n",
    "            ljung_pval = acorr_ljungbox(residuals, lags=[5], return_df=True).iloc[0]['lb_pvalue']\n",
    "            feat['ljung_pval'] = ljung_pval\n",
    "            feat['episode_id'] = ep['episode_id']\n",
    "            feat['resid_acf1'] = pd.Series(residuals).autocorr(lag=1)\n",
    "            feat['resid_std'] = residuals.std()\n",
    "            feat['resid_skew'] = skew(residuals)\n",
    "            feat['resid_kurtosis'] = kurtosis(residuals)\n",
    "            # Predictability label (cross-val RÂ²)\n",
    "            cv_r2 = mean_policy(cross_val_score(model, X, y, cv=2, scoring='r2'))\n",
    "            self.features.append(feat)\n",
    "            self.targets.append(cv_r2)\n",
    "            self.metadata.append((\"AAPL\", str(ep['date'])))\n",
    "            \n",
    "        \n",
    "        return agent,environment,evaluation_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb49b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ff365f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[\"agent_id\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b81af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'symbol': 'AAPL',\n",
       "  'month_str': '2024-01-01',\n",
       "  'mean_return': -0.00217315495308203,\n",
       "  'std_return': 0.011972717765185093,\n",
       "  'skew': -0.8511455333878075,\n",
       "  'kurtosis': 1.7345324574674956,\n",
       "  'entropy': 1.8472986697540643,\n",
       "  'vol_mean': 41490640.90139169,\n",
       "  'vol_std': 16460479.888669945,\n",
       "  'ljung_pval': 0.9480842635535118,\n",
       "  'resid_acf1': 0.08014185924628414,\n",
       "  'resid_std': 0.005028164947951779,\n",
       "  'resid_skew': -0.7163116354205482,\n",
       "  'resid_kurtosis': 1.5369467844632716},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-02-01',\n",
       "  'mean_return': -0.011569650399054046,\n",
       "  'std_return': 0.01223517879599969,\n",
       "  'skew': -0.35847476091546415,\n",
       "  'kurtosis': 0.5330960628384496,\n",
       "  'entropy': 1.9811566737652901,\n",
       "  'vol_mean': 53134145.90497142,\n",
       "  'vol_std': 15040296.392909277,\n",
       "  'ljung_pval': 0.8464709129200448,\n",
       "  'resid_acf1': 0.0064673057163466535,\n",
       "  'resid_std': 0.0048243155197991465,\n",
       "  'resid_skew': -0.22244313640745023,\n",
       "  'resid_kurtosis': 0.3417824934522793},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-03-01',\n",
       "  'mean_return': -0.002575992407750049,\n",
       "  'std_return': 0.01083876365791124,\n",
       "  'skew': -0.04489627029477858,\n",
       "  'kurtosis': 0.6032821422005412,\n",
       "  'entropy': 1.877779385805161,\n",
       "  'vol_mean': 78166448.49402256,\n",
       "  'vol_std': 16435004.101274649,\n",
       "  'ljung_pval': 0.9257494378930228,\n",
       "  'resid_acf1': 0.0303911149137011,\n",
       "  'resid_std': 0.0043284807598582215,\n",
       "  'resid_skew': 0.17708360873798132,\n",
       "  'resid_kurtosis': 1.8293562913765449},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-04-01',\n",
       "  'mean_return': -0.0012636215023346048,\n",
       "  'std_return': 0.011717423678107613,\n",
       "  'skew': -0.3457882687724312,\n",
       "  'kurtosis': 1.1729227443036931,\n",
       "  'entropy': 1.8303841832240004,\n",
       "  'vol_mean': 64316897.05431775,\n",
       "  'vol_std': 17739894.175780337,\n",
       "  'ljung_pval': 0.9652400981381055,\n",
       "  'resid_acf1': 0.027076590893883336,\n",
       "  'resid_std': 0.004415869901536914,\n",
       "  'resid_skew': -0.48972174309212,\n",
       "  'resid_kurtosis': 1.5057968904268435},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-05-01',\n",
       "  'mean_return': 0.0001895068364131195,\n",
       "  'std_return': 0.012329824106472556,\n",
       "  'skew': 0.07208559458601303,\n",
       "  'kurtosis': 1.6956541071949713,\n",
       "  'entropy': 1.754548616170989,\n",
       "  'vol_mean': 59139205.7126464,\n",
       "  'vol_std': 18580986.030615598,\n",
       "  'ljung_pval': 0.8024927137600594,\n",
       "  'resid_acf1': 0.06811228276903092,\n",
       "  'resid_std': 0.004989258116258428,\n",
       "  'resid_skew': 0.06485812047834692,\n",
       "  'resid_kurtosis': 1.6956763678487636},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-06-01',\n",
       "  'mean_return': 0.0033483895629268894,\n",
       "  'std_return': 0.01354435069256945,\n",
       "  'skew': 0.6626851315501774,\n",
       "  'kurtosis': 3.4169897880545967,\n",
       "  'entropy': 1.640837788933596,\n",
       "  'vol_mean': 57624862.85342636,\n",
       "  'vol_std': 20958671.468376294,\n",
       "  'ljung_pval': 0.9382616303345864,\n",
       "  'resid_acf1': 0.014328735726101993,\n",
       "  'resid_std': 0.005164203752173547,\n",
       "  'resid_skew': 0.5066328364902927,\n",
       "  'resid_kurtosis': 2.23823820816673},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-07-01',\n",
       "  'mean_return': -0.0017274269508864865,\n",
       "  'std_return': 0.015306227268060475,\n",
       "  'skew': 1.2321354517282683,\n",
       "  'kurtosis': 4.623215688149432,\n",
       "  'entropy': 1.6546650226993156,\n",
       "  'vol_mean': 78091586.42567159,\n",
       "  'vol_std': 30839515.17119012,\n",
       "  'ljung_pval': 0.8131805893494718,\n",
       "  'resid_acf1': 0.09428137960312838,\n",
       "  'resid_std': 0.0059973324543042845,\n",
       "  'resid_skew': 0.9239317887273509,\n",
       "  'resid_kurtosis': 2.513004839484922},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-08-01',\n",
       "  'mean_return': 0.004255695623575947,\n",
       "  'std_return': 0.01574150820208808,\n",
       "  'skew': 1.0054173042899448,\n",
       "  'kurtosis': 3.9596162427848594,\n",
       "  'entropy': 1.6987516085803505,\n",
       "  'vol_mean': 45793801.68844404,\n",
       "  'vol_std': 30757193.779759705,\n",
       "  'ljung_pval': 0.5037551749917195,\n",
       "  'resid_acf1': 0.1481583784287895,\n",
       "  'resid_std': 0.006298173291208376,\n",
       "  'resid_skew': 0.7250592999246612,\n",
       "  'resid_kurtosis': 2.830108813908409},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-09-01',\n",
       "  'mean_return': 0.001931191238174118,\n",
       "  'std_return': 0.01609344996737184,\n",
       "  'skew': 0.6208257420913103,\n",
       "  'kurtosis': 3.805247543456164,\n",
       "  'entropy': 1.6037945360482193,\n",
       "  'vol_mean': 45595446.09096539,\n",
       "  'vol_std': 31213453.74589978,\n",
       "  'ljung_pval': 0.2444918962088532,\n",
       "  'resid_acf1': 0.19170007570196715,\n",
       "  'resid_std': 0.006666631265859829,\n",
       "  'resid_skew': 0.39291277075650266,\n",
       "  'resid_kurtosis': 1.7440427228892323},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-10-01',\n",
       "  'mean_return': 0.008870479430737908,\n",
       "  'std_return': 0.01625343357070782,\n",
       "  'skew': 0.6994633135975333,\n",
       "  'kurtosis': 3.4283175891468387,\n",
       "  'entropy': 1.6279280428978478,\n",
       "  'vol_mean': 53314754.91749245,\n",
       "  'vol_std': 38735470.5346394,\n",
       "  'ljung_pval': 0.4155577636983434,\n",
       "  'resid_acf1': 0.10175842249648775,\n",
       "  'resid_std': 0.006398427974960056,\n",
       "  'resid_skew': 0.07860382989856964,\n",
       "  'resid_kurtosis': 1.9120647093037384},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-11-01',\n",
       "  'mean_return': -0.00870392420750779,\n",
       "  'std_return': 0.015090991978048723,\n",
       "  'skew': 0.33853397754179865,\n",
       "  'kurtosis': 3.7290734742362,\n",
       "  'entropy': 1.5621077996722272,\n",
       "  'vol_mean': 48480599.97639626,\n",
       "  'vol_std': 38074327.316799775,\n",
       "  'ljung_pval': 0.497814080688215,\n",
       "  'resid_acf1': 0.1438957945732526,\n",
       "  'resid_std': 0.00636232920493572,\n",
       "  'resid_skew': 0.11296640063423057,\n",
       "  'resid_kurtosis': 1.8734033475659224},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2024-12-01',\n",
       "  'mean_return': 0.0065112959695468875,\n",
       "  'std_return': 0.0150167572197416,\n",
       "  'skew': 0.38788133553708504,\n",
       "  'kurtosis': 3.8243538903622643,\n",
       "  'entropy': 1.586942405525097,\n",
       "  'vol_mean': 40520534.993693694,\n",
       "  'vol_std': 38297710.112155706,\n",
       "  'ljung_pval': 0.6956110979681677,\n",
       "  'resid_acf1': 0.11222184442068225,\n",
       "  'resid_std': 0.005568864037053492,\n",
       "  'resid_skew': -0.47923094491990625,\n",
       "  'resid_kurtosis': 0.7157699685510868},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-01-01',\n",
       "  'mean_return': -0.005466754262075393,\n",
       "  'std_return': 0.012606302731225311,\n",
       "  'skew': -0.7440534982401138,\n",
       "  'kurtosis': 1.5740783916916605,\n",
       "  'entropy': 1.7185484428715219,\n",
       "  'vol_mean': 40765978.71079244,\n",
       "  'vol_std': 29877427.188161533,\n",
       "  'ljung_pval': 0.998081719997459,\n",
       "  'resid_acf1': -0.0034261702954103937,\n",
       "  'resid_std': 0.0050457383558779045,\n",
       "  'resid_skew': -0.7654022122310459,\n",
       "  'resid_kurtosis': 1.8041668580871084},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-02-01',\n",
       "  'mean_return': 0.0018481236987858204,\n",
       "  'std_return': 0.013052400805295282,\n",
       "  'skew': -0.30453498161392456,\n",
       "  'kurtosis': 1.0258779476479187,\n",
       "  'entropy': 1.8331686932802056,\n",
       "  'vol_mean': 74725887.08476181,\n",
       "  'vol_std': 29854400.557948854,\n",
       "  'ljung_pval': 0.9964432905675922,\n",
       "  'resid_acf1': -0.03457980365131767,\n",
       "  'resid_std': 0.005266417647763597,\n",
       "  'resid_skew': -0.14996393942347658,\n",
       "  'resid_kurtosis': 0.7410289183351941},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-03-01',\n",
       "  'mean_return': 0.0003488981832900327,\n",
       "  'std_return': 0.013919579582054543,\n",
       "  'skew': -0.3243144107445247,\n",
       "  'kurtosis': 0.6279635513499193,\n",
       "  'entropy': 1.9107507472471583,\n",
       "  'vol_mean': 48523959.4775999,\n",
       "  'vol_std': 29899408.99964523,\n",
       "  'ljung_pval': 0.9854198389095667,\n",
       "  'resid_acf1': -0.028250888861459062,\n",
       "  'resid_std': 0.005980599338757063,\n",
       "  'resid_skew': -0.3192935471594085,\n",
       "  'resid_kurtosis': 1.1782587385501477},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-04-01',\n",
       "  'mean_return': 0.003002466846346771,\n",
       "  'std_return': 0.014958319402587312,\n",
       "  'skew': -0.6055324366925466,\n",
       "  'kurtosis': 0.5245818086724965,\n",
       "  'entropy': 1.9355972347224768,\n",
       "  'vol_mean': 49507935.29243306,\n",
       "  'vol_std': 17527012.50397073,\n",
       "  'ljung_pval': 0.8625516189402549,\n",
       "  'resid_acf1': 0.04473890202376203,\n",
       "  'resid_std': 0.006060745308776083,\n",
       "  'resid_skew': -0.6037120272379801,\n",
       "  'resid_kurtosis': 0.49908722981971154},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-05-01',\n",
       "  'mean_return': 0.007131544738204924,\n",
       "  'std_return': 0.024685818116930266,\n",
       "  'skew': 1.1250228833508482,\n",
       "  'kurtosis': 12.543054138494213,\n",
       "  'entropy': 1.2715468281252085,\n",
       "  'vol_mean': 45814187.03922692,\n",
       "  'vol_std': 26565992.119276814,\n",
       "  'ljung_pval': 0.012192680007733554,\n",
       "  'resid_acf1': -0.24910694227540947,\n",
       "  'resid_std': 0.010771169856278753,\n",
       "  'resid_skew': -0.36223878313282026,\n",
       "  'resid_kurtosis': 7.074995192583716},\n",
       " {'symbol': 'AAPL',\n",
       "  'month_str': '2025-06-01',\n",
       "  'mean_return': 0.000559134538966443,\n",
       "  'std_return': 0.025806291450376453,\n",
       "  'skew': 1.1768036007716285,\n",
       "  'kurtosis': 10.570018715592752,\n",
       "  'entropy': 1.3278871661106333,\n",
       "  'vol_mean': 59046064.8096933,\n",
       "  'vol_std': 26247422.16084428,\n",
       "  'ljung_pval': 0.007016127111473448,\n",
       "  'resid_acf1': -0.30234774382283547,\n",
       "  'resid_std': 0.011731806150157812,\n",
       "  'resid_skew': -0.5046315359351916,\n",
       "  'resid_kurtosis': 8.960098785226633}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_ranker.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cdf0d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomAgent at 0x23bcd534bd0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "class CustomAgent:\n",
    "    def __init__(self,model_cls,policy, **kwargs):\n",
    "        #self.model = model_cls(policy,environment,**kwargs)\n",
    "        self.__version__ = model_cls.__name__\n",
    "        self.model_cls = model_cls\n",
    "        self.policy = policy\n",
    "        self.kwargs = {\n",
    "            **kwargs\n",
    "        }\n",
    "        self.config = {\n",
    "            \"name\":self.__version__,\n",
    "            \"policy\": policy,\n",
    "            **kwargs\n",
    "        }\n",
    "    def init(self,environment):\n",
    "        self.model = self.model_cls(self.policy,environment,**self.kwargs)\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "PpoAgent = CustomAgent(PPO,\"MlpPolicy\", ent_coef=0.1, verbose=1)\n",
    "PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b9b03d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco SÃ¡\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\trackers.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  record['kwargs']= json.loads(record['kwargs'])\n",
      "C:\\Users\\Francisco SÃ¡\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\trackers.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  record['kwargs']= json.loads(record['kwargs'])\n"
     ]
    }
   ],
   "source": [
    "ep_ranker = EpisodeRanker('AAPL',120)\n",
    "a,_,e=ep_ranker.evaluate(PositionTradingEnvV2,PpoAgent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-feature & Label Extraction =======================\n",
    "\n",
    "tickers = ohlcv_df['symbol'].unique()\n",
    "tickers = tickers[~np.isin(tickers, excluded_tickers)]\n",
    "def mean_policy(arr):\n",
    "    # return np.median(arr)\n",
    "    return pd.Series(arr).ewm(span=5).mean().iloc[-1]\n",
    "\n",
    "# Attempt to load if already exists (resumability)\n",
    "if all([os.path.exists(path) for path in [FEATURES_PATH, TARGETS_PATH, META_PATH]]):\n",
    "    features = joblib.load(FEATURES_PATH)\n",
    "    targets = joblib.load(TARGETS_PATH)\n",
    "    metadata = joblib.load(META_PATH)\n",
    "    print(\"Loaded cached feature/target/meta lists.\")\n",
    "    \n",
    "else:\n",
    "    features, targets, metadata = [], [], []\n",
    "    #tickers = ohlcv_df['symbol'].unique()\n",
    "    #tickers = [t for t in tickers if t not in run_settings[\"excluded_tickers\"]]\n",
    "    for symbol in tqdm(tickers):\n",
    "        df = ohlcv_df[ohlcv_df['symbol'] == symbol].sort_values('date').copy()\n",
    "        months = df['month'].unique()\n",
    "        for i in range(1, len(months)):\n",
    "            m_t = months[i-1]\n",
    "            m_t1 = months[i]\n",
    "            df_t = df[df['month'] == m_t]\n",
    "            df_t1 = df[df['month'] == m_t1]\n",
    "            if len(df_t1) < run_settings[\"min_samples\"]:\n",
    "                continue\n",
    "            r1d = df_t['return_1d'].astype(float).values\n",
    "            v = df_t['volume'].astype(float).values\n",
    "            feat = {\n",
    "                'symbol': symbol,\n",
    "                'month_str': str(m_t),\n",
    "                'mean_return': mean_policy(r1d),\n",
    "                'std_return': r1d.std(),\n",
    "                'skew': skew(r1d),\n",
    "                'kurtosis': kurtosis(r1d),\n",
    "                'entropy': entropy(np.histogram(r1d, bins=10, density=True)[0] + 1e-8),\n",
    "                'vol_mean': mean_policy(v),\n",
    "                'vol_std': v.std()\n",
    "            }\n",
    "            # Residual diagnostics from simple RF on t+1\n",
    "            df_lag = df_t1.copy()\n",
    "            for lag in range(1, 5+ 1):\n",
    "                df_lag[f'return_lag_{lag}'] = df_lag['return_1d'].shift(lag)\n",
    "            df_lag = df_lag.dropna()\n",
    "            if len(df_lag) < run_settings[\"min_samples\"]:\n",
    "                continue\n",
    "            X = df_lag[[f'return_lag_{i}' for i in range(1, 5 + 1)]].values\n",
    "            y = df_lag['return_1d'].values\n",
    "            model = RandomForestRegressor(n_estimators=300, random_state=config['random_state'])\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            # Meta-diagnostics\n",
    "            ljung_pval = acorr_ljungbox(residuals, lags=5, return_df=True).iloc[0]['lb_pvalue']\n",
    "            feat['ljung_pval'] = ljung_pval\n",
    "            feat['resid_acf1'] = pd.Series(residuals).autocorr(lag=1)\n",
    "            feat['resid_std'] = residuals.std()\n",
    "            feat['resid_skew'] = skew(residuals)\n",
    "            feat['resid_kurtosis'] = kurtosis(residuals)\n",
    "            # Predictability label (cross-val RÂ²)\n",
    "            cv_r2 = mean_policy(cross_val_score(model, X, y, cv=run_settings[\"cv_folds\"], scoring='r2'))\n",
    "            features.append(feat)\n",
    "            targets.append(cv_r2)\n",
    "            metadata.append((symbol, str(m_t)))\n",
    "    # Save for future resumes\n",
    "    joblib.dump(features, FEATURES_PATH)\n",
    "    joblib.dump(targets, TARGETS_PATH)\n",
    "    joblib.dump(metadata, META_PATH)\n",
    "    print(\"Feature/target/meta lists saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdf6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75389104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4abc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f7492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8bdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7cfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ab9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c56f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltm_test_suite.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2  # assumed to exist\n",
    "from data import sample_valid_episodes, extract_meta_features \n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SCORES_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(BENCHMARK_PATH), exist_ok=True)\n",
    "\n",
    "# ========== UTILITIES ==========\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "# ========== STEP 1: Load Data ==========\n",
    "print(\"[INFO] Loading data...\")\n",
    "# Replace this with real OHLCV loading\n",
    "df = OHLCV_DF.copy()#[OHLCV_DF['symbol']==TICKER].copy()\n",
    "\n",
    "\n",
    "# ========== STEP 2: Sample Benchmark Episodes ==========\n",
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(df, TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # â† âœ… Convert to list here\n",
    "\n",
    "# ========== STEP 3: Run Learnability Tests ==========\n",
    "meta_records = []\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Running episode from idx {start_idx} with seed {seed}\")\n",
    "\n",
    "        # Prepare Env\n",
    "        env = Monitor(PositionTradingEnv(df, TICKER, N_TIMESTEPS, LOOKBACK, start_idx=start_idx, seed=seed))\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO agent\n",
    "        obs,_ = env.reset()\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "\n",
    "        # Evaluate random agent\n",
    "        obs,_ = env.reset()\n",
    "        done, rand_score = False, 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rand_score += reward\n",
    "\n",
    "        # Calculate advantage\n",
    "        advantage = score - rand_score\n",
    "\n",
    "        # Log meta-data\n",
    "        start_date = df.loc[start_idx, \"date\"]\n",
    "        end_date = df.loc[start_idx + N_TIMESTEPS - 1, \"date\"]\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        meta = extract_meta_features(df.iloc[start_idx: start_idx + N_TIMESTEPS])\n",
    "        meta.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score\": score,\n",
    "            \"rand_score\": rand_score,\n",
    "            \"advantage\": advantage,\n",
    "            \"seed\": seed,\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date)\n",
    "        })\n",
    "        meta_records.append(meta)\n",
    "\n",
    "# ========== STEP 4: Save Results ==========\n",
    "pd.DataFrame(meta_records).to_csv(META_PATH, index=False)\n",
    "print(\"[INFO] Learnability test complete. Results saved to:\", META_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f63498",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(meta_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] Loading benchmark episodes...\")\n",
    "with open(BENCHMARK_PATH) as f:\n",
    "    benchmark_episodes = json.load(f)\n",
    "\n",
    "meta_records = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Transferability episode: seed={seed}, train_idx={start_idx}\")\n",
    "\n",
    "        # TRAIN on Month T\n",
    "        env_train = Monitor(PositionTradingEnv(df[df['symbol'] ==TICKER].reset_index(), TICKER, N_TIMESTEPS, LOOKBACK, seed=seed, start_idx=start_idx))\n",
    "        model = PPO(\"MlpPolicy\", env_train, verbose=0, seed=seed)\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO on Month T\n",
    "        obs, _= env_train.reset()\n",
    "        done, score_train = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env_train.step(action)\n",
    "            score_train += reward\n",
    "\n",
    "        # Random agent on Month T\n",
    "        obs, _ = env_train.reset()\n",
    "        done, rand_train = False, 0\n",
    "        while not done:\n",
    "            action = env_train.action_space.sample()\n",
    "            obs, reward, done, _, _ = env_train.step(action)\n",
    "            rand_train += reward\n",
    "\n",
    "        # TEST on Month T+1\n",
    "        test_idx = start_idx + 60  # approx. one month later\n",
    "        if test_idx + N_TIMESTEPS >= len(df):\n",
    "            print(\"[WARN] Skipping episode â€” test idx out of range\")\n",
    "            continue\n",
    "\n",
    "        env_test = Monitor(PositionTradingEnv(df[df['symbol'] ==TICKER].reset_index(), TICKER, N_TIMESTEPS, LOOKBACK, seed=seed, start_idx=test_idx))\n",
    "\n",
    "        obs, _ = env_test.reset()\n",
    "        done, score_test = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env_test.step(action)\n",
    "            score_test += reward\n",
    "\n",
    "        # Random agent on Month T+1\n",
    "        obs, _ = env_test.reset()\n",
    "        done, rand_test = False, 0\n",
    "        while not done:\n",
    "            action = env_test.action_space.sample()\n",
    "            obs, reward, done, _, _ = env_test.step(action)\n",
    "            rand_test += reward\n",
    "\n",
    "        advantage_train = score_train - rand_train\n",
    "        advantage_test = score_test - rand_test\n",
    "        transfer_delta = score_test - score_train\n",
    "\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"train_idx\": int(start_idx),\n",
    "            \"test_idx\": int(test_idx),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        features = extract_meta_features(df.iloc[start_idx:start_idx + N_TIMESTEPS])\n",
    "        features.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score_train\": score_train,\n",
    "            \"score_test\": score_test,\n",
    "            \"advantage_train\": advantage_train,\n",
    "            \"advantage_test\": advantage_test,\n",
    "            \"transfer_delta\": transfer_delta,\n",
    "            \"transfer_success\": int(transfer_delta > 0),\n",
    "            \"ticker\": TICKER,\n",
    "            \"seed\": seed\n",
    "        })\n",
    "        meta_records.append(features)\n",
    "\n",
    "pd.DataFrame(meta_records).to_csv(TRANSFER_META_PATH, index=False)\n",
    "print(\"[INFO] Transferability test complete. Results saved to:\", TRANSFER_META_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['symbol']==\"AAPL\"].reset_index().iloc[672 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Union\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example agent registry\n",
    "AGENT_REGISTRY = {\n",
    "    \"ppo\": PPO,\n",
    "    \"a2c\": A2C\n",
    "}\n",
    "\n",
    "def compute_additional_metrics(env):\n",
    "    values = np.array(env.values)\n",
    "    rewards = np.array(env.rewards)\n",
    "    actions = np.array(env.actions)\n",
    "\n",
    "    returns = pd.Series(values).pct_change().dropna()\n",
    "    volatility = returns.std()\n",
    "    entropy = -np.sum(np.bincount(actions, minlength=2)/len(actions) * np.log2(np.bincount(actions, minlength=2)/len(actions) + 1e-9))\n",
    "    max_drawdown = (values / np.maximum.accumulate(values)).min() - 1\n",
    "    sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "    sortino = returns.mean() / (returns[returns < 0].std() + 1e-9) * np.sqrt(252)\n",
    "    calmar = returns.mean() / abs(max_drawdown + 1e-9)\n",
    "    success_trades = np.sum((np.diff(values) > 0) & (actions[1:] == 1)) + np.sum((np.diff(values) < 0) & (actions[1:] == 0))\n",
    "\n",
    "    return {\n",
    "        \"volatility\": volatility,\n",
    "        \"entropy\": entropy,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"calmar\": calmar,\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(actions == 0),\n",
    "        \"action_long_ratio\": np.mean(actions == 1)\n",
    "    }\n",
    "\n",
    "def formalized_learning_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    agents: List[str] = [\"ppo\", \"a2c\"],\n",
    "    env_cls: Callable = None,\n",
    "    env_name: str = \"PositionTradingEnv\",\n",
    "    env_version: str = \"v0\",\n",
    "    env_config: Dict = None,\n",
    "    timesteps: int = 10_000,\n",
    "    eval_episodes: int = 10,\n",
    "    n_timesteps: int = 60,\n",
    "    lookback: int = 0,\n",
    "    seed: int = 42,\n",
    "    result_path: str = \"data/eval/ltm_learnability.csv\"\n",
    "):\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    # Load previously completed configs\n",
    "    if os.path.exists(result_path):\n",
    "        past_df = pd.read_csv(result_path)\n",
    "        past_configs = set(past_df['config_hash'].unique())\n",
    "    else:\n",
    "        past_df = pd.DataFrame()\n",
    "        past_configs = set()\n",
    "\n",
    "    # Filter data and sample episodes\n",
    "    df = df[df['symbol'] == ticker].copy().sort_values('date')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    mondays = df[df['date'].dt.weekday == 0]\n",
    "    valid_starts = []\n",
    "    for date in mondays['date']:\n",
    "        start_idx = df.index[df['date'] == date][0]\n",
    "        if start_idx + n_timesteps < len(df):\n",
    "            valid_starts.append(start_idx)\n",
    "    sampled_starts = np.random.default_rng(seed).choice(valid_starts, size=min(eval_episodes, len(valid_starts)), replace=False)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for agent_name in agents:\n",
    "        agent_cls = AGENT_REGISTRY[agent_name]\n",
    "        for start_idx in sampled_starts:\n",
    "            config = {\n",
    "                \"agent\": agent_name,\n",
    "                \"ticker\": ticker,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"timesteps\": timesteps,\n",
    "                \"n_timesteps\": n_timesteps,\n",
    "                \"lookback\": lookback,\n",
    "                \"seed\": seed,\n",
    "                \"env_name\": env_name,\n",
    "                \"env_version\": env_version,\n",
    "                \"env_config\": env_config or {}\n",
    "            }\n",
    "            config_hash = hash(json.dumps(config, sort_keys=True))\n",
    "            if config_hash in past_configs:\n",
    "                continue\n",
    "\n",
    "            env_train = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            env_train = Monitor(env_train)\n",
    "            model = agent_cls(\"MlpPolicy\", env_train, seed=seed, verbose=0)\n",
    "            model.learn(total_timesteps=timesteps)\n",
    "\n",
    "            # Eval PPO\n",
    "            env_eval = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            obs, _ = env_eval.reset()\n",
    "            done, ppo_score = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_eval.step(action)\n",
    "                ppo_score += reward\n",
    "\n",
    "            metrics = compute_additional_metrics(env_eval)\n",
    "\n",
    "            # Eval Random\n",
    "            env_rand = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            obs, _ = env_rand.reset()\n",
    "            done, rand_score = False, 0\n",
    "            while not done:\n",
    "                action = env_rand.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_rand.step(action)\n",
    "                rand_score += reward\n",
    "\n",
    "            row = {\n",
    "                \"config_hash\": config_hash,\n",
    "                \"agent\": agent_name,\n",
    "                \"env_name\": env_name,\n",
    "                \"env_version\": env_version,\n",
    "                \"ticker\": ticker,\n",
    "                \"seed\": seed,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"timesteps\": timesteps,\n",
    "                \"ppo_score\": ppo_score,\n",
    "                \"rand_score\": rand_score,\n",
    "                \"ppo_advantage\": ppo_score - rand_score,\n",
    "                \"ppo_std\": np.std([ppo_score]),\n",
    "                \"rand_std\": np.std([rand_score]),\n",
    "                \"ppo_median\": ppo_score,\n",
    "                \"rand_median\": rand_score,\n",
    "                \"train_start_date\": df.loc[start_idx, 'date'],\n",
    "                \"train_end_date\": df.loc[start_idx + n_timesteps, 'date'],\n",
    "                \"config_json\": json.dumps(config, sort_keys=True),\n",
    "                **metrics\n",
    "            }\n",
    "            all_results.append(row)\n",
    "\n",
    "    if all_results:\n",
    "        new_df = pd.DataFrame(all_results)\n",
    "        combined = pd.concat([past_df, new_df], ignore_index=True)\n",
    "        combined.to_csv(result_path, index=False)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172db668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb11f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e3a096",
   "metadata": {},
   "source": [
    "Beautiful â€” youâ€™ve just outlined the foundation of a **meta-learning curriculum engine** for reinforcement learning in financial environments. Letâ€™s break this down methodically, with **concrete scoring logic** for each goal and then tie it all into a full meta-classifier pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ GOAL: Quantify and predict *how good or bad* an episode is for learning or transferring.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 1. **Scoring Learnability \\[0â€“100]**\n",
    "\n",
    "> \"How well can an agent learn from this episode?\"\n",
    "\n",
    "### âœ”ï¸ Definition:\n",
    "\n",
    "Based on the PPO agentâ€™s performance **vs. random policy on the training episode**.\n",
    "\n",
    "### âœ… Formula:\n",
    "\n",
    "```python\n",
    "learnability_score = score_train_norm = normalize(score_train, 0, 100)\n",
    "```\n",
    "\n",
    "If your environment already outputs `score_train` normalized to \\[0, 100], you're done.\n",
    "\n",
    "You can also smooth it:\n",
    "\n",
    "```python\n",
    "learnability_score = 100 * sigmoid(advantage_train / scaling_factor)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 2. **Scoring Transferability \\[0â€“100]**\n",
    "\n",
    "> \"How well does what was learned transfer to a new episode?\"\n",
    "\n",
    "### âœ”ï¸ Definition:\n",
    "\n",
    "Based on the **delta** between training and test advantage:\n",
    "\n",
    "### âœ… Formula:\n",
    "\n",
    "```python\n",
    "transferability_score = 100 * sigmoid(advantage_test)\n",
    "```\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "```python\n",
    "transferability_score = 100 - abs(score_test - score_train)\n",
    "```\n",
    "\n",
    "You may also combine both:\n",
    "\n",
    "```python\n",
    "transferability_score = 100 * sigmoid(advantage_test) * (1 - abs(transfer_delta)/100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 3. **Scoring Difficulty \\[0â€“100]**\n",
    "\n",
    "> \"How inherently hard is this episode for an agent to learn?\"\n",
    "\n",
    "This should be **inversely related** to learnability and advantage.\n",
    "\n",
    "### âœ… Formula:\n",
    "\n",
    "```python\n",
    "difficulty_score = 100 - learnability_score\n",
    "```\n",
    "\n",
    "Or more robust:\n",
    "\n",
    "```python\n",
    "difficulty_score = 100 * (1 - sigmoid(advantage_train))  # inverse learnability\n",
    "```\n",
    "\n",
    "You could enhance this using entropy, kurtosis, adf\\_pval:\n",
    "\n",
    "```python\n",
    "difficulty_score += chaos_bonus  # if entropy or pval is high\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 4. **Clustering Episodes by Agent Performance**\n",
    "\n",
    "Use unsupervised learning to group episodes based on key outcome metrics:\n",
    "\n",
    "### âœ… Input Features:\n",
    "\n",
    "```python\n",
    "['score_train', 'score_test', 'advantage_train', 'advantage_test', 'transfer_delta', 'success_trades', 'action_hold_ratio']\n",
    "```\n",
    "\n",
    "### âœ… Method:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "X = df[feature_columns]\n",
    "clusters = KMeans(n_clusters=3, random_state=42).fit_predict(X)\n",
    "df['performance_cluster'] = clusters\n",
    "```\n",
    "\n",
    "You can later analyze:\n",
    "\n",
    "* Cluster 0 = high learn, high transfer\n",
    "* Cluster 1 = learnable but not transferable\n",
    "* Cluster 2 = hard/misleading episodes\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 5. **Meta-Classify an Episode Before Running It**\n",
    "\n",
    "Now we build a classifier that learns:\n",
    "\n",
    "> **f(episode\\_meta\\_features) â†’ predict (learnability, transferability)**\n",
    "\n",
    "### âœ… Input Features:\n",
    "\n",
    "```python\n",
    "['entropy', 'volatility', 'hurst', 'adf_pval', 'mean_return', 'return_trend', 'kurtosis', 'skew_return', 'max_drawdown', ...]\n",
    "```\n",
    "\n",
    "### âœ… Targets:\n",
    "\n",
    "```python\n",
    "'learnability_score' and 'transferability_score'\n",
    "```\n",
    "\n",
    "### âœ… Classifier Example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = df[meta_feature_columns]\n",
    "y_learn = df['learnability_score']\n",
    "y_transfer = df['transferability_score']\n",
    "\n",
    "clf_learn = RandomForestRegressor().fit(X, y_learn)\n",
    "clf_transfer = RandomForestRegressor().fit(X, y_transfer)\n",
    "\n",
    "# Predict on new episodes\n",
    "new_df['predicted_learnability'] = clf_learn.predict(new_df[meta_feature_columns])\n",
    "new_df['predicted_transferability'] = clf_transfer.predict(new_df[meta_feature_columns])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Step: Filter by Predicted Scores\n",
    "\n",
    "```python\n",
    "train_candidates = new_df[new_df['predicted_learnability'] > 70]\n",
    "test_candidates = new_df[\n",
    "    (new_df['predicted_learnability'] > 50) &\n",
    "    (new_df['predicted_transferability'] > 50)\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "| Goal                | Metric Used                      | Scale       |\n",
    "| ------------------- | -------------------------------- | ----------- |\n",
    "| Learnability        | `score_train`, `advantage_train` | 0â€“100       |\n",
    "| Transferability     | `advantage_test`, `score_test`   | 0â€“100       |\n",
    "| Difficulty          | Inverse of learnability          | 0â€“100       |\n",
    "| Clustering          | KMeans on agent outcome metrics  | categorical |\n",
    "| Predict meta-scores | RF or XGBoost on meta-features   | regression  |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate the full code to:\n",
    "\n",
    "* Compute the three scores\n",
    "* Cluster episodes\n",
    "* Train a meta-model\n",
    "* Predict and filter future training/test episodes?\n",
    "\n",
    "This can become your **adaptive episode sampler**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44b4a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIENCE_NAME = \"stock_predictability\"\n",
    "FEATURES_PATH = f\".data/cache/features_{EXPERIENCE_NAME}.pkl\"\n",
    "TARGETS_PATH = f\"data/cache/targets_{EXPERIENCE_NAME}.pkl\"\n",
    "META_PATH = f\"data/cache/meta_{EXPERIENCE_NAME}.pkl\"\n",
    "\n",
    "excluded_tickers=['CEG', 'GEHC', 'GEV', 'KVUE', 'SOLV']\n",
    "excluded_tickers.sort()\n",
    "#tickers = TOP2_STOCK_BY_SECTOR\n",
    "\n",
    "config={\n",
    "    \"regressor\":\"RandomForestRegressor\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"random_state\":314\n",
    "}\n",
    "run_settings={\n",
    "    \"excluded_tickers\": excluded_tickers,\n",
    "    \"min_samples\": 10,\n",
    "    \"cv_folds\": 3,\n",
    "    \"lags\": 5,\n",
    "    \"start_date\":\"2022-01-01\",\n",
    "    \"end_date\":\"2023-01-01\"\n",
    "}\n",
    "\n",
    "# Config section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e45d3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD OHLCV ==========================================\n",
    "\n",
    "\n",
    "ohlcv_df = load_base_dataframe()\n",
    "ohlcv_df['date'] = pd.to_datetime(ohlcv_df['date'])\n",
    "ohlcv_df = ohlcv_df[(ohlcv_df['date'] >= run_settings[\"start_date\"]) & (ohlcv_df['date'] < run_settings[\"end_date\"])]\n",
    "ohlcv_df['month'] = ohlcv_df['date'].dt.to_period('M')\n",
    "ohlcv_df['return_1d'] = ohlcv_df['return_1d'].fillna(0)\n",
    "ohlcv_df['sector_id'] = ohlcv_df['sector_id'].fillna('unknown')\n",
    "ohlcv_df['industry_id'] = ohlcv_df['industry_id'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e842e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498/498 [38:40<00:00,  4.66s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.data/cache/features_stock_predictability.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m         metadata\u001b[38;5;241m.\u001b[39mappend((symbol, \u001b[38;5;28mstr\u001b[39m(m_t)))\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Save for future resumes\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(features, FEATURES_PATH)\n\u001b[0;32m     92\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(targets, TARGETS_PATH)\n\u001b[0;32m     93\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(metadata, META_PATH)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.data/cache/features_stock_predictability.pkl'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Meta-feature & Label Extraction =======================\n",
    "\"\"\"\n",
    "# BASIC PREPROCESSING ===================================\n",
    "excluded_tickers = run_settings[\"excluded_tickers\"]\n",
    "min_samples = run_settings[\"min_samples\"]\n",
    "cv_folds = run_settings[\"cv_folds\"]\n",
    "lags = run_settings[\"lags\"]\n",
    "start_date = run_settings[\"start_date\"]\n",
    "end_date = run_settings[\"end_date\"]\n",
    "\n",
    "# CROP THE SAMPLE =======================================\n",
    "tickers = ohlcv_df['symbol'].unique()[:100]\n",
    "tickers = tickers[~np.isin(tickers, excluded_tickers)]\n",
    "tickers = [\"AAPL\",\"MSFT\",\"JPM\",\"V\",'LLY','UNH','AMZN','TSLA','META','GOOGL','GE','UBER','COST','WMT','XOM','CVX'.'NEE','SO','AMT','PLD','LIN','SHW']\n",
    "\n",
    "# FOR POC ONLY\n",
    "\n",
    "\n",
    "ohlcv_df = ohlcv_df.copy()\n",
    "ohlcv_df['date'] = pd.to_datetime(ohlcv_df['date'])\n",
    "ohlcv_df = ohlcv_df[(ohlcv_df['date'] >= start_date) & (ohlcv_df['date'] < end_date)]\n",
    "ohlcv_df['month'] = ohlcv_df['date'].dt.to_period('M')\n",
    "ohlcv_df['return_1d'] = ohlcv_df['return_1d'].fillna(0)\n",
    "\"\"\"\n",
    "tickers = ohlcv_df['symbol'].unique()\n",
    "tickers = tickers[~np.isin(tickers, excluded_tickers)]\n",
    "def mean_policy(arr):\n",
    "    # return np.median(arr)\n",
    "    return pd.Series(arr).ewm(span=5).mean().iloc[-1]\n",
    "\n",
    "# Attempt to load if already exists (resumability)\n",
    "if all([os.path.exists(path) for path in [FEATURES_PATH, TARGETS_PATH, META_PATH]]):\n",
    "    features = joblib.load(FEATURES_PATH)\n",
    "    targets = joblib.load(TARGETS_PATH)\n",
    "    metadata = joblib.load(META_PATH)\n",
    "    print(\"Loaded cached feature/target/meta lists.\")\n",
    "    \n",
    "else:\n",
    "    features, targets, metadata = [], [], []\n",
    "    #tickers = ohlcv_df['symbol'].unique()\n",
    "    #tickers = [t for t in tickers if t not in run_settings[\"excluded_tickers\"]]\n",
    "    for symbol in tqdm(tickers):\n",
    "        df = ohlcv_df[ohlcv_df['symbol'] == symbol].sort_values('date').copy()\n",
    "        months = df['month'].unique()\n",
    "        for i in range(1, len(months)):\n",
    "            m_t = months[i-1]\n",
    "            m_t1 = months[i]\n",
    "            df_t = df[df['month'] == m_t]\n",
    "            df_t1 = df[df['month'] == m_t1]\n",
    "            if len(df_t1) < run_settings[\"min_samples\"]:\n",
    "                continue\n",
    "            r1d = df_t['return_1d'].astype(float).values\n",
    "            v = df_t['volume'].astype(float).values\n",
    "            feat = {\n",
    "                'symbol': symbol,\n",
    "                'month_str': str(m_t),\n",
    "                'mean_return': mean_policy(r1d),\n",
    "                'std_return': r1d.std(),\n",
    "                'skew': skew(r1d),\n",
    "                'kurtosis': kurtosis(r1d),\n",
    "                'entropy': entropy(np.histogram(r1d, bins=10, density=True)[0] + 1e-8),\n",
    "                'vol_mean': mean_policy(v),\n",
    "                'vol_std': v.std()\n",
    "            }\n",
    "            # Residual diagnostics from simple RF on t+1\n",
    "            df_lag = df_t1.copy()\n",
    "            for lag in range(1, run_settings['lags'] + 1):\n",
    "                df_lag[f'return_lag_{lag}'] = df_lag['return_1d'].shift(lag)\n",
    "            df_lag = df_lag.dropna()\n",
    "            if len(df_lag) < run_settings[\"min_samples\"]:\n",
    "                continue\n",
    "            X = df_lag[[f'return_lag_{i}' for i in range(1, run_settings['lags'] + 1)]].values\n",
    "            y = df_lag['return_1d'].values\n",
    "            model = RandomForestRegressor(n_estimators=config['n_estimators'], random_state=config['random_state'])\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            # Meta-diagnostics\n",
    "            ljung_pval = acorr_ljungbox(residuals, lags=[run_settings['lags']], return_df=True).iloc[0]['lb_pvalue']\n",
    "            feat['ljung_pval'] = ljung_pval\n",
    "            feat['resid_acf1'] = pd.Series(residuals).autocorr(lag=1)\n",
    "            feat['resid_std'] = residuals.std()\n",
    "            feat['resid_skew'] = skew(residuals)\n",
    "            feat['resid_kurtosis'] = kurtosis(residuals)\n",
    "            # Predictability label (cross-val RÂ²)\n",
    "            cv_r2 = mean_policy(cross_val_score(model, X, y, cv=run_settings[\"cv_folds\"], scoring='r2'))\n",
    "            features.append(feat)\n",
    "            targets.append(cv_r2)\n",
    "            metadata.append((symbol, str(m_t)))\n",
    "    # Save for future resumes\n",
    "    joblib.dump(features, FEATURES_PATH)\n",
    "    joblib.dump(targets, TARGETS_PATH)\n",
    "    joblib.dump(metadata, META_PATH)\n",
    "    print(\"Feature/target/meta lists saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33584a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Construction  ============================\n",
    "X_df = pd.DataFrame(features)\n",
    "y_df = pd.Series(targets, name='cv_r2')\n",
    "meta_df = pd.DataFrame(metadata, columns=['symbol', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling & Preparation ==============================\n",
    "\n",
    "X = X_df.drop(columns=['symbol', 'month_str'])\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import joblib\n",
    "from src.env.base_trading_env import CumulativeTradingEnv\n",
    "\n",
    "RL_LABELS_PATH = f\"../../data/cache/meta_rl_labels_{EXPERIENCE_NAME}.pkl\"\n",
    "\n",
    "feature_cols = [\"return_1d\", \"volume\"]  # Or your preferred features\n",
    "episode_length = 18  # Or whatever fits your month\n",
    "train_steps = 300    # Fast!\n",
    "min_ep_len = 18\n",
    "# Resume logic: Load meta_df with RL columns if available\n",
    "if os.path.exists(RL_LABELS_PATH):\n",
    "    meta_df_rl = pd.read_pickle(RL_LABELS_PATH)\n",
    "    print(\"Loaded meta_df with RL columns.\")\n",
    "else:\n",
    "    # Copy original meta_df and initialize RL columns\n",
    "    meta_df_rl = meta_df.copy()\n",
    "    meta_df_rl['agent_reward'] = np.nan\n",
    "    meta_df_rl['random_reward'] = np.nan\n",
    "    meta_df_rl['advantage'] = np.nan\n",
    "    meta_df_rl['sharpe'] = np.nan\n",
    "    meta_df_rl['cum_return'] = np.nan\n",
    "    meta_df_rl['alpha'] = np.nan\n",
    "\n",
    "\n",
    "for i, row in tqdm(meta_df_rl.iterrows(), total=len(meta_df_rl), desc=\"Meta-RL Agent Loop\"):\n",
    "    # Skip if already computed\n",
    "    if not np.isnan(meta_df_rl.loc[i, 'agent_reward']):\n",
    "        continue\n",
    "\n",
    "    symbol, month = row['symbol'], row['month']\n",
    "    df_env = ohlcv_df[(ohlcv_df['symbol'] == symbol) & (ohlcv_df['month'] == month)].sort_values(\"date\")\n",
    "    if len(df_env) < min_ep_len:\n",
    "        min_ep_len = len(df_env)\n",
    "        print('new min',min_ep_len)\n",
    "    if len(df_env) < episode_length:\n",
    "        print('x',len(df_env) ,episode_length)\n",
    "        continue  # Not enough data, skip\n",
    "\n",
    "    try:\n",
    "        env = PositionTradingEnvV2sitionTradingEnvV2sitionTradingEnvV2sitionTradingEnvV2sitionTradingEnvV2(\n",
    "            df=df_env,\n",
    "            feature_cols=feature_cols,\n",
    "            episode_length=episode_length,\n",
    "            transaction_cost=0.0001,\n",
    "            seed=42\n",
    "        )\n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        check_env(env, warn=True)\n",
    "\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, n_steps=64, batch_size=16, learning_rate=0.001, seed=42)\n",
    "        model.learn(total_timesteps=train_steps)\n",
    "\n",
    "        # Evaluate PPO\n",
    "        obs, _ = env.reset()\n",
    "        agent_rewards, done = [], False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            agent_rewards.append(reward)\n",
    "        agent_reward = np.sum(agent_rewards)\n",
    "\n",
    "        # Evaluate Random\n",
    "        obs, _ = env.reset()\n",
    "        random_rewards, done = [], False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            random_rewards.append(reward)\n",
    "        random_reward = np.sum(random_rewards)\n",
    "\n",
    "        advantage = agent_reward - random_reward\n",
    "\n",
    "        meta_df_rl.loc[i, 'agent_reward'] = agent_reward\n",
    "        meta_df_rl.loc[i, 'random_reward'] = random_reward\n",
    "        meta_df_rl.loc[i, 'advantage'] = advantage\n",
    "        meta_df_rl.loc[i, 'sharpe'] = info.get(\"episode_sharpe\", np.nan)\n",
    "        meta_df_rl.loc[i, 'cum_return'] = info.get(\"cumulative_return\", np.nan)\n",
    "        meta_df_rl.loc[i, 'alpha'] = info.get(\"alpha\", np.nan)\n",
    "        #print(info)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipped ({symbol})\",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_rl.to_csv('mrl.csv')\n",
    "meta_df_rl['target'] = (meta_df_rl['advantage'] > 0).astype(int)\n",
    "meta_df_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddab8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in meta_df_rl.columns if col not in ['symbol', 'month', 'agent_reward', 'random_reward', 'advantage', 'target']]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_rl['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615eff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure columns are compatible for merge\n",
    "X_df['month'] = X_df['month_str']\n",
    "merged = pd.merge(X_df, meta_df_rl, on=['symbol', 'month'], how='inner')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    col for col in merged.columns\n",
    "    if col not in ['symbol', 'month', 'month_str', 'agent_reward', 'random_reward', 'advantage', 'target']\n",
    "]\n",
    "\n",
    "X = merged[feature_cols]\n",
    "y = merged['target']\n",
    "\n",
    "# Scale features\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohlcv_df.sort_values(by=\"date\").head().to_csv('ohlcv_to_upload.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08140ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(importances)), importances[sorted_idx])\n",
    "plt.xticks(range(len(importances)), [feature_cols[i] for i in sorted_idx], rotation=90)\n",
    "plt.title(\"Meta-Feature Importances for Predicting RL Agent Advantage\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y_true: true labels, y_pred: predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)  # Optional: color map\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class_names = ['Will Learn', 'Wont learn']  # Adjust to your problem\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Oranges)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3749a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
