{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "#  Core Sequence-Aware Agent Design v2\n",
    "\n",
    "This experiment explores a transformer-based recurrent PPO agent for financial trading. The environment is sequence-aware and includes both regime-based augmentation and per-episode reward normalization. The agent is evaluated across top 2 stocks in each sector using structured episode sequences to assess learning generalization.\n",
    "\n",
    "---\n",
    "\n",
    "##  Experiment Configuration\n",
    "\n",
    "| Parameter               | Value                         |\n",
    "|-------------------------|-------------------------------|\n",
    "| Agent                   | Recurrent PPO + Transformer   |\n",
    "| Env Wrapper             | RegimeAugmentingWrapper + PerEpisodeRewardNormalizer |\n",
    "| Episode Length          | 100                           |\n",
    "| Episodes                | 20                            |\n",
    "| Eval Episodes           | 3 per iteration               |\n",
    "| Steps per Update        | 800                           |\n",
    "| Batch Size              | 100                           |\n",
    "| Total Timesteps         | 20,000                        |\n",
    "| Learning Rate           | 0.0003                        |\n",
    "| Entropy Coefficient     | 0.005                         |\n",
    "| Value Function Coeff    | 0.5                           |\n",
    "| Max Gradient Norm       | 0.5                           |\n",
    "| Normalize Advantage     | True                          |\n",
    "| Optimizer               | Adam                          |\n",
    "| Transformer d_model     | 64                            |\n",
    "| Heads                  | 4                             |\n",
    "| Layers                 | 2                             |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- **Train Set:** 2023-01-01 → 2023-07-01\n",
    "- **Test Set:** 2023-07-01 → 2024-01-01\n",
    "- **Assets:** Top 2 stocks by sector\n",
    "- **Sequence Split:** 80% train / 20% eval sequences\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent Architecture\n",
    "\n",
    "- **Feature Extractor:** Transformer encoder with causal mask and learnable positional encoding.\n",
    "- **Policy Class:** Custom `TransformerPolicy` extending `RecurrentActorCriticPolicy`.\n",
    "- **Reward Normalization:** Online normalization within episodes.\n",
    "- **Regime Augmentation:** Appends one-hot encoded market regime to each timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Summary (Selected Stats)\n",
    "\n",
    "| Timesteps | Ep Rew Mean | Explained Variance | Value Loss | Policy Grad Loss |\n",
    "|-----------|-------------|--------------------|------------|------------------|\n",
    "|  8000     | 3.21        | 0.15               | 4.68       | -0.00277         |\n",
    "| 14400     | 2.90        | 0.645              | 1.71       |  0.00163         |\n",
    "| 20000     | **3.79**    | **0.751**          | 1.59       | -0.00012         |\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Snapshots\n",
    "\n",
    "| Timestep | Mean Reward | Std Dev | Eval Length |\n",
    "|----------|-------------|---------|-------------|\n",
    "| 5000     | -8.17       | ±8.72   | 102         |\n",
    "| 10000    | -4.43       | ±7.59   | 102         |\n",
    "| 15000    | -6.17       | ±4.92   | 102         |\n",
    "| 20000    | **0.24**    | ±2.86   | 102         |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "### Paired t-test and Mann-Whitney U-test\n",
    "\n",
    "| Metric       | t-test p-value | Mann-Whitney p-value |\n",
    "|--------------|----------------|-----------------------|\n",
    "| `total_reward` | 0.0300         | 0.0075                |\n",
    "| `calmar`       | 0.0132         | 0.0075                |\n",
    "\n",
    "✅ Both `total_reward` and `calmar ratio` show **statistically significant** improvements compared to the baseline.  \n",
    "Especially, Calmar implies **more stable and risk-adjusted returns**.\n",
    "\n",
    "> Note: `sharpe`, `sortino`, and `final_wealth` were skipped due to empty values in the evaluation logs. Ensure metrics are logged and valid across all test episodes to include them.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fix missing metrics** (`sharpe`, `sortino`, etc.) in the logging pipeline.\n",
    "2. **Plot distribution comparisons** (boxplots, histograms) for rewards and risk-adjusted returns.\n",
    "3. **Run ablation**:\n",
    "   - Without regime augmentation\n",
    "   - Without reward normalization\n",
    "   - With simpler agents (e.g., MLP or LSTM)\n",
    "4. **Test in unseen market conditions** or during volatility spikes to check robustness.\n",
    "\n",
    "---\n",
    "\n",
    "_Logged using `ExperimentTracker` — Run Hash: `${experiment_tracker.run_hash}`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550c1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/checkpoint/-3663478398062858244--best_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design_v2\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "WINDOW_LENGTH = 10  \n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 800,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"project_name\":EXPERIENCE_NAME\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data ==================================\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# Experiment tracker ========================= \n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "experiment_tracker.set_hash(CONFIG)\n",
    "\n",
    "# Files ======================================\n",
    "checkpoint_path = \"data/checkpoint\" \n",
    "checkpoint_name = experiment_tracker.run_hash\n",
    "checkpoint_preffix = f\"{checkpoint_name}--checkpoint\"\n",
    "checkpoint_best_model=f\"{checkpoint_path}/{checkpoint_name}--best_model\"\n",
    "log_path=\"data/logs\"\n",
    "save_path= f\"{checkpoint_path}/{checkpoint_name}--final\"\n",
    "print(checkpoint_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b1be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 100\n",
    "MAX_LENGTH = 200\n",
    "SAVE_FREQ=10000\n",
    "EVAL_FREQ=5000\n",
    "TOTAL_TIMESTEPS=200000\n",
    "#TOTAL_TIMESTEPS=1000\n",
    "EPISODES_PER_UPDATE = 8          # ~how many episodes before PPO updates\n",
    "EPISODES_PER_BATCH = 1           # number of full episodes per batch\n",
    "\n",
    "# === Auto-derive PPO settings ===\n",
    "N_STEPS = EPISODE_LENGTH * EPISODES_PER_UPDATE\n",
    "BATCH_SIZE = EPISODE_LENGTH * EPISODES_PER_BATCH\n",
    "\n",
    "ENV_CLASS = SequenceAwareCumulativeTradingEnv\n",
    "\n",
    "n = Notify(experiment_tracker.project)\n",
    "n.info('START')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e51121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "data/checkpoint/-3663478398062858244--final\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -2.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 200      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     train_agent()\n",
      "Cell \u001b[1;32mIn[6], line 157\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(save_path)\n\u001b[1;32m--> 157\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTOTAL_TIMESTEPS, callback\u001b[38;5;241m=\u001b[39m[checkpoint_callback, eval_callback])\n\u001b[0;32m    158\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:450\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[0;32m    443\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    451\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    452\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    453\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    454\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    455\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    456\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    457\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:287\u001b[0m, in \u001b[0;36mRecurrentPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    284\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs, terminal_lstm_state, episode_starts)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    285\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 287\u001b[0m rollout_buffer\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs,\n\u001b[0;32m    289\u001b[0m     actions,\n\u001b[0;32m    290\u001b[0m     rewards,\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts,\n\u001b[0;32m    292\u001b[0m     values,\n\u001b[0;32m    293\u001b[0m     log_probs,\n\u001b[0;32m    294\u001b[0m     lstm_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_lstm_states,\n\u001b[0;32m    295\u001b[0m )\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\common\\recurrent\\buffers.py:145\u001b[0m, in \u001b[0;36mRecurrentRolloutBuffer.add\u001b[1;34m(self, lstm_states, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_states_vf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lstm_states\u001b[38;5;241m.\u001b[39mvf[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_states_vf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lstm_states\u001b[38;5;241m.\u001b[39mvf[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\buffers.py:475\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "# Causal Mask Function ============================\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# Transformer Feature Extractor ===================\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=64, n_heads=4, n_layers=2, max_len=MAX_LENGTH):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "        input_dim = observation_space.shape[-1]\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward_v1(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "        return x[:, -1]  # return the last token output\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        #print(\">>> [Transformer] Input shape:\", obs.shape)\n",
    "\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "\n",
    "        pooled_output = x[:, -1]\n",
    "        #print(\">>> [Transformer] Pooled output mean/std:\", pooled_output.mean().item(), pooled_output.std().item())\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "# Transformer Policy ===================================\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs,\n",
    "                         features_extractor_class=TransformerFeatureExtractor,\n",
    "                         features_extractor_kwargs=dict(\n",
    "                             d_model=64, n_heads=4, n_layers=2, max_len=32\n",
    "                         ))\n",
    "        #self._build(self.lr_schedule)\n",
    "\n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)\n",
    "\n",
    "class PerEpisodeRewardNormalizer(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_rewards = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean = np.mean(self.episode_rewards)\n",
    "        std = np.std(self.episode_rewards) + 1e-8\n",
    "        norm_reward = (reward - mean) / std\n",
    "        return obs, norm_reward, done, truncated, info\n",
    "# Training =============================================================\n",
    "train_df = ohlcv_df[(ohlcv_df['date']>=\"2023-01-01\") & (ohlcv_df['date']<\"2023-07-01\")]\n",
    "test_df = ohlcv_df[(ohlcv_df['date']>=\"2023-07-01\") & (ohlcv_df['date']<\"2024-01-01\")]\n",
    "train_df = train_df[train_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "test_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "\n",
    "train_seq = train_env.generate_episode_sequences(TOTAL_TIMESTEPS)\n",
    "test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS*0.2))\n",
    "\n",
    "print(f\"Training on {len(train_seq)} different episodes accross the top 2 stocks for each sector\")\n",
    "\n",
    "def train_agent():\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    train_env.set_episode_sequence(train_seq)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    \n",
    "    train_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(train_env))\n",
    "    eval_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env))\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=SAVE_FREQ, save_path=checkpoint_path, name_prefix=checkpoint_preffix\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, best_model_save_path=checkpoint_best_model,\n",
    "        log_path=log_path, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    model = RecurrentPPO(\n",
    "        policy=TransformerPolicy,\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        #tensorboard_log=\"./tensorboard_logs\",\n",
    "        n_steps=N_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.005,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs=dict(share_features_extractor=True)\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "    model.save(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e18e27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PLD', 18),\n",
       " ('CVX', 13),\n",
       " ('CVX', 4),\n",
       " ('CVX', 21),\n",
       " ('LLY', 19),\n",
       " ('COST', 16),\n",
       " ('AAPL', 21),\n",
       " ('WMT', 15),\n",
       " ('GE', 3),\n",
       " ('LIN', 19),\n",
       " ('V', 19),\n",
       " ('NEE', 5),\n",
       " ('LIN', 13),\n",
       " ('JPM', 13),\n",
       " ('CVX', 19),\n",
       " ('NEE', 2),\n",
       " ('CVX', 14),\n",
       " ('XOM', 15),\n",
       " ('NEE', 5),\n",
       " ('JPM', 16),\n",
       " ('PLD', 13),\n",
       " ('WMT', 1),\n",
       " ('LLY', 21),\n",
       " ('SO', 17),\n",
       " ('SO', 7),\n",
       " ('TSLA', 9),\n",
       " ('CVX', 18),\n",
       " ('SO', 3),\n",
       " ('LLY', 12),\n",
       " ('SO', 6),\n",
       " ('JPM', 1),\n",
       " ('SHW', 9),\n",
       " ('JPM', 4),\n",
       " ('WMT', 21),\n",
       " ('LIN', 14),\n",
       " ('UNH', 1),\n",
       " ('GE', 6),\n",
       " ('MSFT', 1),\n",
       " ('UNH', 7),\n",
       " ('AMT', 13),\n",
       " ('LIN', 18),\n",
       " ('SHW', 10),\n",
       " ('CVX', 16),\n",
       " ('SO', 6),\n",
       " ('COST', 18),\n",
       " ('GE', 6),\n",
       " ('V', 20),\n",
       " ('AAPL', 10),\n",
       " ('XOM', 17),\n",
       " ('TSLA', 2),\n",
       " ('AMT', 7),\n",
       " ('COST', 12),\n",
       " ('COST', 0),\n",
       " ('COST', 3),\n",
       " ('GE', 3),\n",
       " ('SO', 0),\n",
       " ('META', 14),\n",
       " ('AAPL', 10),\n",
       " ('TSLA', 7),\n",
       " ('META', 18),\n",
       " ('GE', 19),\n",
       " ('UNH', 2),\n",
       " ('LIN', 3),\n",
       " ('NEE', 4),\n",
       " ('AAPL', 20),\n",
       " ('AAPL', 16),\n",
       " ('LIN', 3),\n",
       " ('SO', 16),\n",
       " ('PLD', 13),\n",
       " ('PLD', 20),\n",
       " ('CVX', 8),\n",
       " ('UBER', 5),\n",
       " ('MSFT', 7),\n",
       " ('UBER', 5),\n",
       " ('MSFT', 7),\n",
       " ('GE', 17),\n",
       " ('UBER', 0),\n",
       " ('LLY', 14),\n",
       " ('CVX', 0),\n",
       " ('MSFT', 1),\n",
       " ('MSFT', 20),\n",
       " ('WMT', 0),\n",
       " ('LLY', 1),\n",
       " ('META', 8),\n",
       " ('GOOGL', 0),\n",
       " ('LLY', 7),\n",
       " ('GE', 18),\n",
       " ('UBER', 16),\n",
       " ('V', 5),\n",
       " ('TSLA', 20),\n",
       " ('GOOGL', 16),\n",
       " ('COST', 14),\n",
       " ('UBER', 16),\n",
       " ('SO', 14),\n",
       " ('AMZN', 15),\n",
       " ('GOOGL', 7),\n",
       " ('NEE', 11),\n",
       " ('NEE', 8),\n",
       " ('V', 4),\n",
       " ('META', 15),\n",
       " ('GE', 7),\n",
       " ('JPM', 7),\n",
       " ('AMZN', 0),\n",
       " ('AMT', 20),\n",
       " ('GE', 8),\n",
       " ('SO', 1),\n",
       " ('COST', 6),\n",
       " ('AMZN', 20),\n",
       " ('MSFT', 16),\n",
       " ('V', 21),\n",
       " ('SO', 5),\n",
       " ('XOM', 15),\n",
       " ('LLY', 15),\n",
       " ('SHW', 6),\n",
       " ('META', 5),\n",
       " ('PLD', 10),\n",
       " ('CVX', 0),\n",
       " ('CVX', 7),\n",
       " ('UNH', 14),\n",
       " ('SO', 8),\n",
       " ('SHW', 14),\n",
       " ('CVX', 7),\n",
       " ('JPM', 19),\n",
       " ('GE', 12),\n",
       " ('WMT', 11),\n",
       " ('SO', 21),\n",
       " ('XOM', 3),\n",
       " ('UBER', 15),\n",
       " ('PLD', 9),\n",
       " ('LIN', 5),\n",
       " ('AMT', 17),\n",
       " ('UNH', 13),\n",
       " ('AMZN', 21),\n",
       " ('AMZN', 6),\n",
       " ('META', 20),\n",
       " ('COST', 5),\n",
       " ('META', 14),\n",
       " ('AMT', 2),\n",
       " ('UNH', 7),\n",
       " ('WMT', 9),\n",
       " ('META', 18),\n",
       " ('SHW', 19),\n",
       " ('CVX', 13),\n",
       " ('V', 8),\n",
       " ('TSLA', 15),\n",
       " ('UBER', 0),\n",
       " ('LIN', 6),\n",
       " ('AMZN', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS))\n",
    "\n",
    "\n",
    "episodes = _test_seq\n",
    "\n",
    "unique_episodes = {}\n",
    "for ticker, start in episodes:\n",
    "    if ticker not in unique_episodes:\n",
    "        unique_episodes[ticker] = start\n",
    "\n",
    "# Convert back to a list of tuples\n",
    "test_seq = [(ticker, start) for ticker, start in unique_episodes.items()]\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38331a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Notify(experiment_tracker.project)\n",
    "n.info('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634feade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from src.env.base_timeseries_trading_env import SequenceAwareCumulativeTradingEnv\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "#from src.env.base_timeseries_trading_env import RegimeAugmentingWrapper\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "N_EVAL_EPISODES = 5\n",
    "MODEL_PATH = save_path\n",
    "\n",
    "# === Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "test_df = ohlcv_df[(ohlcv_df['date'] >= \"2023-07-01\") & (ohlcv_df['date'] < \"2024-01-01\")]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "\n",
    "# === Evaluation Logic ===\n",
    "def evaluate_agent(agent, env, n_episodes=5):\n",
    "    episode_metrics = []\n",
    "    action_counts = []\n",
    "\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        print(\">>> [Eval] Obs shape:\", obs.shape)  # Check input dimensions\n",
    "\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        while not done:\n",
    "            action, state = agent.predict(obs, state=state, deterministic=True)\n",
    "            print(action)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": infos[-1].get(\"wealth\", np.nan),\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "        }\n",
    "\n",
    "        episode_metrics.append(metrics)\n",
    "    \n",
    "    print(\">>> [Eval] Action counts:\", pd.Series(action_counts).value_counts())\n",
    "    return pd.DataFrame(episode_metrics)\n",
    "\n",
    "\n",
    "def evaluate_random_agent(env, n_episodes=5):\n",
    "    episode_metrics = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": infos[-1].get(\"wealth\", np.nan),\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "        }\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics)\n",
    "\n",
    "\n",
    "# === Run Evaluation ===\n",
    "test_env = RegimeAugmentingWrapper(SequenceAwareCumulativeTradingEnv(test_df, episode_length=EPISODE_LENGTH,feature_cols=FEATURE_COLS))\n",
    "model = RecurrentPPO.load(MODEL_PATH)\n",
    "\n",
    "ppo_agent_df = evaluate_agent(model, test_env, n_episodes=N_EVAL_EPISODES)\n",
    "random_agent_df = evaluate_random_agent(test_env, n_episodes=N_EVAL_EPISODES)\n",
    "\n",
    "ppo_agent_df[\"agent\"] = \"recurrent_ppo\"\n",
    "random_agent_df[\"agent\"] = \"random\"\n",
    "results_df = pd.concat([ppo_agent_df, random_agent_df])\n",
    "\n",
    "\n",
    "# === Plotting ===\n",
    "melted = results_df.melt(id_vars=\"agent\", var_name=\"metric\", value_name=\"value\")\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=melted, x=\"metric\", y=\"value\", hue=\"agent\")\n",
    "plt.title(\"Agent Performance Comparison (Random vs Recurrent PPO)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Statistical Tests ===\n",
    "comparison_results = []\n",
    "\n",
    "for metric in ppo_agent_df.columns[:-1]:  # exclude 'agent'\n",
    "    a = ppo_agent_df[metric].dropna()\n",
    "    b = random_agent_df[metric].dropna()\n",
    "\n",
    "    # Skip if either is empty\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        print(f\"Skipping metric {metric}: empty values\")\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val_t = ttest_ind(a, b)\n",
    "    u_stat, p_val_u = mannwhitneyu(a, b, alternative='two-sided')\n",
    "    comparison_results.append({\n",
    "        \"metric\": metric,\n",
    "        \"t-test p-value\": p_val_t,\n",
    "        \"mann-whitney p-value\": p_val_u\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c1deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf68c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
