{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c29e00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Self-Aware Transformer Agent for Trading\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional RL trading agents rely heavily on market signals. But the market is noisy, volatile, and unpredictable ‚Äî especially in short windows. Instead of chasing external patterns, we **shift the focus inward**:\n",
    "\n",
    "> Teach the agent to interpret itself, its context, and act based on internal understanding and consistent behavior.\n",
    "\n",
    "This system mimics how **career traders** operate: not by predicting price ticks, but by reacting intelligently to their strategy performance, risk levels, and regime shifts.\n",
    "\n",
    "---\n",
    "\n",
    "## Agent Design Philosophy\n",
    "\n",
    "| Principle                | Implementation                                                         |\n",
    "| ------------------------ | ---------------------------------------------------------------------- |\n",
    "| Internal State Awareness | Agent tracks position, time held, drawdown, unrealized PnL             |\n",
    "| Compact, Dense Features  | Handcrafted candlestick + PCA-compressed signals                       |\n",
    "| Regime-Driven Training   | Train using rolling market windows with real history                   |\n",
    "| Transformer Context Use  | Use long context (2 months) to act intelligently in the next (1 month) |\n",
    "| Reward Discipline        | Combine PnL, drawdown penalty, and Sharpe-style consistency bonus      |\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering: `FeatureCompressor`\n",
    "\n",
    "| Feature Group        | Description                                          |\n",
    "| -------------------- | ---------------------------------------------------- |\n",
    "| Candlestick Shape    | `body`, `upper_shadow`, `lower_shadow`, `body_ratio` |\n",
    "| Relative Behavior    | `z_close`, `rel_volume`, `price_vs_range`            |\n",
    "| Signal Strength      | `rolling_sharpe`, `entropy`                          |\n",
    "| Historical Embedding | `pca_1`, `pca_2` from rolling PCA of close/volume    |\n",
    "\n",
    "All features are window-smoothed to reduce noise and expose true structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Custom Environment: `SelfAwareTradingEnv`\n",
    "\n",
    "* Builds the agent's **state** with both market and internal agent features\n",
    "* Internal metrics: position, time held, cumulative reward, drawdown, unrealized PnL, pct episode completed\n",
    "* **Reward Function**:\n",
    "\n",
    "  ```python\n",
    "  shaped_reward = (\n",
    "      0.5 * raw_pnl\n",
    "      + 0.3 * sharpe_bonus\n",
    "      - 0.2 * drawdown_penalty\n",
    "  )\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "* Input: full feature state (market + internal), padded or clipped to 60 steps\n",
    "* Positional encoding: learnable\n",
    "* Backbone: TransformerEncoder (2 layers, 4 heads)\n",
    "* Outputs a context vector per timestep ‚Üí passed to actor & critic\n",
    "* Integrated with `RecurrentPPO` for temporal memory\n",
    "\n",
    "---\n",
    "\n",
    "## Windowed Training Design\n",
    "\n",
    "Each training episode is built from a **single ticker**:\n",
    "\n",
    "| Step | Description                                                                     |\n",
    "| ---- | ------------------------------------------------------------------------------- |\n",
    "| 1    | Select a ticker and sort its data by date                                       |\n",
    "| 2    | Slice 60 days as `context` and next 30 as `target`                              |\n",
    "| 3    | Use only `target` for training, but allow agent to leverage Transformer context |\n",
    "| 4    | Repeat across all valid rolling windows and tickers                             |\n",
    "\n",
    "This creates hundreds of **coherent, regime-aware episodes** for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Full Pipeline Summary\n",
    "\n",
    "```text\n",
    "load_base_dataframe()\n",
    "‚Üì\n",
    "MarketWindowBuilder ‚Üí [context_df, target_df] for each ticker\n",
    "‚Üì\n",
    "FeatureCompressor ‚Üí Compress raw OHLCV into ~10 descriptive features\n",
    "‚Üì\n",
    "SelfAwareTradingEnv ‚Üí Builds internal + market state, shaped rewards\n",
    "‚Üì\n",
    "RecurrentPPO (TransformerPolicy) ‚Üí Trains over target month\n",
    "‚Üì\n",
    "EpisodeLoggerCallback ‚Üí Tracks Sharpe, Win Rate, Drawdown, # Trades\n",
    "‚Üì\n",
    "EvaluationRunner ‚Üí Loads each agent window and scores deterministically\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation & Benchmarking\n",
    "\n",
    "Each agent is evaluated on the same `target_df` it was trained on, using **deterministic inference**. Logged metrics include:\n",
    "\n",
    "| Metric         | Description                                      |\n",
    "| -------------- | ------------------------------------------------ |\n",
    "| `total_reward` | Sum of all rewards in episode                    |\n",
    "| `avg_reward`   | Mean reward per step                             |\n",
    "| `sharpe_bonus` | Volatility-adjusted consistency score            |\n",
    "| `drawdown`     | Max drawdown during episode                      |\n",
    "| `win_rate`     | % of profitable actions (to be optionally added) |\n",
    "\n",
    "Results are saved in `evaluation_results.csv` and can be compared across:\n",
    "\n",
    "* Tickers\n",
    "* Time periods\n",
    "* Volatility regimes\n",
    "\n",
    "---\n",
    "\n",
    "## Benchmark Plan\n",
    "\n",
    "| Baseline             | Description                                                |\n",
    "| -------------------- | ---------------------------------------------------------- |\n",
    "| Random Policy        | Random action sampling, same environment                   |\n",
    "| Hold-only Agent      | Buy once and hold through episode                          |\n",
    "| Classical PPO w/ MLP | Same pipeline, no memory or context                        |\n",
    "| Market Benchmark     | Cumulative return of passive exposure during target period |\n",
    "\n",
    "We will compute **performance deltas** against these baselines and track:\n",
    "\n",
    "* Sharpe outperformance\n",
    "* Drawdown reduction\n",
    "* Adaptation to volatile periods\n",
    "\n",
    "---\n",
    "\n",
    "## What Makes This Unique?\n",
    "\n",
    "* Agent **doesn‚Äôt predict**, it *reacts adaptively*\n",
    "* Context is used **intelligently**, not just stacked\n",
    "* Agent performance is **relative to itself**, not absolute return\n",
    "* Internal state + compressed signals = **compact but expressive space**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "from IPython.display import display\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b78b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "You want short-term sensitivity, reduce to 5\n",
    "You want longer context, increase to 20+\n",
    "But since the Transformer also has its own memory (seq_len=60), this window should capture local structure, while the Transformer will detect global structure across those.\n",
    "Would you like to try a few window values and compare feature statistics or training metrics?\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class FeatureCompressor:\n",
    "    def __init__(self, window=10):\n",
    "        self.window = window\n",
    "        self.pca = PCA(n_components=2)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        # === CANDLE FEATURES ===\n",
    "        df['body'] = df['close'] - df['open']\n",
    "        df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "        df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "        df['range'] = df['high'] - df['low']\n",
    "        df['body_ratio'] = df['body'] / (df['range'] + 1e-8)\n",
    "\n",
    "        # === RELATIVE CONTEXT FEATURES ===\n",
    "        df['z_close'] = (df['close'] - df['close'].rolling(self.window).mean()) / (df['close'].rolling(self.window).std() + 1e-8)\n",
    "        df['rel_volume'] = df['volume'] / (df['volume'].rolling(self.window).mean() + 1e-8)\n",
    "        df['price_vs_range'] = (df['close'] - df['low'].rolling(self.window).min()) / (\n",
    "            df['high'].rolling(self.window).max() - df['low'].rolling(self.window).min() + 1e-8)\n",
    "\n",
    "        # === SIGNAL QUALITY (EWM) ===\n",
    "        df['return'] = df['close'].pct_change()\n",
    "        df['ewm_return_mean'] = df['return'].ewm(span=self.window).mean()\n",
    "        df['ewm_return_std'] = df['return'].ewm(span=self.window).std()\n",
    "        df['rolling_sharpe'] = df['ewm_return_mean'] / (df['ewm_return_std'] + 1e-8)\n",
    "        df['entropy'] = df['return'].rolling(self.window).apply(self.shannon_entropy, raw=True)\n",
    "\n",
    "        # === PCA EMBEDDING ===\n",
    "        pca_inputs = []\n",
    "        raw = df[['close', 'volume']].fillna(0).values\n",
    "        for i in range(self.window, len(raw)):\n",
    "            window = raw[i - self.window:i].flatten()\n",
    "            pca_inputs.append(window)\n",
    "\n",
    "        if len(pca_inputs) > 0:\n",
    "            compressed = self.pca.fit_transform(np.array(pca_inputs))\n",
    "            df.loc[df.index[-len(compressed):], 'pca_1'] = compressed[:, 0]\n",
    "            df.loc[df.index[-len(compressed):], 'pca_2'] = compressed[:, 1]\n",
    "        else:\n",
    "            df['pca_1'] = 0\n",
    "            df['pca_2'] = 0\n",
    "\n",
    "        # === DATE-TIME FEATURES ===\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['day_of_month'] = df['date'].dt.day\n",
    "            df['month'] = df['date'].dt.month\n",
    "        else:\n",
    "            df['day_of_week'] = 0\n",
    "            df['day_of_month'] = 0\n",
    "            df['month'] = 0\n",
    "\n",
    "        # === FINAL FEATURE SET ===\n",
    "        features = [\n",
    "            'body', 'upper_shadow', 'lower_shadow', 'body_ratio',\n",
    "            'z_close', 'rel_volume', 'price_vs_range',\n",
    "            'rolling_sharpe', 'entropy',\n",
    "            'pca_1', 'pca_2',\n",
    "            'day_of_week', 'day_of_month', 'month'\n",
    "        ]\n",
    "\n",
    "        return df[features].dropna().reset_index(drop=True)\n",
    "\n",
    "    def shannon_entropy(self, x):\n",
    "        hist, bins = np.histogram(x, bins=5, density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return -np.sum(hist * np.log(hist + 1e-8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db6df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SelfAwareTradingEnv(gym.Env):\n",
    "    def __init__(self, market_array, sequence_length=10, episode_length=None):\n",
    "        super().__init__()\n",
    "        self.market_data = market_array\n",
    "        self.sequence_length = sequence_length\n",
    "        self.episode_length = episode_length or len(market_array)\n",
    "\n",
    "        self.feature_dim = self.market_data.shape[1]\n",
    "        self.internal_dim = 6  # position, time_in_pos, unrealized_pnl, total_reward, drawdown, step_pct\n",
    "        self.observation_dim = self.feature_dim + self.internal_dim\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.sequence_length, self.observation_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(3)  # Buy, Sell, Hold\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.total_reward = 0\n",
    "        self.current_position = 0\n",
    "        self.entry_price = None\n",
    "        self.time_in_position = 0\n",
    "        self.max_reward = 0\n",
    "        self.observation_buffer = []\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.current_step >= len(self.market_data):\n",
    "            # Prevent out-of-bounds\n",
    "            pad = np.zeros(self.feature_dim)\n",
    "            market_features = pad\n",
    "        else:\n",
    "            market_features = self.market_data[self.current_step]\n",
    "\n",
    "        internal_features = np.array([\n",
    "            self.current_position,\n",
    "            self.time_in_position,\n",
    "            self.calc_unrealized_pnl(),\n",
    "            self.total_reward,\n",
    "            self.calc_drawdown(),\n",
    "            self.current_step / self.episode_length\n",
    "        ])\n",
    "\n",
    "        full_obs = np.concatenate([market_features, internal_features])\n",
    "        self.observation_buffer.append(full_obs)\n",
    "\n",
    "        if len(self.observation_buffer) > self.sequence_length:\n",
    "            self.observation_buffer.pop(0)\n",
    "\n",
    "        while len(self.observation_buffer) < self.sequence_length:\n",
    "            self.observation_buffer.insert(0, np.zeros_like(full_obs))\n",
    "\n",
    "        return np.stack(self.observation_buffer, axis=0)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= self.episode_length:\n",
    "            # Episode over\n",
    "            return self._get_obs(), 0.0, True, False, {}\n",
    "\n",
    "        price = self.market_data[self.current_step, 0]  # Assuming first feature is close\n",
    "\n",
    "        raw_reward = 0.0\n",
    "        if action == 0:  # SELL\n",
    "            if self.current_position == 1:\n",
    "                raw_reward = price - self.entry_price\n",
    "                self.total_reward += raw_reward\n",
    "                self.current_position = 0\n",
    "                self.entry_price = None\n",
    "                self.time_in_position = 0\n",
    "\n",
    "        elif action == 1:  # BUY\n",
    "            if self.current_position == 0:\n",
    "                self.entry_price = price\n",
    "                self.current_position = 1\n",
    "                self.time_in_position = 1\n",
    "            else:\n",
    "                self.time_in_position += 1\n",
    "\n",
    "        elif action == 2:  # HOLD\n",
    "            if self.current_position == 1:\n",
    "                self.time_in_position += 1\n",
    "\n",
    "        # Reward shaping\n",
    "        shaped_reward = (\n",
    "            0.5 * raw_reward +\n",
    "            0.3 * self.sharpe_bonus() -\n",
    "            0.2 * self.calc_drawdown()\n",
    "        )\n",
    "\n",
    "        self.total_reward += shaped_reward\n",
    "        self.max_reward = max(self.max_reward, self.total_reward)\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = self.current_step >= self.episode_length\n",
    "        return self._get_obs(), shaped_reward, done, False, {}\n",
    "\n",
    "    def calc_unrealized_pnl(self):\n",
    "        if self.current_position == 0 or self.entry_price is None:\n",
    "            return 0.0\n",
    "        price = self.market_data[min(self.current_step, len(self.market_data) - 1), 0]\n",
    "        return price - self.entry_price\n",
    "\n",
    "    def calc_drawdown(self):\n",
    "        return max(0.0, self.max_reward - self.total_reward)\n",
    "\n",
    "    def sharpe_bonus(self):\n",
    "        # Placeholder: simple proxy\n",
    "        return np.sign(self.total_reward) * np.sqrt(abs(self.total_reward)) / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd76a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "class EpisodeLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_data = []\n",
    "\n",
    "    def _on_step(self):\n",
    "        info = self.locals.get('infos', [{}])[0]\n",
    "        if 'episode' in info:\n",
    "            self.episode_data.append(info['episode'])\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        df = pd.DataFrame(self.episode_data)\n",
    "        df.to_csv('agent_journal.csv', index=False)\n",
    "        print(\"‚úÖ Saved trading journal to agent_journal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5e3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MarketWindowBuilder:\n",
    "    def __init__(self, df: pd.DataFrame, context_days=40, target_days=20, ticker_col='symbol', date_col='date'):\n",
    "        self.df = df.copy()\n",
    "        self.context_days = context_days\n",
    "        self.target_days = target_days\n",
    "        self.ticker_col = ticker_col\n",
    "        self.date_col = date_col\n",
    "\n",
    "    def generate_windows(self):\n",
    "        windows = []\n",
    "        grouped = self.df.groupby(self.ticker_col)\n",
    "\n",
    "        for symbol, group in grouped:\n",
    "            group = group.sort_values(self.date_col).reset_index(drop=True)\n",
    "            total_days = self.context_days + self.target_days\n",
    "\n",
    "            for start_idx in range(0, len(group) - total_days):\n",
    "                context = group.iloc[start_idx : start_idx + self.context_days].copy()\n",
    "                target = group.iloc[start_idx + self.context_days : start_idx + total_days].copy()\n",
    "\n",
    "                windows.append({\n",
    "                    'symbol': symbol,\n",
    "                    'context': context.reset_index(drop=True),\n",
    "                    'target': target.reset_index(drop=True)\n",
    "                })\n",
    "\n",
    "        return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e1c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from market_window_builder import MarketWindowBuilder\n",
    "#from self_aware_trading_env import SelfAwareTradingEnv\n",
    "import pandas as pd\n",
    "\n",
    "class WindowedTrainingLoader:\n",
    "    def __init__(self, raw_df, context_days=40, target_days=20, window_step=20):\n",
    "        self.raw_df = raw_df\n",
    "        self.context_days = context_days\n",
    "        self.target_days = target_days\n",
    "        self.window_step = window_step\n",
    "        self.compressor = FeatureCompressor(window=10)\n",
    "\n",
    "    def get_training_environments(self):\n",
    "        envs = []\n",
    "        for ticker, group in self.raw_df.groupby(\"symbol\"):\n",
    "            group = group.sort_values(\"date\").reset_index(drop=True)\n",
    "            total_days = len(group)\n",
    "            start_indices = range(0, total_days - (self.context_days + self.target_days), self.window_step)\n",
    "\n",
    "            for start in start_indices:\n",
    "                context = group.iloc[start : start + self.context_days]\n",
    "                target = group.iloc[start + self.context_days : start + self.context_days + self.target_days]\n",
    "\n",
    "                if len(context) < self.context_days or len(target) < self.target_days:\n",
    "                    continue\n",
    "\n",
    "                # === APPLY FEATURE COMPRESSION ===\n",
    "                full = pd.concat([context, target])\n",
    "                compressed = self.compressor.transform(full)\n",
    "                target_compressed = compressed.iloc[self.context_days:].to_numpy()\n",
    "                \n",
    "                env = SelfAwareTradingEnv(\n",
    "                    market_array=target_compressed,\n",
    "                    sequence_length=10,\n",
    "                    episode_length=len(target_compressed)\n",
    "                )\n",
    "                envs.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"context\": context,\n",
    "                    \"target_env\": env\n",
    "                })\n",
    "\n",
    "        return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c755c04",
   "metadata": {},
   "source": [
    "| Component         | Purpose                                                   | Use Cases / Benefits                                    |\n",
    "| ----------------- | --------------------------------------------------------- | ------------------------------------------------------- |\n",
    "| `advantage_head`  | Predicts how much **edge** the agent expects from a state | Detect market opportunities or stale/noisy environments |\n",
    "| `confidence_head` | Predicts how **confident** the agent is in its decision   | Suppress overtrading in noisy periods                   |\n",
    "| Logging those     | Helps you **analyze** what the agent sees as risky, easy  | Visualize uncertainty over time or by ticker            |\n",
    "\n",
    "### Dev note: \n",
    "Decidi alimentar observation state com confidence_score para o cabr√£o se autoregular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4ecd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, seq_len=60, embed_dim=64, n_heads=4, n_layers=2):\n",
    "        super().__init__(observation_space, features_dim=embed_dim + 1)  # +1 for confidence\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.input_dim = observation_space.shape[-1]\n",
    "        self.embedding = nn.Linear(self.input_dim, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Confidence between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        B, T, F = obs.shape  # Batch, Time, Features\n",
    "        x = self.embedding(obs)\n",
    "        x = x + self.pos_embedding[:, :T, :]\n",
    "        x = self.encoder(x)\n",
    "        x_last = x[:, -1]\n",
    "        confidence = self.confidence_head(x_last)\n",
    "        return torch.cat([x_last, confidence], dim=-1)\n",
    "\n",
    "\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "class SelfAwareTransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            features_extractor_class=TransformerFeatureExtractor,\n",
    "            features_extractor_kwargs=dict(seq_len=60, embed_dim=64, n_heads=4, n_layers=2)\n",
    "        )\n",
    "\n",
    "    def extract_features(self, obs: torch.Tensor):\n",
    "        return self.features_extractor(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8770d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "class TrainingRunner:\n",
    "    def __init__(self, raw_ohlcv_df, model_save_path=\"self_aware_agent\", total_timesteps=10_000):\n",
    "        self.raw_df = raw_ohlcv_df\n",
    "        self.model_save_path = model_save_path\n",
    "        self.total_timesteps = total_timesteps\n",
    "\n",
    "    def run(self):\n",
    "        print('Run start')\n",
    "        loader = WindowedTrainingLoader(self.raw_df)\n",
    "        print('Loader initialized')\n",
    "        episodes = loader.get_training_environments()\n",
    "        print('Episodes loaded ',len(episodes))\n",
    "\n",
    "        # Resume logic\n",
    "        journal_path = \"training_journal.csv\"\n",
    "        if os.path.exists(journal_path):\n",
    "            completed_windows = set(pd.read_csv(journal_path)['window'].unique())\n",
    "            all_metrics = [pd.read_csv(journal_path)]\n",
    "        else:\n",
    "            completed_windows = set()\n",
    "            all_metrics = []\n",
    "\n",
    "        for i, episode in enumerate(tqdm(episodes, desc=\"Training windows\")):\n",
    "            window_id = i + 1\n",
    "            model_path = os.path.join(self.model_save_path, f\"agent_window_{window_id}.zip\")\n",
    "\n",
    "            if window_id in completed_windows or os.path.exists(model_path):\n",
    "                print(f\"‚è© Skipping window {window_id}, already completed.\")\n",
    "                continue\n",
    "\n",
    "            context_df = episode['context']\n",
    "            env = DummyVecEnv([lambda: Monitor(episode['target_env'])])\n",
    "\n",
    "            model = RecurrentPPO(\n",
    "                policy=SelfAwareTransformerPolicy,\n",
    "                env=env,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            print(f\"\\nTraining window {window_id}/{len(episodes)}\")\n",
    "\n",
    "            callback = EpisodeLoggerCallback()\n",
    "            model.learn(total_timesteps=self.total_timesteps, callback=callback)\n",
    "\n",
    "            os.makedirs(self.model_save_path, exist_ok=True)\n",
    "            model.save(model_path)\n",
    "            print('Saved model @ '+model_path)\n",
    "\n",
    "            # Append and immediately persist metrics for crash resilience\n",
    "            episode_df = pd.DataFrame(callback.episode_data)\n",
    "            episode_df['window'] = window_id\n",
    "            all_metrics.append(episode_df)\n",
    "\n",
    "            pd.concat(all_metrics, ignore_index=True).to_csv(journal_path, index=False)\n",
    "\n",
    "        print(\"\\n‚úÖ Training complete. Journal saved to 'training_journal.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6318ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notification = Notify('Self aware trading agent')\n",
    "notification.success('Lets go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1220407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run start\n",
      "Loader initialized\n",
      "Episodes loaded  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training windows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Skipping window 1, already completed.\n",
      "‚è© Skipping window 2, already completed.\n",
      "\n",
      "‚úÖ Training complete. Journal saved to 'training_journal.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df = load_base_dataframe()\n",
    "raw_df = raw_df.sort_values(\"date\").reset_index(drop=True)\n",
    "raw_df = raw_df[(raw_df['date']>=\"2025-03-01\") & (raw_df['date']<'2025-06-01')]\n",
    "raw_df = raw_df[raw_df['symbol'].isin(['AAPL','GOOGL'])]\n",
    "\n",
    "# TRAIN THIS DUDE ====================================\n",
    "runner = TrainingRunner(\n",
    "    raw_ohlcv_df=raw_df,\n",
    "    model_save_path=\"self_aware_transformer\",\n",
    "    total_timesteps=10_000 \n",
    ")\n",
    "notification.info('Train started')\n",
    "runner.run()\n",
    "notification.info('Train complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e622bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketVersusWalletHistoryTracker:\n",
    "    def __init__(self, initial_wallet=1.0):\n",
    "        self.wallet_value = initial_wallet\n",
    "        self.prev_wallet_value = initial_wallet\n",
    "        self.wallet_locked = False\n",
    "        self.buy_price = None\n",
    "        self.market_entry_price = None\n",
    "        self.last_price = None\n",
    "        self.has_opened_position = False  # NEW: ensure proper update after first buy\n",
    "\n",
    "        self.wallet_history = []\n",
    "        self.market_history = []\n",
    "        self.price_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset(self, initial_price):\n",
    "        self.__init__(initial_wallet=1.0)\n",
    "        self.market_entry_price = initial_price\n",
    "        self.last_price = initial_price\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.market_history.append(1.0)\n",
    "        self.price_history.append(initial_price)\n",
    "        self.action_history.append(0)\n",
    "\n",
    "    def step(self, action, current_price):\n",
    "        self.price_history.append(current_price)\n",
    "        agent_action = 0\n",
    "\n",
    "        # === 1. Update market benchmark ===\n",
    "        market_perf = current_price / self.market_entry_price\n",
    "        self.market_history.append(market_perf)\n",
    "\n",
    "        # === 2. Update wallet value ===\n",
    "        if self.wallet_locked and self.has_opened_position:\n",
    "            self.wallet_value *= current_price / self.last_price\n",
    "\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.prev_wallet_value = self.wallet_value\n",
    "        self.last_price = current_price  # must be set after wallet update!\n",
    "\n",
    "        # === 3. Process Action ===\n",
    "        if action == 1 and not self.wallet_locked:\n",
    "            self.buy_price = current_price\n",
    "            self.wallet_locked = True\n",
    "            self.has_opened_position = True\n",
    "            agent_action = 1\n",
    "\n",
    "        elif action == 2 and self.wallet_locked:\n",
    "            self.wallet_locked = False\n",
    "            self.buy_price = None\n",
    "            self.has_opened_position = False\n",
    "            agent_action = 2\n",
    "\n",
    "        self.action_history.append(agent_action)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"wallet_history\": self.wallet_history,\n",
    "            \"market_history\": self.market_history,\n",
    "            \"market_price_history\": self.price_history,\n",
    "            \"performed_action_history\": self.action_history\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1368ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class EvaluationRunner:\n",
    "    def __init__(self, raw_df, model_path=\"self_aware_transformer\"):\n",
    "        self.raw_df = raw_df\n",
    "        self.model_path = model_path\n",
    "        self.compressor = FeatureCompressor(window=10)\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting evaluation...\")\n",
    "        loader = WindowedTrainingLoader(self.raw_df)\n",
    "        windows = loader.get_training_environments()\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, episode in enumerate(tqdm(windows, desc=\"Evaluating windows\")):\n",
    "            window_id = i + 1\n",
    "            model_file = os.path.join(self.model_path, f\"agent_window_{window_id}.zip\")\n",
    "            if not os.path.exists(model_file):\n",
    "                print(f\"‚ö†Ô∏è Model for window {window_id} not found, skipping.\")\n",
    "                continue\n",
    "\n",
    "            env = DummyVecEnv([lambda: Monitor(episode['target_env'])])\n",
    "            model = RecurrentPPO.load(model_file, env=env, policy=SelfAwareTransformerPolicy)\n",
    "\n",
    "            obs = env.reset()\n",
    "            lstm_states = None\n",
    "            done = False\n",
    "            rewards = []\n",
    "            actions = []\n",
    "            confidences = []\n",
    "\n",
    "            tracker = MarketVersusWalletHistoryTracker()\n",
    "            tracker.reset(initial_price=episode['target_env'].market_data[0, 0])\n",
    "\n",
    "            while not done:\n",
    "                action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "                rewards.append(reward[0])\n",
    "                actions.append(action[0])\n",
    "\n",
    "                current_price = episode['target_env'].market_data[\n",
    "                    min(episode['target_env'].current_step, len(episode['target_env'].market_data) - 1), 0\n",
    "                ]\n",
    "                tracker.step(action[0], current_price)\n",
    "\n",
    "                confidence = obs[0, -1, -1].item()\n",
    "                confidences.append(confidence)\n",
    "\n",
    "            cumulative_reward = np.sum(rewards)\n",
    "            mean_confidence = np.mean(confidences)\n",
    "            num_trades = np.sum(np.array(actions) != 2)\n",
    "\n",
    "            export = tracker.export()\n",
    "\n",
    "            all_results.append({\n",
    "                \"window\": window_id,\n",
    "                \"ticker\": episode[\"ticker\"],\n",
    "                \"cumulative_reward\": cumulative_reward,\n",
    "                \"mean_confidence\": mean_confidence,\n",
    "                \"num_trades\": num_trades,\n",
    "                \"buy_count\": np.sum(np.array(export['performed_action_history']) == 1),\n",
    "                \"sell_count\": np.sum(np.array(export['performed_action_history']) == 2),\n",
    "                \"wallet_history\": export['wallet_history'],\n",
    "                \"market_history\": export['market_history'],\n",
    "                \"price_history\": export['market_price_history'],\n",
    "                \"action_history\": export['performed_action_history']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(all_results)\n",
    "        df.to_csv(\"evaluation_report.csv\", index=False)\n",
    "        all_results_dict =json.dumps(all_results)\n",
    "        json_object = json.dumps(all_results_dict, indent=4)\n",
    "\n",
    "        # Writing to sample.json\n",
    "        with open(\"evaluation_history.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "        print(\"‚úÖ Evaluation report saved to evaluation_report.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55fff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating windows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'window': 1, 'ticker': 'AAPL', 'cumulative_reward': 0.0, 'mean_confidence': 0.45000000223517417, 'num_trades': 0, 'buy_count': 0, 'sell_count': 0, 'wallet_history': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'market_history': [1.0, -0.039999999999997725, 0.2, -0.4400000000000091, 0.3480000000000018, -0.3239999999999895, -1.2319999999999935, 0.2600000000000023, 0.6420000000000072, 0.7639999999999987, 1.0], 'price_history': [2.5, -0.09999999999999432, 0.5, -1.1000000000000227, 0.8700000000000045, -0.8099999999999739, -3.079999999999984, 0.6500000000000057, 1.6050000000000182, 1.9099999999999966, 2.5], 'action_history': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'window': 2, 'ticker': 'GOOGL', 'cumulative_reward': 1.9178619, 'mean_confidence': 0.45000000223517417, 'num_trades': 3, 'buy_count': 1, 'sell_count': 0, 'wallet_history': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -4.683760683760636, -1.2649572649572636], 'market_history': [1.0, 7.310810810810716, -2.540540540540503, -2.0743243243242944, 2.743243243243211, -3.310810810810793, 6.581081081081006, -1.3243243243242941, -0.7905405405405416, 3.7027027027026693, 1.0], 'price_history': [0.7400000000000091, 5.409999999999997, -1.8799999999999955, -1.5349999999999966, 2.030000000000001, -2.450000000000017, 4.8700000000000045, -0.9799999999999898, -0.585000000000008, 2.740000000000009, 0.7400000000000091], 'action_history': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}]\n",
      "‚úÖ Evaluation report saved to evaluation_report.csv\n"
     ]
    }
   ],
   "source": [
    "#raw_df = load_base_dataframe()\n",
    "#raw_df = raw_df.sort_values(\"date\").reset_index(drop=True)\n",
    "#raw_df = raw_df[(raw_df['date']>=\"2024-12-01\") & (raw_df['date']<'2025-06-01')]\n",
    "notification.info('Test started')\n",
    "eval_runner = EvaluationRunner(raw_df)\n",
    "eval_runner.run()\n",
    "notification.info('Test complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "829c9f6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'evaluation_history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m openfile:\n\u001b[0;32m      5\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(openfile)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_object)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'evaluation_history.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"evaluation_history.json\", 'r') as openfile:\n",
    "\n",
    "    json_object = json.load(openfile)\n",
    "    \n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4c7a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 'A', 0.0, 0.0, 0.0], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf = pd.read_csv(\"evaluation_results.csv\")\n",
    "edf.iloc[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8601a",
   "metadata": {},
   "source": [
    "Hell yes ‚Äî we‚Äôve built a killer POC already, but now we enter the fun zone: **tightening screws, pushing limits, and future-proofing**.\n",
    "\n",
    "Here are my **top strategic suggestions** to improve the system:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ 1. **Allow Agent to See the Context Directly**\n",
    "\n",
    "Right now the agent **only trains on `target_df`** ‚Äî even though the Transformer *could* learn from the `context_df`.\n",
    "\n",
    "### üîß Option:\n",
    "\n",
    "Concatenate `context_df + target_df` into one episode:\n",
    "\n",
    "* Train the agent only on rewards from the `target_df` portion (e.g. via masking or zero rewards during context)\n",
    "* Allows the Transformer to **build internal market memory naturally**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. **Auxiliary Heads: Meta-Predictions**\n",
    "\n",
    "Add small auxiliary outputs to the Transformer that:\n",
    "\n",
    "* Predict future volatility\n",
    "* Estimate next step reward\n",
    "* Classify regime (trend, chop, revert)\n",
    "\n",
    "Why?\n",
    "\n",
    "> Forces the encoder to learn **richer representations** beyond just immediate actions.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 3. **Regime-Based Curriculum**\n",
    "\n",
    "Instead of sampling all episodes equally:\n",
    "\n",
    "* Stratify by volatility or trend\n",
    "* Train easier episodes first\n",
    "* Slowly introduce difficult / noisy environments\n",
    "\n",
    "You can also **balance the regime mix** so the agent doesn't overfit to \"easy\" windows.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ 4. **Relative Advantage Evaluation**\n",
    "\n",
    "Instead of just measuring reward:\n",
    "\n",
    "* Compare to baseline strategies (random, hold)\n",
    "* Define `agent_advantage = reward_agent - reward_baseline`\n",
    "\n",
    "You already built the machinery ‚Äî just run each `target_df` twice and subtract.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 5. **RL Memory Injection (optional but powerful)**\n",
    "\n",
    "If we move to **RecurrentPPO with LSTM memory**:\n",
    "\n",
    "* Let the agent process `context_df` passively (no reward, no action)\n",
    "* Then start the episode normally with memory *initialized*\n",
    "\n",
    "This mimics how traders study history before acting.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è 6. **Online Deployment Skeleton**\n",
    "\n",
    "Later, you could wire this up for:\n",
    "\n",
    "* Daily retraining using recent data\n",
    "* Live execution of agent policy on top tickers\n",
    "* Continual journaling + self-analysis\n",
    "\n",
    "Use a rolling horizon evaluation loop with `context ‚Üí target` sliding forward by 1 week or 1 day.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Bonus Tactical Tips\n",
    "\n",
    "| Area              | Suggestion                                                      |\n",
    "| ----------------- | --------------------------------------------------------------- |\n",
    "| Logging           | Add `num_trades`, `avg_holding_time`, `win_rate` to journal     |\n",
    "| Feature Expansion | Add cross-ticker info: sector trend, market index return        |\n",
    "| Reward Design     | Add `position_penalty` to discourage churning                   |\n",
    "| Model Variants    | Try `Transformer + LSTM`, or `Temporal Convolution + Attention` |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Want to act on any of these now?\n",
    "\n",
    "I recommend either:\n",
    "\n",
    "1. Concatenating `context + target` into single episodes (with smart reward masking)\n",
    "2. Adding relative performance benchmarking (agent vs. random/hold)\n",
    "\n",
    "Let me know and I‚Äôll implement it right now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
