{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab69b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "WINDOW_LENGTH = 10  # or any value you want\n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 128,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "features_extractor_kwargs={\n",
    "    'window_length': WINDOW_LENGTH,\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'd_model': 32,\n",
    "    'nhead': ...,\n",
    "    'num_layers': ...,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Load data ---\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# --- Experiment tracker ---\n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b577449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(df, ticker, feature_cols, episode_length, window_length):\n",
    "    df_ticker = df[df['symbol'] == ticker].copy()\n",
    "    return CumulativeTradingEnv(\n",
    "        df=df_ticker,\n",
    "        feature_cols=feature_cols,\n",
    "        episode_length=episode_length,\n",
    "        transaction_cost=TRANSACTION_COST,\n",
    "        window_length=window_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f584434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, window_length, n_features, d_model=32, nhead=1, num_layers=1):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.window_length = window_length\n",
    "        self.n_features = n_features\n",
    "        self.embedding = nn.Linear(n_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: [batch, window_length * n_features]\n",
    "        batch = obs.shape[0]\n",
    "        # reshape flat vector to (batch, window_length, n_features)\n",
    "        x = obs.view(batch, self.window_length, self.n_features)\n",
    "        x = self.embedding(x)      # (batch, window_length, d_model)\n",
    "        x = x.permute(1, 0, 2)    # (window_length, batch, d_model)\n",
    "        x = self.transformer(x)    # (window_length, batch, d_model)\n",
    "        # Use last token as pooled output\n",
    "        return x[-1]              # (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df24b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, nhead=1, num_layers=1, window_length=WINDOW_LENGTH, n_features=2, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=TransformerExtractor,\n",
    "            features_extractor_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': n_features,\n",
    "                'd_model': 32,\n",
    "                'nhead': nhead,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986ac5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8: Learnability\n",
    "from src.env.realistic_synthetic_environment import realistic_synthetic_market_sample\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return self.env.action_space.sample(), {}\n",
    "\n",
    "class AlwaysLongAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return 1, {}  # Always go long\n",
    "    \n",
    "def evaluate_baseline_agent(env, agent, n_episodes=20, episode_sequence=None):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fda94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "def make_agent(agent_type, env, window_length, feature_cols, **kwargs):\n",
    "    if agent_type == 'mlp':\n",
    "        return PPO(\"MlpPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type == 'lstm':\n",
    "        return RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "        #return PPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type.startswith('transformer'):\n",
    "        \n",
    "        n_features = len(feature_cols)\n",
    "       \n",
    "        return PPO(\n",
    "            TransformerPolicy,\n",
    "            env,\n",
    "            verbose=0,\n",
    "            policy_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': env.observation_space.shape[1],\n",
    "                'nhead': 2,        # set as desired\n",
    "                'num_layers': 2,   # set as desired\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe15189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_episodes=10, episode_sequence=None, is_sb3=False):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs,_ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            if is_sb3:\n",
    "                action, _ = agent.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d848a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: mean -0.0240, std 0.1038\n",
      "Always Long: mean 0.1616, std 0.0000\n",
      "\n",
      "Training mlp agent...\n",
      "\n",
      "Training Env 1\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT AND SEQUENCES ======================\n",
    "df = ohlcv_df[(ohlcv_df['date'] >= \"2023-01-01\") & (ohlcv_df['date']<\"2025-01-01\")].copy()\n",
    "df = ohlcv_df[ohlcv_df['symbol']== \"AAPL\"]\n",
    "feature_cols = FEATURE_COLS\n",
    "env = SequenceAwareCumulativeTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "\n",
    "seq = env.generate_episode_sequences(train_steps=100)\n",
    "\n",
    "# Baseline agents =================================\n",
    "random_agent = RandomAgent(env)\n",
    "always_long_agent = AlwaysLongAgent(env)\n",
    "mean_rand, std_rand = evaluate_agent(env, random_agent, n_episodes=20, episode_sequence=seq)\n",
    "mean_long, std_long = evaluate_agent(env, always_long_agent, n_episodes=20, episode_sequence=seq)\n",
    "print(f\"Random: mean {mean_rand:.4f}, std {std_rand:.4f}\")\n",
    "print(f\"Always Long: mean {mean_long:.4f}, std {std_long:.4f}\")\n",
    "\n",
    "# RL agents =======================================\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "#AGENT_TYPES = ['transformer_single', 'transformer_multi']\n",
    "for agent_type in AGENT_TYPES:\n",
    "    print(f\"\\nTraining {agent_type} agent...\")\n",
    "    env = SequenceAwareCumulativeTradingEnv(\n",
    "        df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "    env2 = SequenceAwareCumulativeTradingEnv(\n",
    "        df, feature_cols=['return_1d','volume'], episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "    env.set_episode_sequence(seq)\n",
    "    env2.set_episode_sequence(seq)\n",
    "    print(f\"\\nTraining Env 1\")\n",
    "    model = make_agent(agent_type, env, window_length=WINDOW_LENGTH, feature_cols=feature_cols, n_steps=EPISODE_LENGTH, batch_size=4)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "    mean_rl, std_rl = evaluate_agent(env, model, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "    print(f\"ALL FEATURES {agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")\n",
    "    print(f\"\\nTraining Env 2\")\n",
    "    model = make_agent(agent_type, env2, window_length=WINDOW_LENGTH, feature_cols=[\"return_1d\",\"volume\"], n_steps=EPISODE_LENGTH, batch_size=4)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "    mean_rl, std_rl = evaluate_agent(en2v, model, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "    print(f\"2 FEATURES {agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3394327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8125a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
