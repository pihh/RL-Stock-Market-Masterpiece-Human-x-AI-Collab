{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP: Imports & Paths ===========================\n",
    "import jupyter\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.feature_pipeline import basic_chart_features,load_base_dataframe\n",
    "from src.predictability.easiness import rolling_sharpe, rolling_r2, rolling_info_ratio, rolling_autocorr\n",
    "from src.predictability.pipeline import generate_universe_easiness_report\n",
    "from IPython import display\n",
    "\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.config import TOP2_STOCK_BY_SECTOR\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import acf, acovf\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "from IPython.display import display\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_trading_env import CumulativeTradingEnv,AlphaTradingEnv\n",
    "from src.env.base_timeseries_trading_env import SequenceAwareAlphaTradingEnv,SequenceAwareCumulativeTradingEnv\n",
    "from src.agent.base_model import TransformerPpo\n",
    "from src.defaults import RANDOM_SEEDS\n",
    "from src.utils.db import ConfigurableMixin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIENCE_NAME = \"episode_learning_transferability\"\n",
    "FEATURE_COLS= [\"return_1d\", \"volume\", \"vix\"]\n",
    "# Prepare your dataframes for two consecutive episodes:\n",
    "SEEDS = RANDOM_SEEDS\n",
    "EPISODE_LENGTH = 21\n",
    "WINDOW_LENGTH = EPISODE_LENGTH*2\n",
    "N_UPDATES = 10\n",
    "\n",
    "excluded_tickers=['CEG', 'GEHC', 'GEV', 'KVUE', 'SOLV']\n",
    "excluded_tickers.sort()\n",
    "\n",
    "config={\n",
    "    \"excluded_tickers\": excluded_tickers,\n",
    "    \"episode_length\":EPISODE_LENGTH,\n",
    "    \"window_length\":WINDOW_LENGTH,\n",
    "    \"n_updates\":N_UPDATES,\n",
    "    \"agent\":\"TransformerPpoAgent\",\n",
    "    \"environment\":\"SequenceAwareCumulativeTradingEnv\"\n",
    "}\n",
    "run_settings={\n",
    "    \"excluded_tickers\": excluded_tickers,\n",
    "    \"start_date\": '2022-01-01',\n",
    "    \"end_date\":\"2026-01-01\",\n",
    "    \"episode_length\":EPISODE_LENGTH,\n",
    "    \"window_length\":WINDOW_LENGTH,\n",
    "    \"n_updates\":N_UPDATES\n",
    "}\n",
    "\n",
    "# Config section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd2209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b684e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD OHLCV ==========================================\n",
    "\n",
    "\n",
    "ohlcv_df = load_base_dataframe()\n",
    "ohlcv_df['date'] = pd.to_datetime(ohlcv_df['date'])\n",
    "ohlcv_df = ohlcv_df[(ohlcv_df['date'] >= run_settings[\"start_date\"]) & (ohlcv_df['date'] < run_settings[\"end_date\"])]\n",
    "ohlcv_df['month'] = ohlcv_df['date'].dt.to_period('M')\n",
    "ohlcv_df['return_1d'] = ohlcv_df['return_1d'].fillna(0)\n",
    "ohlcv_df['sector_id'] = ohlcv_df['sector_id'].fillna('unknown')\n",
    "ohlcv_df['industry_id'] = ohlcv_df['industry_id'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_df = load_base_dataframe()\n",
    "notification = Notify(EXPERIENCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8facb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "def sinusoidal_positional_encoding(seq_len, d_model, device):\n",
    "    \"\"\"Returns a (seq_len, d_model) matrix with classic Transformer sin-cos encoding.\"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model, device=device)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=64, n_heads=4, n_layers=2):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "        input_dim = observation_space.shape[-1]\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: (batch, seq_len, input_dim)\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        device = x.device\n",
    "        pe = sinusoidal_positional_encoding(seq_len, self.d_model, device)\n",
    "        x = x + pe.unsqueeze(0)  # (1, seq_len, d_model) for broadcasting\n",
    "        causal_mask = generate_causal_mask(seq_len).to(device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "        pooled_output = x[:, -1]\n",
    "        return pooled_output\n",
    "\n",
    "# Transformer Policy ===================================\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs,\n",
    "                         features_extractor_class=TransformerFeatureExtractor,\n",
    "                         features_extractor_kwargs=dict(\n",
    "                             d_model=64, n_heads=4, n_layers=2\n",
    "                         ))\n",
    "        #self._build(self.lr_schedule)\n",
    "\n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ca2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import json\n",
    "import hashlib\n",
    "def encode_hash(model_id,environment_id,train_episode_id,test_episode_id,n_updates):\n",
    "    xx = json.dumps({\n",
    "        \"n_updates\":n_updates,\n",
    "        \"model_id\":model_id,\n",
    "        \"environment_id\":environment_id,\n",
    "        \"train_episode_id\":train_episode_id,\n",
    "        \"test_episode_id\":test_episode_id\n",
    "    }, sort_keys=True) \n",
    "    return hashlib.sha256(xx.encode()).hexdigest()\n",
    "                         \n",
    "\n",
    "def test_transferability(\n",
    "    env_maker, agent_maker,df,\n",
    "    episode_sequence_train, episode_sequence_test, feature_cols, episode_length,\n",
    "    episode_id_train,episode_id_test,\n",
    "    n_updates=8, window_length=10, random_seeds=[7,314],\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train agent on episode A, test on episode B, N times (new seed each time).\n",
    "    Compares performance to random on episode B.\n",
    "    \"\"\"\n",
    "    agent_rewards_B, random_rewards_B = [], []\n",
    "    total_timesteps = episode_length * n_updates\n",
    "    run_results = []\n",
    "  \n",
    "\n",
    " \n",
    "    e = env_maker(1)\n",
    "    a = agent_maker(e,1)\n",
    "    environment_id = e.db_id\n",
    "    model_id = a.db_id\n",
    "    \n",
    "    csv_path = EXPERIENCE_NAME + '.csv'\n",
    "    run_hash = encode_hash(model_id,environment_id,episode_id_train,episode_id_test,n_updates)\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    if not df.empty and (df['hash'] ==run_hash).any():\n",
    "        return [],[],0,0,{}\n",
    "        #df = pd.concat([df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "        #df.to_csv(csv_path, index=False)\n",
    "        #print(\"Row appended.\")\n",
    "    \n",
    "    for run in range(len(random_seeds)):\n",
    "        seed = random_seeds[run]\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # --- Train agent on Episode A ---\n",
    "        envA = env_maker(seed)\n",
    "        envA.set_episode_sequence(episode_sequence_train)  # or your symbol/start tuple\n",
    "   \n",
    "        model = agent_maker(envA,seed)\n",
    "        agent = model.agent\n",
    "        agent.learn(total_timesteps=total_timesteps)\n",
    "   \n",
    "\n",
    "        # --- Test trained agent on Episode B ---\n",
    "    \n",
    "        envB = env_maker(seed)\n",
    "        envB.set_episode_sequence(episode_sequence_test)\n",
    "        obs, _ = envB.reset()\n",
    "        rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, truncated, info = envB.step(action)\n",
    "            rewards.append(reward)\n",
    "        agent_rewards_B.append(np.sum(rewards))\n",
    "\n",
    "        # --- Random policy on Episode B (for reference) ---\n",
    "        envB_rand = env_maker(seed)\n",
    "        envB_rand.set_episode_sequence(episode_sequence_test)\n",
    "        obs, _ = envB_rand.reset()\n",
    "        rand_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = envB_rand.action_space.sample()\n",
    "            obs, reward, done, truncated, info = envB_rand.step(action)\n",
    "            rand_rewards.append(reward)\n",
    "        random_rewards_B.append(np.sum(rand_rewards))\n",
    "\n",
    "        run_config= {\n",
    "            \"train_episode_id\":envA.db_id,\n",
    "            \"test_episode_id\":envB.db_id,\n",
    "            \"agent_id\":model.db_id,\n",
    "            \"test_rand_reward\":np.sum(rand_rewards),\n",
    "            \"test_agent_reward\":np.sum(rewards),\n",
    "        }\n",
    "        run_results.append(run_config)\n",
    "        if verbose:\n",
    "            print(f\"[Run {run+1}/{len(random_seeds)}] Agent (on B): {agent_rewards_B[-1]:.4f}, Random (on B): {random_rewards_B[-1]:.4f}\")\n",
    "\n",
    "    # --- Statistical testing on Episode B ---\n",
    "    agent_rewards_B = np.array(agent_rewards_B)\n",
    "    random_rewards_B = np.array(random_rewards_B)\n",
    "\n",
    "    t_stat, t_pval = ttest_ind(agent_rewards_B, random_rewards_B, equal_var=False)\n",
    "    mw_stat, mw_pval = mannwhitneyu(agent_rewards_B, random_rewards_B, alternative='greater')\n",
    "\n",
    "    print(\"\\n==== Transferability Significance Results (Episode B) ====\")\n",
    "    print(f\"Agent mean (B): {agent_rewards_B.mean():.4f} ± {agent_rewards_B.std():.4f}\")\n",
    "    print(f\"Random mean (B): {random_rewards_B.mean():.4f} ± {random_rewards_B.std():.4f}\")\n",
    "    print(f\"T-test p-value: {t_pval:.4g}\")\n",
    "    print(f\"Mann-Whitney U p-value: {mw_pval:.4g}\")\n",
    "    outcome = \"\"\n",
    "    if t_pval < 0.05:\n",
    "        outcome = \"Transferable: Agent outperforms random with **statistical significance** (t-test)!\"\n",
    "    else:\n",
    "        outcome = \"No significant transfer detected (t-test).\"\n",
    "    if mw_pval < 0.05:\n",
    "        outcome = \"Transferable: Agent outperforms random with **statistical significance** (Mann-Whitney U)!\"\n",
    "    else:\n",
    "        outcome = \"No significant transfer detected (Mann-Whitney U).\"\n",
    "    print(outcome)\n",
    "    envA.episode_df.iloc[envA.window_length]['date']\n",
    "    date_str = str(envA.episode_df.iloc[envA.window_length]['date'].date())\n",
    "    train_date = date_str\n",
    "    envB.episode_df.iloc[envB.window_length]['date']\n",
    "    date_str = str(envB.episode_df.iloc[envB.window_length]['date'].date())\n",
    "    test_date = date_str\n",
    "    study_result = {\n",
    "        \"experience_name\": EXPERIENCE_NAME,\n",
    "        \"config\": config,\n",
    "        \"agent_id\":model.db_id,\n",
    "        \"environment_id\": envA.db_id,\n",
    "        \"train_date\":train_date,\n",
    "        \"test_date\":test_date,\n",
    "        \"run_settings\": run_settings,\n",
    "        \"train_episode_id\": episode_id_train,\n",
    "        \"test_episode_id\": episode_id_test,\n",
    "        \"feature_cols\": envA.feature_cols, \n",
    "        \"episode_length\": EPISODE_LENGTH,\n",
    "        \"window_length\": WINDOW_LENGTH,\n",
    "        \"random_seeds\": RANDOM_SEEDS,\n",
    "        \n",
    "        'advantage':  np.median(agent_rewards_B)-np.median(random_rewards_B),\n",
    "        \"agent_rewards_mean\": np.median(agent_rewards_B),\n",
    "        \"agent_rewards_std\": agent_rewards_B.std(),\n",
    "        \n",
    "        \"random_rewards_mean\": np.median(random_rewards_B),\n",
    "        \"random_rewards_std\": random_rewards_B.std(),\n",
    "        \"total_timesteps\":total_timesteps,\n",
    "        \"t_pval\": float(t_pval),\n",
    "        \"mw_pval\": float(mw_pval),\n",
    "        \n",
    "        \"symbol\":episode_sequence_train[0][0],\n",
    "        \"date\": episode_sequence_train[0][1],\n",
    "        \"hash\":run_hash,\n",
    "        \"agent_rewards\":json.dumps(agent_rewards_B.tolist()),\n",
    "        \"random_rewards\": json.dumps(random_rewards_B.tolist())\n",
    "    }\n",
    "    if t_pval < 1.05 and mw_pval < 1.05:\n",
    "        notification.success(f\"Favorable episode for {episode_sequence_train[0][0]} @ {train_date}\")\n",
    "        \n",
    "    df = pd.concat([df, pd.DataFrame([study_result])], ignore_index=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return agent_rewards_B, random_rewards_B, t_pval, mw_pval,study_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab458321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c06da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_maker(seed):\n",
    "    environment= SequenceAwareCumulativeTradingEnv(\n",
    "        ohlcv_df,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        episode_length=EPISODE_LENGTH, \n",
    "        transaction_cost=0, \n",
    "        seed=seed, \n",
    "        window_length=WINDOW_LENGTH )\n",
    "    return environment\n",
    "    \n",
    "def agent_maker(environment,seed):\n",
    "    agent = TransformerPpo(\n",
    "        RecurrentPPO,\n",
    "        TransformerPolicy,\n",
    "        environment,\n",
    "        model_config={\n",
    "            \"n_steps\":EPISODE_LENGTH,\n",
    "            \"batch_size\":EPISODE_LENGTH,\n",
    "            \n",
    "            }, run_config={\"seed\":seed,\"verbose\":0})\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab473d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def month_ranges(start_date_str):\n",
    "    # Parse start date\n",
    "    start = pd.Timestamp(start_date_str).replace(day=1)\n",
    "    # Get first day of *next* month after today\n",
    "    today = pd.Timestamp.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    next_month = (today + pd.offsets.MonthBegin(1)).replace(day=1)\n",
    "    final_month = (next_month + pd.offsets.MonthBegin(1)).replace(day=1)\n",
    "    \n",
    "    ranges = []\n",
    "    d = start\n",
    "    while d < final_month:\n",
    "        next_d = (d + pd.offsets.MonthBegin(1)).replace(day=1)\n",
    "        ranges.append([d.strftime('%Y-%m-%d'), next_d.strftime('%Y-%m-%d')])\n",
    "        d = next_d\n",
    "    return ranges\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = month_ranges('2024-01-01')\n",
    "tickers = TOP2_STOCK_BY_SECTOR\n",
    "for ticker in tickers:\n",
    "    run_start_time = time.time()\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    notification.info(f\"Train start for {ticker} | {timestamp}\")\n",
    "    \n",
    "    for p in pairs:\n",
    "        print('train '+ticker+' '+p[0]+' '+p[1])\n",
    "        train_env = SequenceAwareCumulativeTradingEnv(\n",
    "            ohlcv_df,\n",
    "            feature_cols=FEATURE_COLS,\n",
    "            episode_length=EPISODE_LENGTH, \n",
    "            transaction_cost=0, \n",
    "            seed=RANDOM_SEEDS[0], \n",
    "            window_length=WINDOW_LENGTH \n",
    "        )\n",
    "        _,start_train,id_train = train_env.get_episode_by_start_date(ticker,p[0])\n",
    "        _,start_test,id_test  = train_env.get_episode_by_start_date(ticker,p[1])\n",
    "\n",
    "        train_sequence = [[ticker,start_train]]\n",
    "        test_sequence = [[ticker,start_test]]\n",
    "        agent_rewards, random_rewards, t_pval, mw_pval,results = test_transferability(\n",
    "            env_maker,\n",
    "            agent_maker,\n",
    "            ohlcv_df,\n",
    "            train_sequence,\n",
    "            test_sequence,\n",
    "            feature_cols=FEATURE_COLS,\n",
    "            episode_length=EPISODE_LENGTH,\n",
    "            random_seeds=RANDOM_SEEDS,\n",
    "            n_updates=N_UPDATES,\n",
    "            window_length=WINDOW_LENGTH,\n",
    "            episode_id_train=id_train,\n",
    "            episode_id_test=id_test\n",
    "        )\n",
    "   \n",
    "    run_end_time = time.time()\n",
    "    elapsed = run_end_time - run_start_time\n",
    "    elapsed_str = f\"{elapsed:.1f}s\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    " \n",
    "    notification.info(f\"Train complete for {ticker} | {timestamp} | Exec time: {elapsed_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681c1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ee95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.read_csv(EXPERIENCE_NAME+'.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4709a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx\n",
    "agent_rewards, random_rewards, t_pval, mw_pval,results = test_transferability(\n",
    "    env_maker,\n",
    "    agent_maker,\n",
    "    ohlcv_df,\n",
    "    train_sequence,\n",
    "    test_sequence,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    random_seeds=RANDOM_SEEDS,\n",
    "    n_updates=N_UPDATES,\n",
    "    window_length=WINDOW_LENGTH,\n",
    "    episode_id_train=id_train,\n",
    "    episode_id_test=id_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664d795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0af33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056adf22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
