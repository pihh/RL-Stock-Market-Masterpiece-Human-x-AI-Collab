{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "#  Core Sequence-Aware Agent Design v2\n",
    "\n",
    "This experiment explores a transformer-based recurrent PPO agent for financial trading. The environment is sequence-aware and includes both regime-based augmentation and per-episode reward normalization. The agent is evaluated across top 2 stocks in each sector using structured episode sequences to assess learning generalization.\n",
    "\n",
    "---\n",
    "\n",
    "##  Experiment Configuration\n",
    "\n",
    "| Parameter               | Value                         |\n",
    "|-------------------------|-------------------------------|\n",
    "| Agent                   | Recurrent PPO + Transformer   |\n",
    "| Env Wrapper             | RegimeAugmentingWrapper + PerEpisodeRewardNormalizer |\n",
    "| Episode Length          | 100                           |\n",
    "| Episodes                | 20                            |\n",
    "| Eval Episodes           | 3 per iteration               |\n",
    "| Steps per Update        | 800                           |\n",
    "| Batch Size              | 100                           |\n",
    "| Total Timesteps         | 20,000                        |\n",
    "| Learning Rate           | 0.0003                        |\n",
    "| Entropy Coefficient     | 0.005                         |\n",
    "| Value Function Coeff    | 0.5                           |\n",
    "| Max Gradient Norm       | 0.5                           |\n",
    "| Normalize Advantage     | True                          |\n",
    "| Optimizer               | Adam                          |\n",
    "| Transformer d_model     | 64                            |\n",
    "| Heads                  | 4                             |\n",
    "| Layers                 | 2                             |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- **Train Set:** 2023-01-01 → 2023-07-01\n",
    "- **Test Set:** 2023-07-01 → 2024-01-01\n",
    "- **Assets:** Top 2 stocks by sector\n",
    "- **Sequence Split:** 80% train / 20% eval sequences\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent Architecture\n",
    "\n",
    "- **Feature Extractor:** Transformer encoder with causal mask and learnable positional encoding.\n",
    "- **Policy Class:** Custom `TransformerPolicy` extending `RecurrentActorCriticPolicy`.\n",
    "- **Reward Normalization:** Online normalization within episodes.\n",
    "- **Regime Augmentation:** Appends one-hot encoded market regime to each timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Summary (Selected Stats)\n",
    "\n",
    "| Timesteps | Ep Rew Mean | Explained Variance | Value Loss | Policy Grad Loss |\n",
    "|-----------|-------------|--------------------|------------|------------------|\n",
    "|  8000     | 3.21        | 0.15               | 4.68       | -0.00277         |\n",
    "| 14400     | 2.90        | 0.645              | 1.71       |  0.00163         |\n",
    "| 20000     | **3.79**    | **0.751**          | 1.59       | -0.00012         |\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Snapshots\n",
    "\n",
    "| Timestep | Mean Reward | Std Dev | Eval Length |\n",
    "|----------|-------------|---------|-------------|\n",
    "| 5000     | -8.17       | ±8.72   | 102         |\n",
    "| 10000    | -4.43       | ±7.59   | 102         |\n",
    "| 15000    | -6.17       | ±4.92   | 102         |\n",
    "| 20000    | **0.24**    | ±2.86   | 102         |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "### Paired t-test and Mann-Whitney U-test\n",
    "\n",
    "| Metric       | t-test p-value | Mann-Whitney p-value |\n",
    "|--------------|----------------|-----------------------|\n",
    "| `total_reward` | 0.0300         | 0.0075                |\n",
    "| `calmar`       | 0.0132         | 0.0075                |\n",
    "\n",
    "✅ Both `total_reward` and `calmar ratio` show **statistically significant** improvements compared to the baseline.  \n",
    "Especially, Calmar implies **more stable and risk-adjusted returns**.\n",
    "\n",
    "> Note: `sharpe`, `sortino`, and `final_wealth` were skipped due to empty values in the evaluation logs. Ensure metrics are logged and valid across all test episodes to include them.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fix missing metrics** (`sharpe`, `sortino`, etc.) in the logging pipeline.\n",
    "2. **Plot distribution comparisons** (boxplots, histograms) for rewards and risk-adjusted returns.\n",
    "3. **Run ablation**:\n",
    "   - Without regime augmentation\n",
    "   - Without reward normalization\n",
    "   - With simpler agents (e.g., MLP or LSTM)\n",
    "4. **Test in unseen market conditions** or during volatility spikes to check robustness.\n",
    "\n",
    "---\n",
    "\n",
    "_Logged using `ExperimentTracker` — Run Hash: `${experiment_tracker.run_hash}`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550c1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design_v2\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "WINDOW_LENGTH = 10  \n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 800,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"project_name\":EXPERIENCE_NAME\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data ==================================\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# Experiment tracker ========================= \n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "experiment_tracker.set_hash(CONFIG)\n",
    "\n",
    "# Files ======================================\n",
    "checkpoint_path = \"/data/checkpoint\" \n",
    "checkpoint_name = experiment_tracker.run_hash\n",
    "checkpoint_preffix = f\"{checkpoint_name}--checkpoint\"\n",
    "checkpoint_best_model=f\"{checkpoint_path}/{checkpoint_name}--best_model\"\n",
    "log_path=\"/data/logs\"\n",
    "save_path= checkpoint_path+ f\"{checkpoint_path}/{checkpoint_name}--final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 100\n",
    "MAX_LENGTH = 200\n",
    "SAVE_FREQ=10000\n",
    "EVAL_FREQ=5000\n",
    "TOTAL_TIMESTEPS=200000\n",
    "#TOTAL_TIMESTEPS=1000\n",
    "EPISODES_PER_UPDATE = 8          # ~how many episodes before PPO updates\n",
    "EPISODES_PER_BATCH = 1           # number of full episodes per batch\n",
    "\n",
    "# === Auto-derive PPO settings ===\n",
    "N_STEPS = EPISODE_LENGTH * EPISODES_PER_UPDATE\n",
    "BATCH_SIZE = EPISODE_LENGTH * EPISODES_PER_BATCH\n",
    "\n",
    "ENV_CLASS = SequenceAwareCumulativeTradingEnv\n",
    "\n",
    "n = Notify(experiment_tracker.project)\n",
    "n.info('START')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e51121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'regime': 2, 'episode_sharpe': -0.06863072175451103, 'episode_sortino': -0.0845715544096993, 'episode_total_reward': -0.47521371290970316, 'calmar': -0.9625190272271583, 'max_drawdown': 0.40562452738811433, 'win_rate': 0.0, 'alpha': -0.5045100480485901, 'returns': array([-0.01805754, -0.02094398, -0.02538383,  0.02415459,  0.01897047,\n",
      "       -0.02445406, -0.01959975, -0.0160866 , -0.07278702, -0.02747355,\n",
      "        0.01794236, -0.04606414, -0.07683017, -0.04387642,  0.00463109,\n",
      "        0.00042297, -0.00073988, -0.02386988,  0.01475139, -0.01214533,\n",
      "        0.02713302, -0.00152522, -0.00829242, -0.01936407,  0.00830248,\n",
      "       -0.00211416, -0.00333111,  0.01814962,  0.01793478, -0.01580352,\n",
      "        0.01335015, -0.00415512, -0.02047745, -0.01830589, -0.00529626,\n",
      "       -0.03139063, -0.02277323, -0.04380397, -0.01295853,  0.00393623,\n",
      "       -0.03655404,  0.01477316,  0.02157299, -0.00151515,  0.02826252,\n",
      "        0.01395667,  0.0035633 , -0.00493145,  0.02814947,  0.00607346,\n",
      "       -0.0034496 , -0.00238732,  0.0378099 ,  0.01826231,  0.01024051,\n",
      "        0.00673944, -0.02666284, -0.01340408,  0.02663727, -0.01387185,\n",
      "       -0.0030622 ,  0.01065464, -0.00113971, -0.00531259, -0.02000566,\n",
      "       -0.0013481 , -0.03741201, -0.00232364, -0.00130427, -0.01753731,\n",
      "       -0.00085454,  0.00683047,  0.00840577,  0.02083925,  0.00389719,\n",
      "       -0.04098742, -0.04313199,  0.00806382,  0.00850991, -0.02574886,\n",
      "       -0.01112878, -0.01646806,  0.00056989,  0.01865428, -0.01991204,\n",
      "        0.01354439,  0.02133995,  0.00915128,  0.00754354,  0.00646883,\n",
      "        0.00691788,  0.00767863, -0.01074838,  0.01031664, -0.03778179,\n",
      "       -0.00293878,  0.00073686,  0.01153563,  0.00153672, -0.00129209,\n",
      "       -0.01148217,  0.01247102]), 'downside': array([-0.01805754, -0.02094398, -0.02538383, -0.02445406, -0.01959975,\n",
      "       -0.0160866 , -0.07278702, -0.02747355, -0.04606414, -0.07683017,\n",
      "       -0.04387642, -0.00073988, -0.02386988, -0.01214533, -0.00152522,\n",
      "       -0.00829242, -0.01936407, -0.00211416, -0.00333111, -0.01580352,\n",
      "       -0.00415512, -0.02047745, -0.01830589, -0.00529626, -0.03139063,\n",
      "       -0.02277323, -0.04380397, -0.01295853, -0.03655404, -0.00151515,\n",
      "       -0.00493145, -0.0034496 , -0.00238732, -0.02666284, -0.01340408,\n",
      "       -0.01387185, -0.0030622 , -0.00113971, -0.00531259, -0.02000566,\n",
      "       -0.0013481 , -0.03741201, -0.00232364, -0.00130427, -0.01753731,\n",
      "       -0.00085454, -0.04098742, -0.04313199, -0.02574886, -0.01112878,\n",
      "       -0.01646806, -0.01991204, -0.01074838, -0.03778179, -0.00293878,\n",
      "       -0.00129209, -0.01148217]), 'final_wealth': 0.6095786744789163, 'action_hold_count': 30, 'action_buy_count': 34, 'action_sell_count': 38, 'cumulative_return': -0.39042132552108366}\n",
      "{'regime': 2, 'episode_sharpe': 0.029742412894913046, 'episode_sortino': 0.051690978109951526, 'episode_total_reward': 0.04165298566002343, 'calmar': 0.10552537018926655, 'max_drawdown': 0.19637045636883343, 'win_rate': 0.0, 'alpha': -0.0773486372707175, 'returns': array([-0.00454442, -0.03505994,  0.00426136, -0.0108636 , -0.00900999,\n",
      "       -0.0018622 ,  0.02118086,  0.05341214,  0.01805754, -0.02094398,\n",
      "       -0.02538383, -0.02415459, -0.01897047,  0.02445406,  0.01959975,\n",
      "        0.0160866 ,  0.07278702, -0.02747355,  0.01794236, -0.04606414,\n",
      "        0.07683017,  0.04387642,  0.00463109,  0.00042297,  0.00073988,\n",
      "        0.02386988,  0.01475139,  0.01214533, -0.02713302, -0.00152522,\n",
      "       -0.00829242, -0.01936407,  0.00830248,  0.00211416,  0.00333111,\n",
      "       -0.01814962,  0.01793478,  0.01580352, -0.01335015, -0.00415512,\n",
      "       -0.02047745, -0.01830589, -0.00529626, -0.03139063, -0.02277323,\n",
      "       -0.04380397, -0.01295853,  0.00393623,  0.03655404, -0.01477316,\n",
      "        0.02157299, -0.00151515, -0.02826252,  0.01395667, -0.0035633 ,\n",
      "        0.00493145,  0.02814947,  0.00607346, -0.0034496 ,  0.00238732,\n",
      "        0.0378099 ,  0.01826231,  0.01024051,  0.00673944, -0.02666284,\n",
      "        0.01340408, -0.02663727,  0.01387185, -0.0030622 ,  0.01065464,\n",
      "        0.00113971,  0.00531259, -0.02000566, -0.0013481 , -0.03741201,\n",
      "        0.00232364, -0.00130427,  0.01753731, -0.00085454,  0.00683047,\n",
      "        0.00840577,  0.02083925, -0.00389719, -0.04098742, -0.04313199,\n",
      "       -0.00806382, -0.00850991, -0.02574886,  0.01112878,  0.01646806,\n",
      "        0.00056989, -0.01865428,  0.01991204,  0.01354439,  0.02133995,\n",
      "        0.00915128, -0.00754354,  0.00646883, -0.00691788,  0.00767863,\n",
      "       -0.01074838, -0.01031664]), 'downside': array([-0.00454442, -0.03505994, -0.0108636 , -0.00900999, -0.0018622 ,\n",
      "       -0.02094398, -0.02538383, -0.02415459, -0.01897047, -0.02747355,\n",
      "       -0.04606414, -0.02713302, -0.00152522, -0.00829242, -0.01936407,\n",
      "       -0.01814962, -0.01335015, -0.00415512, -0.02047745, -0.01830589,\n",
      "       -0.00529626, -0.03139063, -0.02277323, -0.04380397, -0.01295853,\n",
      "       -0.01477316, -0.00151515, -0.02826252, -0.0035633 , -0.0034496 ,\n",
      "       -0.02666284, -0.02663727, -0.0030622 , -0.02000566, -0.0013481 ,\n",
      "       -0.03741201, -0.00130427, -0.00085454, -0.00389719, -0.04098742,\n",
      "       -0.04313199, -0.00806382, -0.00850991, -0.02574886, -0.01865428,\n",
      "       -0.00754354, -0.00691788, -0.01074838, -0.01031664]), 'final_wealth': 1.0207220651025564, 'action_hold_count': 34, 'action_buy_count': 38, 'action_sell_count': 30, 'cumulative_return': 0.020722065102556364}\n",
      "{'regime': 2, 'episode_sharpe': -0.15490632219188152, 'episode_sortino': -0.24358759694131438, 'episode_total_reward': -0.25551311117923536, 'calmar': -0.7792892245742298, 'max_drawdown': 0.3108023768879997, 'win_rate': 0.0, 'alpha': -0.3506369697507712, 'returns': array([-0.01322506, -0.00778567, -0.00454442, -0.03505994, -0.00426136,\n",
      "        0.0108636 , -0.00900999, -0.0018622 , -0.02118086, -0.05341214,\n",
      "       -0.01805754, -0.02094398, -0.02538383,  0.02415459,  0.01897047,\n",
      "       -0.02445406,  0.01959975,  0.0160866 ,  0.07278702, -0.02747355,\n",
      "       -0.01794236,  0.04606414, -0.07683017,  0.04387642,  0.00463109,\n",
      "        0.00042297,  0.00073988,  0.02386988, -0.01475139, -0.01214533,\n",
      "        0.02713302,  0.00152522, -0.00829242, -0.01936407,  0.00830248,\n",
      "       -0.00211416, -0.00333111, -0.01814962, -0.01793478, -0.01580352,\n",
      "       -0.01335015,  0.00415512,  0.02047745,  0.01830589, -0.00529626,\n",
      "       -0.03139063, -0.02277323, -0.04380397, -0.01295853,  0.00393623,\n",
      "       -0.03655404, -0.01477316, -0.02157299, -0.00151515, -0.02826252,\n",
      "        0.01395667,  0.0035633 , -0.00493145, -0.02814947, -0.00607346,\n",
      "       -0.0034496 , -0.00238732,  0.0378099 ,  0.01826231,  0.01024051,\n",
      "        0.00673944,  0.02666284,  0.01340408,  0.02663727,  0.01387185,\n",
      "        0.0030622 , -0.01065464, -0.00113971, -0.00531259, -0.02000566,\n",
      "       -0.0013481 , -0.03741201,  0.00232364, -0.00130427, -0.01753731,\n",
      "        0.00085454, -0.00683047, -0.00840577,  0.02083925,  0.00389719,\n",
      "        0.04098742,  0.04313199, -0.00806382,  0.00850991, -0.02574886,\n",
      "        0.01112878,  0.01646806, -0.00056989,  0.01865428, -0.01991204,\n",
      "       -0.01354439, -0.02133995, -0.00915128,  0.00754354,  0.00646883,\n",
      "       -0.00691788, -0.00767863]), 'downside': array([-0.01322506, -0.00778567, -0.00454442, -0.03505994, -0.00426136,\n",
      "       -0.00900999, -0.0018622 , -0.02118086, -0.05341214, -0.01805754,\n",
      "       -0.02094398, -0.02538383, -0.02445406, -0.02747355, -0.01794236,\n",
      "       -0.07683017, -0.01475139, -0.01214533, -0.00829242, -0.01936407,\n",
      "       -0.00211416, -0.00333111, -0.01814962, -0.01793478, -0.01580352,\n",
      "       -0.01335015, -0.00529626, -0.03139063, -0.02277323, -0.04380397,\n",
      "       -0.01295853, -0.03655404, -0.01477316, -0.02157299, -0.00151515,\n",
      "       -0.02826252, -0.00493145, -0.02814947, -0.00607346, -0.0034496 ,\n",
      "       -0.00238732, -0.01065464, -0.00113971, -0.00531259, -0.02000566,\n",
      "       -0.0013481 , -0.03741201, -0.00130427, -0.01753731, -0.00683047,\n",
      "       -0.00840577, -0.00806382, -0.02574886, -0.00056989, -0.01991204,\n",
      "       -0.01354439, -0.02133995, -0.00915128, -0.00691788, -0.00767863]), 'final_wealth': 0.7577950567191232, 'action_hold_count': 39, 'action_buy_count': 33, 'action_sell_count': 30, 'cumulative_return': -0.24220494328087683}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regime': 2, 'episode_sharpe': -0.060291030581778864, 'episode_sortino': -0.12356879102137679, 'episode_total_reward': 0.1458141059934151, 'calmar': 0.5844482887596258, 'max_drawdown': 0.2293051961930786, 'win_rate': 1.0, 'alpha': 0.04683020526297654, 'returns': array([-0.        , -0.02118086,  0.05341214,  0.01805754,  0.02094398,\n",
      "        0.02538383, -0.02415459,  0.01897047,  0.02445406, -0.01959975,\n",
      "        0.0160866 ,  0.07278702, -0.02747355, -0.01794236,  0.04606414,\n",
      "        0.07683017,  0.04387642,  0.00463109,  0.00042297,  0.00073988,\n",
      "        0.02386988, -0.01475139,  0.01214533, -0.02713302,  0.00152522,\n",
      "        0.00829242,  0.01936407, -0.00830248, -0.00211416,  0.00333111,\n",
      "       -0.01814962, -0.01793478, -0.01580352, -0.01335015, -0.00415512,\n",
      "        0.02047745,  0.01830589,  0.00529626,  0.03139063,  0.02277323,\n",
      "        0.04380397, -0.01295853,  0.00393623, -0.03655404, -0.01477316,\n",
      "       -0.02157299, -0.00151515,  0.02826252, -0.01395667, -0.0035633 ,\n",
      "        0.00493145,  0.02814947, -0.00607346, -0.0034496 ,  0.00238732,\n",
      "       -0.0378099 , -0.01826231, -0.01024051,  0.00673944,  0.02666284,\n",
      "        0.01340408, -0.02663727,  0.01387185, -0.0030622 ,  0.01065464,\n",
      "       -0.00113971, -0.00531259, -0.02000566, -0.0013481 , -0.03741201,\n",
      "        0.00232364, -0.00130427, -0.01753731,  0.00085454, -0.00683047,\n",
      "       -0.00840577, -0.02083925,  0.00389719,  0.04098742, -0.04313199,\n",
      "       -0.00806382, -0.00850991,  0.02574886,  0.01112878, -0.01646806,\n",
      "        0.00056989,  0.01865428,  0.01991204,  0.01354439, -0.02133995,\n",
      "       -0.00915128, -0.00754354, -0.00646883,  0.00691788, -0.00767863,\n",
      "       -0.01074838, -0.01031664, -0.03778179, -0.00293878, -0.00073686,\n",
      "       -0.01153563, -0.00153672]), 'downside': array([-0.02118086, -0.02415459, -0.01959975, -0.02747355, -0.01794236,\n",
      "       -0.01475139, -0.02713302, -0.00830248, -0.00211416, -0.01814962,\n",
      "       -0.01793478, -0.01580352, -0.01335015, -0.00415512, -0.01295853,\n",
      "       -0.03655404, -0.01477316, -0.02157299, -0.00151515, -0.01395667,\n",
      "       -0.0035633 , -0.00607346, -0.0034496 , -0.0378099 , -0.01826231,\n",
      "       -0.01024051, -0.02663727, -0.0030622 , -0.00113971, -0.00531259,\n",
      "       -0.02000566, -0.0013481 , -0.03741201, -0.00130427, -0.01753731,\n",
      "       -0.00683047, -0.00840577, -0.02083925, -0.04313199, -0.00806382,\n",
      "       -0.00850991, -0.01646806, -0.02133995, -0.00915128, -0.00754354,\n",
      "       -0.00646883, -0.00767863, -0.01074838, -0.01031664, -0.03778179,\n",
      "       -0.00293878, -0.00073686, -0.01153563, -0.00153672]), 'final_wealth': 1.134017029518735, 'action_hold_count': 32, 'action_buy_count': 33, 'action_sell_count': 37, 'cumulative_return': 0.13401702951873506}\n",
      "{'regime': 2, 'episode_sharpe': 0.034685389528272936, 'episode_sortino': 0.06249308997213592, 'episode_total_reward': 0.034755793180035424, 'calmar': 0.07393712592176663, 'max_drawdown': 0.20775967136451504, 'win_rate': 1.0, 'alpha': -0.0771866755932642, 'returns': array([ 0.02538383,  0.02415459,  0.01897047, -0.02445406, -0.01959975,\n",
      "       -0.0160866 ,  0.07278702, -0.02747355, -0.01794236, -0.04606414,\n",
      "        0.07683017,  0.04387642,  0.00463109,  0.00042297,  0.00073988,\n",
      "       -0.02386988,  0.01475139,  0.01214533, -0.02713302,  0.00152522,\n",
      "        0.00829242,  0.01936407, -0.00830248, -0.00211416, -0.00333111,\n",
      "        0.01814962, -0.01793478, -0.01580352,  0.01335015,  0.00415512,\n",
      "       -0.02047745, -0.01830589,  0.00529626, -0.03139063, -0.02277323,\n",
      "       -0.04380397, -0.01295853, -0.00393623, -0.03655404,  0.01477316,\n",
      "       -0.02157299,  0.00151515,  0.02826252,  0.01395667, -0.0035633 ,\n",
      "        0.00493145, -0.02814947, -0.00607346, -0.0034496 , -0.00238732,\n",
      "        0.0378099 , -0.01826231, -0.01024051, -0.00673944, -0.02666284,\n",
      "        0.01340408,  0.02663727,  0.01387185,  0.0030622 ,  0.01065464,\n",
      "        0.00113971,  0.00531259,  0.02000566,  0.0013481 , -0.03741201,\n",
      "        0.00232364,  0.00130427, -0.01753731, -0.00085454,  0.00683047,\n",
      "       -0.00840577,  0.02083925,  0.00389719, -0.04098742,  0.04313199,\n",
      "        0.00806382, -0.00850991,  0.02574886,  0.01112878,  0.01646806,\n",
      "       -0.00056989, -0.01865428, -0.01991204, -0.01354439,  0.02133995,\n",
      "        0.00915128, -0.00754354,  0.00646883,  0.00691788, -0.00767863,\n",
      "       -0.01074838, -0.01031664,  0.03778179, -0.00293878,  0.00073686,\n",
      "        0.01153563, -0.00153672,  0.00129209,  0.01148217, -0.01247102,\n",
      "       -0.00348094,  0.02071487]), 'downside': array([-0.02445406, -0.01959975, -0.0160866 , -0.02747355, -0.01794236,\n",
      "       -0.04606414, -0.02386988, -0.02713302, -0.00830248, -0.00211416,\n",
      "       -0.00333111, -0.01793478, -0.01580352, -0.02047745, -0.01830589,\n",
      "       -0.03139063, -0.02277323, -0.04380397, -0.01295853, -0.00393623,\n",
      "       -0.03655404, -0.02157299, -0.0035633 , -0.02814947, -0.00607346,\n",
      "       -0.0034496 , -0.00238732, -0.01826231, -0.01024051, -0.00673944,\n",
      "       -0.02666284, -0.03741201, -0.01753731, -0.00085454, -0.00840577,\n",
      "       -0.04098742, -0.00850991, -0.00056989, -0.01865428, -0.01991204,\n",
      "       -0.01354439, -0.00754354, -0.00767863, -0.01074838, -0.01031664,\n",
      "       -0.00293878, -0.00153672, -0.01247102, -0.00348094]), 'final_wealth': 1.015361152983143, 'action_hold_count': 37, 'action_buy_count': 30, 'action_sell_count': 35, 'cumulative_return': 0.015361152983143}\n",
      "{'regime': 2, 'episode_sharpe': 0.09037781503492968, 'episode_sortino': 0.1398866985729051, 'episode_total_reward': 0.24581675845994322, 'calmar': 1.1036921005935374, 'max_drawdown': 0.2277256865851896, 'win_rate': 1.0, 'alpha': 0.15160942897672558, 'returns': array([ 0.01166966,  0.02134423, -0.01322506, -0.00778567,  0.00454442,\n",
      "       -0.03505994,  0.00426136, -0.0108636 ,  0.00900999,  0.0018622 ,\n",
      "       -0.02118086, -0.05341214,  0.01805754, -0.02094398,  0.02538383,\n",
      "       -0.02415459,  0.01897047, -0.02445406, -0.01959975,  0.0160866 ,\n",
      "       -0.07278702, -0.02747355, -0.01794236,  0.04606414,  0.07683017,\n",
      "        0.04387642, -0.00463109,  0.00042297,  0.00073988, -0.02386988,\n",
      "        0.01475139,  0.01214533, -0.02713302, -0.00152522,  0.00829242,\n",
      "        0.01936407, -0.00830248,  0.00211416,  0.00333111, -0.01814962,\n",
      "       -0.01793478, -0.01580352,  0.01335015, -0.00415512,  0.02047745,\n",
      "        0.01830589, -0.00529626,  0.03139063, -0.02277323,  0.04380397,\n",
      "        0.01295853, -0.00393623,  0.03655404,  0.01477316,  0.02157299,\n",
      "        0.00151515,  0.02826252,  0.01395667, -0.0035633 ,  0.00493145,\n",
      "       -0.02814947,  0.00607346,  0.0034496 ,  0.00238732, -0.0378099 ,\n",
      "        0.01826231, -0.01024051,  0.00673944, -0.02666284,  0.01340408,\n",
      "       -0.02663727,  0.01387185, -0.0030622 ,  0.01065464,  0.00113971,\n",
      "       -0.00531259,  0.02000566,  0.0013481 ,  0.03741201, -0.00232364,\n",
      "       -0.00130427, -0.01753731,  0.00085454, -0.00683047,  0.00840577,\n",
      "        0.02083925, -0.00389719,  0.04098742,  0.04313199, -0.00806382,\n",
      "        0.00850991,  0.02574886,  0.01112878,  0.01646806, -0.00056989,\n",
      "        0.01865428, -0.01991204, -0.01354439,  0.02133995,  0.00915128,\n",
      "       -0.00754354, -0.00646883]), 'downside': array([-0.01322506, -0.00778567, -0.03505994, -0.0108636 , -0.02118086,\n",
      "       -0.05341214, -0.02094398, -0.02415459, -0.02445406, -0.01959975,\n",
      "       -0.07278702, -0.02747355, -0.01794236, -0.00463109, -0.02386988,\n",
      "       -0.02713302, -0.00152522, -0.00830248, -0.01814962, -0.01793478,\n",
      "       -0.01580352, -0.00415512, -0.00529626, -0.02277323, -0.00393623,\n",
      "       -0.0035633 , -0.02814947, -0.0378099 , -0.01024051, -0.02666284,\n",
      "       -0.02663727, -0.0030622 , -0.00531259, -0.00232364, -0.00130427,\n",
      "       -0.01753731, -0.00683047, -0.00389719, -0.00806382, -0.00056989,\n",
      "       -0.01991204, -0.01354439, -0.00754354, -0.00646883]), 'final_wealth': 1.2513390413863135, 'action_hold_count': 35, 'action_buy_count': 34, 'action_sell_count': 33, 'cumulative_return': 0.25133904138631347}\n",
      "{'regime': 2, 'episode_sharpe': -0.03077571581712827, 'episode_sortino': -0.04561137003963853, 'episode_total_reward': -0.1699478829666239, 'calmar': -0.6931231937331561, 'max_drawdown': 0.24959897198209138, 'win_rate': 1.0, 'alpha': -0.2699676185302925, 'returns': array([-0.02094398,  0.02538383, -0.02415459, -0.01897047, -0.02445406,\n",
      "        0.01959975, -0.0160866 , -0.07278702,  0.02747355,  0.01794236,\n",
      "       -0.04606414,  0.07683017,  0.04387642,  0.00463109, -0.00042297,\n",
      "        0.00073988,  0.02386988, -0.01475139, -0.01214533, -0.02713302,\n",
      "        0.00152522, -0.00829242, -0.01936407, -0.00830248, -0.00211416,\n",
      "       -0.00333111,  0.01814962,  0.01793478, -0.01580352,  0.01335015,\n",
      "        0.00415512, -0.02047745, -0.01830589,  0.00529626,  0.03139063,\n",
      "        0.02277323, -0.04380397, -0.01295853,  0.00393623, -0.03655404,\n",
      "        0.01477316, -0.02157299,  0.00151515,  0.02826252, -0.01395667,\n",
      "        0.0035633 , -0.00493145, -0.02814947,  0.00607346,  0.0034496 ,\n",
      "       -0.00238732,  0.0378099 , -0.01826231,  0.01024051,  0.00673944,\n",
      "       -0.02666284,  0.01340408, -0.02663727,  0.01387185,  0.0030622 ,\n",
      "       -0.01065464, -0.00113971, -0.00531259,  0.02000566, -0.0013481 ,\n",
      "       -0.03741201,  0.00232364,  0.00130427, -0.01753731,  0.00085454,\n",
      "       -0.00683047,  0.00840577, -0.02083925,  0.00389719, -0.04098742,\n",
      "       -0.04313199,  0.00806382, -0.00850991,  0.02574886, -0.01112878,\n",
      "       -0.01646806, -0.00056989,  0.01865428,  0.01991204, -0.01354439,\n",
      "        0.02133995, -0.00915128,  0.00754354,  0.00646883,  0.00691788,\n",
      "       -0.00767863,  0.01074838,  0.01031664,  0.03778179,  0.00293878,\n",
      "       -0.00073686, -0.01153563, -0.00153672, -0.00129209,  0.01148217,\n",
      "       -0.01247102, -0.00348094]), 'downside': array([-0.02094398, -0.02415459, -0.01897047, -0.02445406, -0.0160866 ,\n",
      "       -0.07278702, -0.04606414, -0.00042297, -0.01475139, -0.01214533,\n",
      "       -0.02713302, -0.00829242, -0.01936407, -0.00830248, -0.00211416,\n",
      "       -0.00333111, -0.01580352, -0.02047745, -0.01830589, -0.04380397,\n",
      "       -0.01295853, -0.03655404, -0.02157299, -0.01395667, -0.00493145,\n",
      "       -0.02814947, -0.00238732, -0.01826231, -0.02666284, -0.02663727,\n",
      "       -0.01065464, -0.00113971, -0.00531259, -0.0013481 , -0.03741201,\n",
      "       -0.01753731, -0.00683047, -0.02083925, -0.04098742, -0.04313199,\n",
      "       -0.00850991, -0.01112878, -0.01646806, -0.00056989, -0.01354439,\n",
      "       -0.00915128, -0.00767863, -0.00073686, -0.01153563, -0.00153672,\n",
      "       -0.00129209, -0.01247102, -0.00348094]), 'final_wealth': 0.8269971633872603, 'action_hold_count': 38, 'action_buy_count': 34, 'action_sell_count': 30, 'cumulative_return': -0.17300283661273974}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 1.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 141      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 800      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 160\u001b[0m     train_agent()\n",
      "Cell \u001b[1;32mIn[4], line 156\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[0;32m    135\u001b[0m     eval_env, best_model_save_path\u001b[38;5;241m=\u001b[39mcheckpoint_best_model,\n\u001b[0;32m    136\u001b[0m     log_path\u001b[38;5;241m=\u001b[39mlog_path, eval_freq\u001b[38;5;241m=\u001b[39mEVAL_FREQ, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    137\u001b[0m )\n\u001b[0;32m    139\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentPPO(\n\u001b[0;32m    140\u001b[0m     policy\u001b[38;5;241m=\u001b[39mTransformerPolicy,\n\u001b[0;32m    141\u001b[0m     env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(share_features_extractor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m )\n\u001b[1;32m--> 156\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTOTAL_TIMESTEPS, callback\u001b[38;5;241m=\u001b[39m[checkpoint_callback, eval_callback])\n\u001b[0;32m    157\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:450\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[0;32m    443\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    451\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    452\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    453\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    454\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    455\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    456\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    457\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:345\u001b[0m, in \u001b[0;36mRecurrentPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Convert mask from float to bool\u001b[39;00m\n\u001b[0;32m    343\u001b[0m mask \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[1;32m--> 345\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mevaluate_actions(\n\u001b[0;32m    346\u001b[0m     rollout_data\u001b[38;5;241m.\u001b[39mobservations,\n\u001b[0;32m    347\u001b[0m     actions,\n\u001b[0;32m    348\u001b[0m     rollout_data\u001b[38;5;241m.\u001b[39mlstm_states,\n\u001b[0;32m    349\u001b[0m     rollout_data\u001b[38;5;241m.\u001b[39mepisode_starts,\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    352\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sb3_contrib\\common\\recurrent\\policies.py:326\u001b[0m, in \u001b[0;36mRecurrentActorCriticPolicy.evaluate_actions\u001b[1;34m(self, obs, actions, lstm_states, episode_starts)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03mEvaluate actions according to the current policy,\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03mgiven the observations.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    and entropy of the action distribution.\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    328\u001b[0m     pi_features \u001b[38;5;241m=\u001b[39m vf_features \u001b[38;5;241m=\u001b[39m features  \u001b[38;5;66;03m# alias\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features_extractor)\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mTransformerFeatureExtractor.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:seq_len]\n\u001b[0;32m     45\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m generate_causal_mask(seq_len)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x, mask\u001b[38;5;241m=\u001b[39mcausal_mask)\n\u001b[0;32m     48\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#print(\">>> [Transformer] Pooled output mean/std:\", pooled_output.mean().item(), pooled_output.std().item())\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:514\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    511\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 514\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    515\u001b[0m         output,\n\u001b[0;32m    516\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    517\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    518\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    522\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:916\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    913\u001b[0m         x\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    915\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:941\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 941\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "# Causal Mask Function ============================\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# Transformer Feature Extractor ===================\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=64, n_heads=4, n_layers=2, max_len=MAX_LENGTH):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "        input_dim = observation_space.shape[-1]\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward_v1(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "        return x[:, -1]  # return the last token output\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        #print(\">>> [Transformer] Input shape:\", obs.shape)\n",
    "\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "\n",
    "        pooled_output = x[:, -1]\n",
    "        #print(\">>> [Transformer] Pooled output mean/std:\", pooled_output.mean().item(), pooled_output.std().item())\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "# Transformer Policy ===================================\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs,\n",
    "                         features_extractor_class=TransformerFeatureExtractor,\n",
    "                         features_extractor_kwargs=dict(\n",
    "                             d_model=64, n_heads=4, n_layers=2, max_len=32\n",
    "                         ))\n",
    "        #self._build(self.lr_schedule)\n",
    "\n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)\n",
    "\n",
    "class PerEpisodeRewardNormalizer(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_rewards = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean = np.mean(self.episode_rewards)\n",
    "        std = np.std(self.episode_rewards) + 1e-8\n",
    "        norm_reward = (reward - mean) / std\n",
    "        return obs, norm_reward, done, truncated, info\n",
    "# Training =============================================================\n",
    "train_df = ohlcv_df[(ohlcv_df['date']>=\"2023-01-01\") & (ohlcv_df['date']<\"2023-07-01\")]\n",
    "test_df = ohlcv_df[(ohlcv_df['date']>=\"2023-07-01\") & (ohlcv_df['date']<\"2024-01-01\")]\n",
    "train_df = train_df[train_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "test_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "\n",
    "train_seq = train_env.generate_episode_sequences(TOTAL_TIMESTEPS)\n",
    "test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS*0.2))\n",
    "\n",
    "print(len(train_seq))\n",
    "\"\"\"\n",
    "Tip: Wrap Order Matters\n",
    "Your wrapper order is correct — reward wrappers should go outside observation/action wrappers, because .step() applies from outermost to innermost.\n",
    "\n",
    "#So this works:\n",
    "\n",
    "\n",
    "env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(base_env))\n",
    "\n",
    "#But this would not normalize rewards from the correct wrapped step:\n",
    "env = RegimeAugmentingWrapper(PerEpisodeRewardNormalizer(base_env))  # ❌ not ideal\n",
    "\"\"\"\n",
    "def train_agent():\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    train_env.set_episode_sequence(train_seq[:int(len(train_seq)*0.8)])\n",
    "    eval_env.set_episode_sequence(train_seq[int(len(train_seq)*0.8):])\n",
    "    \n",
    "    train_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(train_env))\n",
    "    eval_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env))\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=SAVE_FREQ, save_path=checkpoint_path, name_prefix=checkpoint_preffix\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, best_model_save_path=checkpoint_best_model,\n",
    "        log_path=log_path, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    model = RecurrentPPO(\n",
    "        policy=TransformerPolicy,\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        #tensorboard_log=\"./tensorboard_logs\",\n",
    "        n_steps=N_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.005,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs=dict(share_features_extractor=True)\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "    model.save(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38331a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Notify(experiment_tracker.project)\n",
    "n.info('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from src.env.base_timeseries_trading_env import SequenceAwareCumulativeTradingEnv\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "#from src.env.base_timeseries_trading_env import RegimeAugmentingWrapper\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "N_EVAL_EPISODES = 5\n",
    "MODEL_PATH = save_path\n",
    "\n",
    "# === Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "test_df = ohlcv_df[(ohlcv_df['date'] >= \"2023-07-01\") & (ohlcv_df['date'] < \"2024-01-01\")]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "\n",
    "# === Evaluation Logic ===\n",
    "def evaluate_agent(agent, env, n_episodes=5):\n",
    "    episode_metrics = []\n",
    "    action_counts = []\n",
    "\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        print(\">>> [Eval] Obs shape:\", obs.shape)  # Check input dimensions\n",
    "\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        while not done:\n",
    "            action, state = agent.predict(obs, state=state, deterministic=True)\n",
    "            print(action)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": infos[-1].get(\"wealth\", np.nan),\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "        }\n",
    "\n",
    "        episode_metrics.append(metrics)\n",
    "    \n",
    "    print(\">>> [Eval] Action counts:\", pd.Series(action_counts).value_counts())\n",
    "    return pd.DataFrame(episode_metrics)\n",
    "\n",
    "\n",
    "def evaluate_random_agent(env, n_episodes=5):\n",
    "    episode_metrics = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": infos[-1].get(\"wealth\", np.nan),\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "        }\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics)\n",
    "\n",
    "\n",
    "# === Run Evaluation ===\n",
    "test_env = RegimeAugmentingWrapper(SequenceAwareCumulativeTradingEnv(test_df, episode_length=EPISODE_LENGTH,feature_cols=FEATURE_COLS))\n",
    "model = RecurrentPPO.load(MODEL_PATH)\n",
    "\n",
    "ppo_agent_df = evaluate_agent(model, test_env, n_episodes=N_EVAL_EPISODES)\n",
    "random_agent_df = evaluate_random_agent(test_env, n_episodes=N_EVAL_EPISODES)\n",
    "\n",
    "ppo_agent_df[\"agent\"] = \"recurrent_ppo\"\n",
    "random_agent_df[\"agent\"] = \"random\"\n",
    "results_df = pd.concat([ppo_agent_df, random_agent_df])\n",
    "\n",
    "\n",
    "# === Plotting ===\n",
    "melted = results_df.melt(id_vars=\"agent\", var_name=\"metric\", value_name=\"value\")\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=melted, x=\"metric\", y=\"value\", hue=\"agent\")\n",
    "plt.title(\"Agent Performance Comparison (Random vs Recurrent PPO)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Statistical Tests ===\n",
    "comparison_results = []\n",
    "\n",
    "for metric in ppo_agent_df.columns[:-1]:  # exclude 'agent'\n",
    "    a = ppo_agent_df[metric].dropna()\n",
    "    b = random_agent_df[metric].dropna()\n",
    "\n",
    "    # Skip if either is empty\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        print(f\"Skipping metric {metric}: empty values\")\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val_t = ttest_ind(a, b)\n",
    "    u_stat, p_val_u = mannwhitneyu(a, b, alternative='two-sided')\n",
    "    comparison_results.append({\n",
    "        \"metric\": metric,\n",
    "        \"t-test p-value\": p_val_t,\n",
    "        \"mann-whitney p-value\": p_val_u\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc585161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9002810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
