{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69fe254",
   "metadata": {},
   "source": [
    "Absolutely — here's the full **README.md** draft that documents the initial version of our **LTM Test Suite**: Learnability, Transferability, Meta-Evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 📊 LTM Test Suite: Learnability, Transferability, and Meta-Evaluation\n",
    "\n",
    "This project establishes the **foundation for evaluating trading agents** based on their ability to:\n",
    "\n",
    "1. **Learn effectively** in specific environments (Learnability)\n",
    "2. **Generalize their knowledge** to new timeframes (Transferability)\n",
    "3. **Be selected or ranked** using meta-features (Meta-Evaluation)\n",
    "\n",
    "This evaluation system is central to our long-term goal of creating **regime-aware, introspective, intelligent agents** that know when and where they can succeed.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚧 Status\n",
    "\n",
    "✅ **Prototype phase (single-file implementation)**\n",
    "⬜ Modular pipeline with full CLI / batch capabilities\n",
    "⬜ Meta-learning model integration\n",
    "⬜ Curriculum design system\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Core Concepts\n",
    "\n",
    "### 🔹 Learnability\n",
    "\n",
    "> Can an agent learn useful trading behavior in a specific environment?\n",
    "\n",
    "* Agent is trained on a single episode (e.g., 1 stock in 1 month)\n",
    "* Evaluated by its normalized episode score (`0–100`)\n",
    "* Uses fixed seeds and training steps for fair comparison\n",
    "* Logged across multiple runs to assess robustness\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Transferability\n",
    "\n",
    "> Can the knowledge acquired in one environment be applied to the next one?\n",
    "\n",
    "* Agent trained on `Month T`, evaluated (or fine-tuned) on `Month T+1`\n",
    "* Compared against:\n",
    "\n",
    "  * Random agent baseline\n",
    "  * Oracle performance\n",
    "* Transfer success = performance delta relative to training or baseline\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Meta-Evaluation\n",
    "\n",
    "> Can we predict which environments are promising *before* training?\n",
    "\n",
    "* Extracts meta-features from the environment:\n",
    "\n",
    "  * Volatility, momentum, entropy, Hurst, kurtosis, etc.\n",
    "* Generates `meta_df.csv` with:\n",
    "\n",
    "  * `learnability`, `agent advantage`, `transfer_delta` labels\n",
    "* Enables future predictive modeling and curriculum learning\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Test Protocol\n",
    "\n",
    "### Episode Sampling\n",
    "\n",
    "* All episodes:\n",
    "\n",
    "  * Start on Mondays\n",
    "  * Have fixed `n_timesteps`\n",
    "  * Are non-overlapping **or** weekly (depending on config)\n",
    "* Benchmark episodes are stored in:\n",
    "\n",
    "  ```\n",
    "  data/experiments/learnability_test/benchmark_episodes.json\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Agent Setup\n",
    "\n",
    "* Agents trained with PPO (via Stable-Baselines3)\n",
    "* Random agent used as baseline\n",
    "* Oracle score used as upper bound reference\n",
    "* Configurations, seeds, and policies are logged and reproducible\n",
    "\n",
    "---\n",
    "\n",
    "### Logging & Outputs\n",
    "\n",
    "| Output                    | Description                                    |\n",
    "| ------------------------- | ---------------------------------------------- |\n",
    "| `meta_df.csv`             | Meta-features + labels for each run            |\n",
    "| `checkpoints/{id}.zip`    | Trained agents saved with unique config hashes |\n",
    "| `scores/{id}_results.csv` | Per-step and final metrics                     |\n",
    "| `logs/`                   | Training logs and config snapshots             |\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Project Structure (Coming Soon)\n",
    "\n",
    "```\n",
    "ltm_suite/\n",
    "├── configs/\n",
    "│   └── benchmark_episodes.json\n",
    "├── benchmarks/\n",
    "│   └── scores/\n",
    "│   └── checkpoints/\n",
    "│   └── logs/\n",
    "├── results/\n",
    "│   └── meta_df.csv\n",
    "├── runners/\n",
    "│   ├── run_learnability.py\n",
    "│   ├── run_transferability.py\n",
    "│   └── run_meta_evaluation.py\n",
    "├── utils/\n",
    "│   └── env_loader.py\n",
    "│   └── metrics.py\n",
    "│   └── logger.py\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Success Criteria\n",
    "\n",
    "* Episode scores consistently above 50% = good learning\n",
    "* Transfer performance better than random baseline = generalization\n",
    "* Meta-features predictive of good environments = meta-learning success\n",
    "* All results are reproducible and statistically valid (multiple seeds)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Next Milestone\n",
    "\n",
    "We now proceed to:\n",
    "\n",
    "* Implement a single-file prototype for **Learnability Test**\n",
    "* Store benchmark episodes\n",
    "* Save logs, scores, meta-data, and model checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to add/change anything before we start the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco Sá\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from notebooks.environments import PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"environment_test_battery\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1add869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': {'flat_long_random_actions': True,\n",
       "  'oracle_weight_sum': True,\n",
       "  'anti_oracle_weight_sum': True,\n",
       "  'trade_reward_correlation': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compare flat vs long vs random actions.\n",
    "\n",
    "Confirm episode reward ranges.\n",
    "\n",
    "Plot price paths and agent decisions.\n",
    "\n",
    "Check correlation between reward and actual trading logic.\n",
    "\"\"\"\n",
    "\n",
    "BASE_ENV_KWARGS ={\n",
    "    \"full_df\":OHLCV_DF,\n",
    "    \"ticker\":\"AAPL\",\n",
    "    \"market_features\":['close'],\n",
    "    \"start_idx\":100\n",
    "}\n",
    "class EnvironmentTestBattery:\n",
    "    \n",
    "    def __init__(self,env_cls, env_kwargs=BASE_ENV_KWARGS):\n",
    "        self.env_cls = env_cls\n",
    "        self.env_kwargs = env_kwargs\n",
    "        # Storage \n",
    "        self.results_file = DEFAULT_PATH+'/results.csv'\n",
    "        self.load_results()\n",
    "        \n",
    "        # Checklist\n",
    "        self.checklist = {\n",
    "            \"rewards\":{\n",
    "                \"flat_long_random_actions\":False,\n",
    "                \"oracle_weight_sum\":False,\n",
    "                \"anti_oracle_weight_sum\":False,\n",
    "              #  \"price_reward_correlation\":False,\n",
    "                \"trade_reward_correlation\":False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.checklist['rewards']:\n",
    "            self.checklist['rewards'][key] = False\n",
    "        \n",
    "    def run_validators(self, n_episodes=10):\n",
    "        \"\"\"\n",
    "        Environment behaviour requirements:\n",
    "        * Reward system\n",
    "          * Sum of oracle rewards must be 1\n",
    "          * Sum of anti-oracle must be -1\n",
    "          * Agent performance must be statistically aligned with rewards\n",
    "        \"\"\"\n",
    "        self.compare_flat_long_random_actions(n_episodes)\n",
    "        self.validate_reward_weight_sums(n_episodes)\n",
    "        #self.validate_price_reward_correlation(n_episodes)\n",
    "        self.validate_reward_trade_logic_correlation(n_episodes)\n",
    "        return self.checklist\n",
    "    \n",
    "    def load_results(self):\n",
    "        if os.path.exists(self.results_file):\n",
    "            self.results_df= pd.read_csv(self.results_file)\n",
    "        else:\n",
    "            self.results_df= pd.DataFrame()\n",
    "\n",
    "    \n",
    "    # HELPER METHODS ============================================\n",
    "    def collect_rewards(self, policy_fn, n_episodes):\n",
    "        all_rewards = []\n",
    "        all_prices = []\n",
    "        all_actions = []\n",
    "        oracle = []\n",
    "        anti_oracle = []\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            env = self.env_cls(**self.env_kwargs)\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = policy_fn(env)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                all_rewards.append(reward)\n",
    "                all_prices.append(env.prices[env.step_idx])\n",
    "                all_actions.append(action)\n",
    "                oracle.append(abs(reward))\n",
    "                anti_oracle.append(abs(reward)*-1)\n",
    "\n",
    "        return np.array(all_rewards), np.array(all_prices), np.array(all_actions),np.array(oracle),np.array(anti_oracle)\n",
    "\n",
    "    def compare_flat_long_random_actions(self, n_episodes=10):\n",
    "        def always_flat(env): return 0\n",
    "        def always_long(env): return 1\n",
    "        def random_action(env): return np.random.choice([0, 1])\n",
    "\n",
    "        flat_rewards, _, _, oracle,anti_oracle = self.collect_rewards(always_flat, n_episodes)\n",
    "        long_rewards, _, _, _, _               = self.collect_rewards(always_long, n_episodes)\n",
    "        random_rewards, _, _, _, _             = self.collect_rewards(random_action, n_episodes)\n",
    "\n",
    "        summary = {\n",
    "            'flat': np.sum(flat_rewards),\n",
    "            'long': np.sum(long_rewards),\n",
    "            'random': np.sum(random_rewards),\n",
    "            'oracle':np.sum(oracle),\n",
    "            'anti_oracle':np.sum(anti_oracle),\n",
    "            'validations':{\n",
    "                'flat_long': np.sum(flat_rewards)+np.sum(long_rewards)==0,\n",
    "                'between_oracle': (np.sum(random_rewards)/np.sum(oracle) >-1) and (np.sum(random_rewards)/np.sum(oracle) <=1)\n",
    "            }\n",
    "        }\n",
    "        #print(\"Reward comparison:\", summary)\n",
    "        self.checklist['rewards']['flat_long_random_actions'] = summary['validations']['flat_long'] and  summary['validations']['between_oracle']\n",
    "\n",
    "        \n",
    "    def validate_reward_weight_sums(self, n_episodes=5):\n",
    "        oracle_sum = 0\n",
    "        anti_sum = 0\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            env = self.env_cls(**self.env_kwargs)\n",
    "            env.reset()\n",
    "            for i in range(len(env.prices) - 1):\n",
    "                price_diff = env.prices[i + 1] - env.prices[i]\n",
    "                #oracle = np.sign(price_diff) * env.step_weights[i] * 100\n",
    "                oracle = abs(env.step_weights[i]) * 100\n",
    "                anti = -oracle\n",
    "                oracle_sum += oracle\n",
    "                anti_sum += anti\n",
    "\n",
    "        oracle_ok = np.isclose(oracle_sum, 100 * n_episodes, atol=1e-1)\n",
    "        anti_ok = np.isclose(anti_sum, -100 * n_episodes, atol=1e-1)\n",
    "        #print(oracle_ok,anti_ok,oracle_sum, 100 * n_episodes,oracle_sum)\n",
    "        self.checklist['rewards']['oracle_weight_sum'] = oracle_ok\n",
    "        self.checklist['rewards']['anti_oracle_weight_sum'] = anti_ok\n",
    "\n",
    "    def validate_price_reward_correlation(self, n_episodes=5):\n",
    "        def random_action(env): return 0 #np.random.choice([0, 1])\n",
    "        rewards, prices, _,_,_ = self.collect_rewards(random_action, n_episodes)\n",
    "        #print(len(rewards))\n",
    "        if len(rewards) < 2:\n",
    "            self.checklist['rewards']['price_reward_correlation'] = False\n",
    "            return\n",
    "\n",
    "        returns = np.diff(prices) / prices[:-1]\n",
    "        rewards = rewards[:-1]  # align\n",
    "        corr = np.corrcoef(returns, rewards)[0, 1]\n",
    "        self.checklist['rewards']['price_reward_correlation'] = abs(corr) #> 0.2\n",
    "        \n",
    "\n",
    "    def validate_reward_trade_logic_correlation(self, n_episodes=5):\n",
    "        def oracle_action(env):  # follow the price\n",
    "            curr = env.prices[env.step_idx]\n",
    "            next_ = env.prices[min(env.step_idx + 1, len(env.prices) - 1)]\n",
    "            return int(next_ > curr)\n",
    "\n",
    "        rewards, _, actions,_,_ = self.collect_rewards(oracle_action, n_episodes)\n",
    "        action_changes = np.abs(np.diff(actions))\n",
    "        self.checklist['rewards']['trade_reward_correlation'] = (np.sum(rewards) > 0 and np.sum(action_changes) > 0)\n",
    "\n",
    "    \n",
    "\n",
    "env_test_check = EnvironmentTestBattery(PositionTradingEnvV2)\n",
    "env_test_check.run_validators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb2fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
