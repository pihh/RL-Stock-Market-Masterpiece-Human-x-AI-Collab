{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57063e41",
   "metadata": {},
   "source": [
    "Here's a structured **summary and technical breakdown** based on your notebook screenshot:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **Summary of the Trading Environment POC**\n",
    "\n",
    "This notebook implements a **custom Gym environment** tailored for a simple but insightful financial trading simulation. The environment was designed to:\n",
    "\n",
    "* Work with **real historical stock data**.\n",
    "* Focus on **directional decisions**: stay flat or go long.\n",
    "* Use a **normalized oracle-relative reward system** that fairly evaluates agent behavior.\n",
    "\n",
    "The experiment compares the learning performance of a **PPO agent** vs. a **random policy**, using matched episodes sampled with strict rules.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ **Environment Design: `PositionTradingEnv`**\n",
    "\n",
    "### **State & Action Space**\n",
    "\n",
    "* **State (Observation):** `Box(shape=(1,))` → current price only (can be extended later).\n",
    "* **Action Space:** `Discrete(2)`\n",
    "\n",
    "  * `0 = Flat` (no position)\n",
    "  * `1 = Long` (holding stock)\n",
    "\n",
    "### **Environment Rules**\n",
    "\n",
    "Each episode:\n",
    "\n",
    "* Must begin on a **Monday**.\n",
    "* Must be **chronologically ordered** and contain only **one ticker**.\n",
    "* Must be at least `n_timesteps` long.\n",
    "* If `lookback` is used, it must also follow these rules and only include past data.\n",
    "* Internally resamples valid episodes using `sample_valid_episodes()`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 🧮 Reward System — Oracle-Relative Scoring (Normalized to 100)\n",
    "\n",
    "The reward system is the **core innovation** of this environment. It ensures that:\n",
    "\n",
    "> 🟢 The agent's performance is **evaluated relative to the best and worst possible actions** at each step,\n",
    "> 🟢 and scaled so that **total reward per episode always ranges from 0 to 100**, regardless of volatility or duration.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Step-by-Step Breakdown\n",
    "\n",
    "For each timestep `t`:\n",
    "\n",
    "#### 1. **Compute Price Change**\n",
    "\n",
    "```python\n",
    "price_diff = next_price - curr_price\n",
    "```\n",
    "\n",
    "#### 2. **Determine Agent’s Action-Dependent Reward**\n",
    "\n",
    "```python\n",
    "agent_reward = +price_diff if position == Long else -price_diff\n",
    "```\n",
    "\n",
    "* Going **long** is rewarded when price goes **up**\n",
    "* Staying **flat** is rewarded when price goes **down**\n",
    "\n",
    "#### 3. **Determine Oracle and Anti-Oracle Baselines**\n",
    "\n",
    "```python\n",
    "oracle_reward = max(+price_diff, -price_diff)\n",
    "anti_reward   = min(+price_diff, -price_diff)\n",
    "```\n",
    "\n",
    "* The **oracle** always takes the best action in hindsight.\n",
    "* The **anti-oracle** always takes the worst possible action.\n",
    "\n",
    "#### 4. **Normalize Agent Performance**\n",
    "\n",
    "```python\n",
    "step_score = (agent_reward - anti_reward) / (oracle_reward - anti_reward)\n",
    "step_score = np.clip(step_score, 0, 1)\n",
    "```\n",
    "\n",
    "This converts any action into a **score from 0 (worst) to 1 (best)** based on how it compares to the oracle range.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Step Weighting (Optional but Enabled)\n",
    "\n",
    "To avoid rewarding equally across flat and volatile regimes:\n",
    "\n",
    "```python\n",
    "step_weight = abs(price_diff) / total_episode_volatility\n",
    "```\n",
    "\n",
    "* Steps with more meaningful price movements contribute more to the total score.\n",
    "* This prevents agents from scoring well just by being conservative in low-volatility episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### 🏁 Final Scaled Step Reward\n",
    "\n",
    "```python\n",
    "scaled_reward = step_score * step_weight * 100\n",
    "```\n",
    "\n",
    "All rewards are summed across the episode. The environment **precomputes the oracle’s total theoretical reward**, and rescales so that:\n",
    "\n",
    "> 🔥 `agent_total_reward ∈ [0, 100]`\n",
    "\n",
    "This **decouples reward from episode length or price scale**, making learning signals stable and comparable across episodes, tickers, and training sessions.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Benefits\n",
    "\n",
    "* ✅ **Stable learning signal** across different stocks and episodes.\n",
    "* ✅ **Fair benchmarking** against oracle and random baselines.\n",
    "* ✅ **Normalized interpretability**: 100 = perfect hindsight behavior.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want me to append this into your actual notebook or package it into a Markdown cell automatically.\n",
    "\n",
    "\n",
    "### Step Reward Calculation\n",
    "\n",
    "At each step:\n",
    "\n",
    "* Let `price_diff = next_price - curr_price`\n",
    "\n",
    "* Agent reward:\n",
    "\n",
    "  * If `Long`: gets `+price_diff`\n",
    "  * If `Flat`: gets `-price_diff`\n",
    "\n",
    "* Oracle (best hindsight position): `max(|price_diff|)`\n",
    "\n",
    "* Anti-oracle: `-oracle_reward`\n",
    "\n",
    "Then compute:\n",
    "\n",
    "```python\n",
    "step_score = (agent_reward - anti_reward) / (oracle_reward - anti_reward)\n",
    "```\n",
    "\n",
    "* This yields a score ∈ \\[0, 1] indicating how close the agent was to the ideal choice at that step.\n",
    "* The final **step reward is scaled**:\n",
    "\n",
    "```python\n",
    "scaled_reward = step_score * weight * 100\n",
    "```\n",
    "\n",
    "Where `weight` is precomputed to **normalize total oracle reward to 100 per episode** (variable step weighting based on price volatility).\n",
    "\n",
    "✅ **Total possible reward per episode: 100**\n",
    "✅ Ensures fair comparability across episodes of different volatility.\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 **Agent Training and Evaluation**\n",
    "\n",
    "### PPO Agent\n",
    "\n",
    "* Trained using `Stable-Baselines3 PPO` for 5,000 timesteps.\n",
    "* Environment wrapped with `Monitor` for logging.\n",
    "* Observed performance using total reward over multiple sampled episodes.\n",
    "\n",
    "### Evaluation Logic\n",
    "\n",
    "* Sample **fixed episodes** from the data for fairness.\n",
    "* Run both PPO agent and random agent on the **same episodes**.\n",
    "* Track and compare total normalized reward (0–100 scale).\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* `ppo_mean`, `random_mean`: average scores\n",
    "* `t_stat`, `p_val`: statistical significance (usually **very strong**)\n",
    "* Histograms for score distributions (via `plot_evaluation_results()`)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco Sá\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"core_rl_trading_environment\"\n",
    "DEFAULT_PATH = \"/data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "CONFIG = {\n",
    "    \"ticker\": \"AAPL\",\n",
    "    \"start_date\": \"2023-01-01\",\n",
    "    \"end_date\": \"2024-01-01\",\n",
    "    \"window_length_days\": 60,\n",
    "    \"step_size_days\": 30,\n",
    "    \"reward_type\": \"path_score\",\n",
    "    \"model_save_path\": DEFAULT_PATH + \"/models\",\n",
    "    \"log_path\": DEFAULT_PATH + \"/logs\",\n",
    "    \"result_path\": DEFAULT_PATH + \"/results\"\n",
    "}\n",
    "config_hash = experiment_hash(CONFIG)\n",
    "exists = check_if_experiment_exists(config_hash)\n",
    "DEVICE = boot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d87a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ac741",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "* if price goes up and agent is holding, his reward = % price up / max_ep_reward\n",
    "* Best possible score = 1\n",
    "* Worst = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003bbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "class PositionTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        full_df: pd.DataFrame,\n",
    "        ticker: str,\n",
    "        n_timesteps: int = 60,\n",
    "        lookback: int = 0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.full_df = full_df.copy()\n",
    "        self.ticker = ticker\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.lookback = lookback\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "        self.action_space = gym.spaces.Discrete(2)  # 0 = Flat, 1 = Long\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        self.episode_df = None\n",
    "        self.step_idx = 0\n",
    "        self._prepare_ticker_df()\n",
    "        self._resample_episode()\n",
    "\n",
    "    def _prepare_ticker_df(self):\n",
    "        self.df = self.full_df[self.full_df['symbol'] == self.ticker].copy()\n",
    "        self.df = self.df.sort_values(\"date\")\n",
    "        self.df[\"date\"] = pd.to_datetime(self.df[\"date\"])\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "    def _resample_episode(self):\n",
    "        mondays = self.df[self.df[\"date\"].dt.weekday == 0].copy()\n",
    "        valid_starts = []\n",
    "\n",
    "        for date in mondays[\"date\"]:\n",
    "            start_idx = self.df.index[self.df[\"date\"] == date][0]\n",
    "            end_idx = start_idx + self.n_timesteps - 1\n",
    "            if end_idx >= len(self.df):\n",
    "                continue\n",
    "\n",
    "            ep_slice = self.df.iloc[start_idx:end_idx + 1]\n",
    "            if (ep_slice[\"symbol\"].nunique() == 1) and (ep_slice[\"date\"].is_monotonic_increasing):\n",
    "                valid_starts.append(start_idx)\n",
    "\n",
    "        if not valid_starts:\n",
    "            raise ValueError(\"No valid episodes found with the current constraints.\")\n",
    "\n",
    "        self.start_idx = self.random_state.choice(valid_starts)\n",
    "        self.end_idx = self.start_idx + self.n_timesteps - 1\n",
    "        self.lookback_idx = max(0, self.start_idx - self.lookback)\n",
    "        self.episode_df = self.df.iloc[self.lookback_idx:self.end_idx + 1].reset_index(drop=True)\n",
    "\n",
    "        # Set prices used for reward logic\n",
    "        self.prices = self.episode_df[\"close\"].values\n",
    "        self._precompute_step_weights()\n",
    "\n",
    "    def _precompute_step_weights(self):\n",
    "        raw_weights = [abs(self.prices[i + 1] - self.prices[i]) for i in range(len(self.prices) - 1)]\n",
    "        total = sum(raw_weights)\n",
    "        self.step_weights = [w / total if total > 0 else 1 / (len(raw_weights)) for w in raw_weights]\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.random_state.seed(seed)\n",
    "        self._resample_episode()\n",
    "        self.step_idx = self.lookback\n",
    "        self.position = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.values = []\n",
    "        obs = np.array([self.prices[self.step_idx]], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        curr_idx = self.step_idx\n",
    "        next_idx = min(curr_idx + 1, len(self.prices) - 1)\n",
    "        curr_price = self.prices[curr_idx]\n",
    "        next_price = self.prices[next_idx]\n",
    "        price_diff = next_price - curr_price\n",
    "\n",
    "        self.position = action\n",
    "        agent_reward = price_diff if self.position == 1 else -price_diff\n",
    "        oracle_reward = abs(price_diff)\n",
    "        anti_reward = -oracle_reward\n",
    "\n",
    "        if oracle_reward == anti_reward:\n",
    "            step_score = 0.5\n",
    "        else:\n",
    "            step_score = (agent_reward - anti_reward) / (oracle_reward - anti_reward)\n",
    "\n",
    "        step_score = float(np.clip(step_score, 0, 1))\n",
    "        weight = self.step_weights[curr_idx - self.lookback] if curr_idx - self.lookback < len(self.step_weights) else 0\n",
    "        scaled_reward = step_score * weight * 100\n",
    "\n",
    "        self.total_reward += scaled_reward\n",
    "        self.rewards.append(self.total_reward)\n",
    "        self.actions.append(self.position)\n",
    "        self.values.append(curr_price)\n",
    "\n",
    "        self.step_idx += 1\n",
    "        terminated = self.step_idx >= self.lookback + self.n_timesteps - 1\n",
    "        truncated = False\n",
    "        obs = np.array([self.prices[min(self.step_idx, len(self.prices) - 1)]], dtype=np.float32)\n",
    "\n",
    "        return obs, scaled_reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def score_episode(agent_ret, oracle_ret, anti_ret):\n",
    "    if oracle_ret == anti_ret:\n",
    "        return 50\n",
    "    return float(np.clip(100 * (agent_ret - anti_ret) / (oracle_ret - anti_ret), 0, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4480156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>vwap</th>\n",
       "      <th>...</th>\n",
       "      <th>vwap_change</th>\n",
       "      <th>trade_count_change</th>\n",
       "      <th>sector_id</th>\n",
       "      <th>industry_id</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>vix</th>\n",
       "      <th>vix_norm</th>\n",
       "      <th>sp500</th>\n",
       "      <th>sp500_norm</th>\n",
       "      <th>market_return_1d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>251</td>\n",
       "      <td>MMM</td>\n",
       "      <td>2023-01-03 05:00:00</td>\n",
       "      <td>121.52</td>\n",
       "      <td>122.635</td>\n",
       "      <td>120.37</td>\n",
       "      <td>122.47</td>\n",
       "      <td>2612812.0</td>\n",
       "      <td>44229.0</td>\n",
       "      <td>121.846135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.212484</td>\n",
       "      <td>8.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.021264</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>38.2414</td>\n",
       "      <td>-0.004001</td>\n",
       "      <td>-0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>252</td>\n",
       "      <td>MMM</td>\n",
       "      <td>2023-01-04 05:00:00</td>\n",
       "      <td>123.35</td>\n",
       "      <td>125.290</td>\n",
       "      <td>122.71</td>\n",
       "      <td>125.15</td>\n",
       "      <td>2769831.0</td>\n",
       "      <td>46771.0</td>\n",
       "      <td>124.584773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.057474</td>\n",
       "      <td>8.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.021883</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>-0.038865</td>\n",
       "      <td>38.5297</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>253</td>\n",
       "      <td>MMM</td>\n",
       "      <td>2023-01-05 05:00:00</td>\n",
       "      <td>124.21</td>\n",
       "      <td>124.570</td>\n",
       "      <td>122.46</td>\n",
       "      <td>122.96</td>\n",
       "      <td>2606564.0</td>\n",
       "      <td>41426.0</td>\n",
       "      <td>123.168428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011369</td>\n",
       "      <td>-0.114280</td>\n",
       "      <td>8.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>-0.017499</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>0.020445</td>\n",
       "      <td>38.0810</td>\n",
       "      <td>-0.011646</td>\n",
       "      <td>-0.011646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id symbol           timestamp    open     high     low   close  \\\n",
       "date                                                                          \n",
       "2023-01-03  251    MMM 2023-01-03 05:00:00  121.52  122.635  120.37  122.47   \n",
       "2023-01-04  252    MMM 2023-01-04 05:00:00  123.35  125.290  122.71  125.15   \n",
       "2023-01-05  253    MMM 2023-01-05 05:00:00  124.21  124.570  122.46  122.96   \n",
       "\n",
       "               volume  trade_count        vwap  ...  vwap_change  \\\n",
       "date                                            ...                \n",
       "2023-01-03  2612812.0      44229.0  121.846135  ...     0.019225   \n",
       "2023-01-04  2769831.0      46771.0  124.584773  ...     0.022476   \n",
       "2023-01-05  2606564.0      41426.0  123.168428  ...    -0.011369   \n",
       "\n",
       "            trade_count_change  sector_id  industry_id  return_1d     vix  \\\n",
       "date                                                                        \n",
       "2023-01-03            0.212484        8.0      unknown   0.021264  0.2290   \n",
       "2023-01-04            0.057474        8.0      unknown   0.021883  0.2201   \n",
       "2023-01-05           -0.114280        8.0      unknown  -0.017499  0.2246   \n",
       "\n",
       "            vix_norm    sp500  sp500_norm  market_return_1d  \n",
       "date                                                         \n",
       "2023-01-03  0.056760  38.2414   -0.004001         -0.004001  \n",
       "2023-01-04 -0.038865  38.5297    0.007539          0.007539  \n",
       "2023-01-05  0.020445  38.0810   -0.011646         -0.011646  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = OHLCV_DF.copy() \n",
    "df_raw = df_raw[(df_raw['date'] >=CONFIG['start_date']) & (df_raw['date']<CONFIG['end_date'])]\n",
    "df_raw.set_index('date',inplace=True)\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b54d35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | 48.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 860      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 49          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 735         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014029702 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 1.91e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.6        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29          |\n",
      "|    ep_rew_mean          | 51.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005834896 |\n",
      "|    clip_fraction        | 0.00991     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -1.91e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000743   |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco Sá\\AppData\\Local\\Temp\\ipykernel_14272\\286769802.py:83: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p_val = ttest_ind(ppo_scores, random_scores, equal_var=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym import Env\n",
    "\n",
    "\n",
    "def sample_valid_episodes(df, ticker, n_timesteps=60, lookback=0, episodes=30, seed=42):\n",
    "    df = df[df['symbol'] == ticker].copy()\n",
    "    df = df.sort_values('date')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    mondays = df[df['date'].dt.weekday == 0]\n",
    "    valid_starts = []\n",
    "\n",
    "    for date in mondays['date']:\n",
    "        start_idx = df.index[df['date'] == date][0]\n",
    "        end_idx = start_idx + n_timesteps - 1\n",
    "        if end_idx >= len(df):\n",
    "            continue\n",
    "\n",
    "        episode = df.iloc[start_idx - lookback if start_idx - lookback >= 0 else 0 : end_idx + 1]\n",
    "        if episode['symbol'].nunique() == 1 and episode['date'].is_monotonic_increasing:\n",
    "            valid_starts.append(start_idx)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sampled_starts = rng.choice(valid_starts, size=episodes, replace=False)\n",
    "    return sampled_starts\n",
    "\n",
    "\n",
    "def run_learning_evaluation(df, ticker=\"AAPL\", timesteps=10_000, eval_episodes=30, n_timesteps=60, lookback=0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Sample episode start points\n",
    "    sampled_starts = sample_valid_episodes(df, ticker, n_timesteps, lookback, eval_episodes, seed)\n",
    "\n",
    "    # Train on the environment normally\n",
    "    env = Monitor(PositionTradingEnv(df, ticker, n_timesteps, lookback, seed=seed))\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, seed=seed)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "\n",
    "    # Evaluate PPO and Random with same episodes\n",
    "    ppo_scores = []\n",
    "    random_scores = []\n",
    "\n",
    "    for start_idx in sampled_starts:\n",
    "        # PPO agent evaluation\n",
    "        env_ppo = PositionTradingEnv(df, ticker, n_timesteps, lookback, seed=seed)\n",
    "        env_ppo.start_idx = start_idx  # override sampling\n",
    "        env_ppo.end_idx = start_idx + n_timesteps - 1\n",
    "        env_ppo.lookback_idx = max(0, start_idx - lookback)\n",
    "        env_ppo.episode_df = env_ppo.df.iloc[env_ppo.lookback_idx : env_ppo.end_idx + 1].reset_index(drop=True)\n",
    "        env_ppo.prices = env_ppo.episode_df[\"close\"].values\n",
    "        env_ppo._precompute_step_weights()\n",
    "        obs, _ = env_ppo.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _ = env_ppo.step(action)\n",
    "            done = terminated or truncated\n",
    "        ppo_scores.append(env_ppo.total_reward)\n",
    "\n",
    "        # Random agent evaluation\n",
    "        env_rand = PositionTradingEnv(df, ticker, n_timesteps, lookback, seed=seed)\n",
    "        env_rand.start_idx = start_idx\n",
    "        env_rand.end_idx = start_idx + n_timesteps - 1\n",
    "        env_rand.lookback_idx = max(0, start_idx - lookback)\n",
    "        env_rand.episode_df = env_rand.df.iloc[env_rand.lookback_idx : env_rand.end_idx + 1].reset_index(drop=True)\n",
    "        env_rand.prices = env_rand.episode_df[\"close\"].values\n",
    "        env_rand._precompute_step_weights()\n",
    "        obs, _ = env_rand.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env_rand.action_space.sample()\n",
    "            obs, reward, terminated, truncated, _ = env_rand.step(action)\n",
    "            done = terminated or truncated\n",
    "        random_scores.append(env_rand.total_reward)\n",
    "\n",
    "    t_stat, p_val = ttest_ind(ppo_scores, random_scores, equal_var=False)\n",
    "\n",
    "    return {\n",
    "        \"ppo_mean\": np.mean(ppo_scores),\n",
    "        \"random_mean\": np.mean(random_scores),\n",
    "        \"t_stat\": t_stat,\n",
    "        \"p_val\": p_val,\n",
    "        \"ppo_scores\": ppo_scores,\n",
    "        \"random_scores\": random_scores\n",
    "    }, model, env\n",
    "\n",
    "# --- Simulated test series ---\n",
    "def plot_evaluation_results(result_summary, title=\"Agent vs Random Performance\"):\n",
    "    ppo_scores = result_summary[\"ppo_scores\"]\n",
    "    random_scores = result_summary[\"random_scores\"]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(ppo_scores, color=\"green\", label=\"PPO Agent\", kde=True, stat=\"density\", bins=10)\n",
    "    sns.histplot(random_scores, color=\"red\", label=\"Random Policy\", kde=True, stat=\"density\", bins=10)\n",
    "\n",
    "    plt.axvline(np.mean(ppo_scores), color=\"green\", linestyle=\"--\", label=f\"PPO Mean: {np.mean(ppo_scores):.2f}\")\n",
    "    plt.axvline(np.mean(random_scores), color=\"red\", linestyle=\"--\", label=f\"Random Mean: {np.mean(random_scores):.2f}\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode Score (0–100)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "result_summary = run_learning_evaluation(\n",
    "    df_raw[df_raw['symbol']==\"AAPL\"].reset_index(),\n",
    "    ticker='AAPL', \n",
    "    timesteps=5000, \n",
    "    eval_episodes=5, \n",
    "    n_timesteps=30, \n",
    "    lookback=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ae95596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ppo_mean': 51.60744500846024,\n",
       "  'random_mean': 66.59590832179667,\n",
       "  't_stat': -4.345154929945915,\n",
       "  'p_val': 0.012203413860424778,\n",
       "  'ppo_scores': [51.607445008460246,\n",
       "   51.607445008460246,\n",
       "   51.607445008460246,\n",
       "   51.607445008460246,\n",
       "   51.607445008460246],\n",
       "  'random_scores': [59.83694816182125,\n",
       "   73.71173665589914,\n",
       "   56.760498384863865,\n",
       "   71.31210582987234,\n",
       "   71.35825257652674]},\n",
       " <stable_baselines3.ppo.ppo.PPO at 0x1fa8e8f19d0>,\n",
       " <Monitor<PositionTradingEnv instance>>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_evaluation_results(result_summary[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
