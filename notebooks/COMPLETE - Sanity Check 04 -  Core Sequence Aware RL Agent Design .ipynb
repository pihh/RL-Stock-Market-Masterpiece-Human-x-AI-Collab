{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "\n",
    "# Sequence-Aware RL Agent Design: Synthetic Market Study\n",
    "\n",
    "## 1. **Overview**\n",
    "\n",
    "This study benchmarks different RL architectures for financial trading in a **realistic, synthetic market environment**. The pipeline includes careful data generation, environment validation, baseline agents, and custom RL models using MLP, LSTM, and Transformer policies.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Experimental Setup**\n",
    "\n",
    "### **Data & Environment**\n",
    "\n",
    "* **Synthetic OHLCV Data:** Generated to mimic realistic, learnable markets, with controlled feature/reward relationships.\n",
    "* **Environment:** `SequenceAwareCumulativeTradingEnv`\n",
    "\n",
    "  * **Observation**: Time-windowed sequences (window length = 10, features = 25).\n",
    "  * **Episode length**: 100 steps (per episode).\n",
    "* **Features:** Standard trading and engineered features from `FEATURE_COLS`.\n",
    "* **Transaction Cost:** 0 (for pure learning signal).\n",
    "\n",
    "### **Agent Types**\n",
    "\n",
    "* **Baselines:**\n",
    "\n",
    "  * `RandomAgent` (random actions)\n",
    "  * `AlwaysLongAgent` (always goes long/buy)\n",
    "* **RL Agents:**\n",
    "\n",
    "  * `MLP` (Multi-Layer Perceptron, SB3 PPO)\n",
    "  * `LSTM` (RecurrentPPO, SB3-Contrib)\n",
    "  * `Transformer (single)` (Custom SB3 policy)\n",
    "  * `Transformer (multi)` (Custom SB3 policy with multi-head)\n",
    "\n",
    "### **Training Parameters**\n",
    "\n",
    "* `EPISODE_LENGTH = 100`\n",
    "* `WINDOW_LENGTH = 10`\n",
    "* `TOTAL_TIMESTEPS = 15,000` (150 episodes x 100 steps)\n",
    "* `N_STEPS (PPO) = 128`\n",
    "* `BATCH_SIZE = 100`\n",
    "* **Seeds:** 3\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Validation and Unit Tests**\n",
    "\n",
    "* **Observation Shapes:**\n",
    "\n",
    "  * Sequence (2D): `(window_length, features)`\n",
    "  * Flat: `(window_length * features,)`\n",
    "* **Window Consistency:**\n",
    "\n",
    "  * Start-of-episode padding is correct (repeat first row).\n",
    "* **Step-Through Logic:**\n",
    "\n",
    "  * Rewards, done, info dict as expected.\n",
    "* **SB3 Compatibility:**\n",
    "\n",
    "  * PPO with MLP and custom policies pass shape checks and train.\n",
    "* **Transformer Forward Pass:**\n",
    "\n",
    "  * Custom `TransformerExtractor` correctly processes input.\n",
    "* **Action & Reward:**\n",
    "\n",
    "  * Cumulative reward and info dicts consistent.\n",
    "* **Episode Generator:**\n",
    "\n",
    "  * Sequences deterministic under same seed.\n",
    "* **Learnability Check:**\n",
    "\n",
    "  * Agents can learn and outperform random.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Results**\n",
    "\n",
    "| Agent Type           | Mean Reward | Std Reward | Notes                             |\n",
    "| -------------------- | ----------- | ---------- | --------------------------------- |\n",
    "| Random               | **-0.0061** | 0.0313     | No exploitable pattern            |\n",
    "| Always Long          | **0.1209**  | 0.0132     | Environment favors long positions |\n",
    "| MLP                  | **0.1221**  | 0.0091     | Matches/best baseline, stable     |\n",
    "| LSTM                 | **0.0912**  | 0.0125     | Learns, but lower than MLP        |\n",
    "| Transformer (single) | **0.1230**  | 0.0079     | Slightly best, robust learning    |\n",
    "| Transformer (multi)  | **0.1230**  | 0.0079     | Matches single, robust learning   |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Interpretation**\n",
    "\n",
    "* **Baseline RL sanity:** Random agent earns \\~0, AlwaysLong is strong (market bias).\n",
    "* **MLP, Transformer agents:** Can reliably match or outperform the best baseline, showing they exploit the synthetic market's signals.\n",
    "* **LSTM:** Learns, but not as efficiently—may benefit from more tuning or different synthetic patterns.\n",
    "* **Transformers:** Small but consistent edge, indicating capacity to model sequence dependencies in data.\n",
    "* **Std rewards:** Low across agents after training, indicating **stable, consistent policies**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Takeaways & Next Steps**\n",
    "\n",
    "* **The environment and pipeline are validated** for RL research—unit tests pass, learnability is real.\n",
    "* **Transformers look especially promising** for sequence-based trading signals.\n",
    "* **Ablation and robustness**: Future work could include harder synthetic regimes, feature ablations, or regime-switching markets.\n",
    "* **Pipeline can be extended to real-world data** and more advanced reward functions.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Code Snippets**\n",
    "\n",
    "**MLP Agent Example:**\n",
    "\n",
    "```python\n",
    "model = PPO(\"MlpPolicy\", env, n_steps=EPISODE_LENGTH, batch_size=4, verbose=0)\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "mean_rl, std_rl = evaluate_agent(env, model, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "```\n",
    "\n",
    "**Transformer Agent Example:**\n",
    "\n",
    "```python\n",
    "model = PPO(\n",
    "    TransformerPolicy, env, verbose=0,\n",
    "    policy_kwargs={\n",
    "        'window_length': window_length,\n",
    "        'n_features': len(feature_cols),\n",
    "        'nhead': 2,\n",
    "        'num_layers': 2,\n",
    "    },\n",
    "    n_steps=EPISODE_LENGTH,\n",
    "    batch_size=4\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Final Table (Results)**\n",
    "\n",
    "| Agent                | Mean Reward | Std Reward |\n",
    "| -------------------- | ----------- | ---------- |\n",
    "| Random               | -0.0061     | 0.0313     |\n",
    "| Always Long          | 0.1209      | 0.0132     |\n",
    "| MLP                  | 0.1221      | 0.0091     |\n",
    "| LSTM                 | 0.0912      | 0.0125     |\n",
    "| Transformer (single) | 0.1230      | 0.0079     |\n",
    "| Transformer (multi)  | 0.1230      | 0.0079     |\n",
    "\n",
    "---\n",
    "\n",
    "**This study establishes a solid, validated playground for RL in trading, and demonstrates the power of transformer-based sequence agents in structured market environments.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550c1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "WINDOW_LENGTH = 10  \n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 128,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "features_extractor_kwargs={\n",
    "    'window_length': WINDOW_LENGTH,\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'd_model': 32,\n",
    "    'nhead': ...,\n",
    "    'num_layers': ...,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Load data ---\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# --- Experiment tracker ---\n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e9070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(df, ticker, feature_cols, episode_length, window_length):\n",
    "    df_ticker = df[df['symbol'] == ticker].copy()\n",
    "    return CumulativeTradingEnv(\n",
    "        df=df_ticker,\n",
    "        feature_cols=feature_cols,\n",
    "        episode_length=episode_length,\n",
    "        transaction_cost=TRANSACTION_COST,\n",
    "        window_length=window_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b919468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, window_length, n_features, d_model=32, nhead=1, num_layers=1):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.window_length = window_length\n",
    "        self.n_features = n_features\n",
    "        self.embedding = nn.Linear(n_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: [batch, window_length * n_features]\n",
    "        batch = obs.shape[0]\n",
    "        # reshape flat vector to (batch, window_length, n_features)\n",
    "        x = obs.view(batch, self.window_length, self.n_features)\n",
    "        x = self.embedding(x)      # (batch, window_length, d_model)\n",
    "        x = x.permute(1, 0, 2)    # (window_length, batch, d_model)\n",
    "        x = self.transformer(x)    # (window_length, batch, d_model)\n",
    "        # Use last token as pooled output\n",
    "        return x[-1]              # (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c844883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, nhead=1, num_layers=1, window_length=WINDOW_LENGTH, n_features=2, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=TransformerExtractor,\n",
    "            features_extractor_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': n_features,\n",
    "                'd_model': 32,\n",
    "                'nhead': nhead,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e69992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D window shape: (10, 25)\n",
      "Flat window shape: (250,)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Output Shapes\n",
    "\n",
    "# Test windowed obs shape (flat vs. 2D)\n",
    "df = ohlcv_df.copy()\n",
    "feature_cols = FEATURE_COLS\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "print(\"2D window shape:\", obs.shape)  # Expect (5, obs_dim)\n",
    "\n",
    "env_flat = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs_flat, _ = env_flat.reset()\n",
    "print(\"Flat window shape:\", obs_flat.shape)  # Expect (5*obs_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb71f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    obs,reward,done,_,info = env.step(1)\n",
    "    i+=1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5c339a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info[\"returns\"]),env.episode_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df52f8",
   "metadata": {},
   "source": [
    "# Unit tests:\n",
    "1. Output Shapes\n",
    "2. Window Consistency (Padding at Episode Start)\n",
    "3. Step Through Environment\n",
    "4. SB3 Policy Compatibility\n",
    "5. Transformer Policy Compatibility\n",
    "6. Action Space and Reward Consistency\n",
    "7. Episode Generator\n",
    "8. Is able to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d7094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D window shape: (10, 25)\n",
      "Flat window shape: (250,)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Output Shapes\n",
    "\n",
    "# Test windowed obs shape (flat vs. 2D)\n",
    "df = ohlcv_df.copy()\n",
    "feature_cols = FEATURE_COLS\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "print(\"2D window shape:\", obs.shape)  # Expect (5, obs_dim)\n",
    "\n",
    "env_flat = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs_flat, _ = env_flat.reset()\n",
    "print(\"Flat window shape:\", obs_flat.shape)  # Expect (5*obs_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e448cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding and shape OK\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Window consistency\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "assert np.allclose(obs[0], obs[1]), \"Padding at start should repeat first row\"\n",
    "assert obs.shape == (WINDOW_LENGTH, len(feature_cols) + len(env.internal_features))\n",
    "print(\"Padding and shape OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056b0441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Obs shape: (10, 25) | Reward: -0.00000\n",
      "Step 1 | Obs shape: (10, 25) | Reward: 0.00168\n",
      "Step 2 | Obs shape: (10, 25) | Reward: -0.00280\n",
      "Step 3 | Obs shape: (10, 25) | Reward: 0.00415\n",
      "Step 4 | Obs shape: (10, 25) | Reward: 0.00370\n",
      "Step 5 | Obs shape: (10, 25) | Reward: 0.00045\n",
      "Step 6 | Obs shape: (10, 25) | Reward: 0.00595\n",
      "Step 7 | Obs shape: (10, 25) | Reward: 0.00453\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Step Through Environment\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "for i in range(8):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    print(f\"Step {i} | Obs shape: {obs.shape} | Reward: {reward:.5f}\")\n",
    "    if done:\n",
    "        print(\"Episode done:\", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27c2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB3 PPO MLP works!\n"
     ]
    }
   ],
   "source": [
    "# SB3 Policy Compatibility\n",
    "# Train an MLP agent on env with return_sequences=False (flat). \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, n_steps=EPISODE_LENGTH, batch_size=4, verbose=0)\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "print(\"SB3 PPO MLP works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d412dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output shape: torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Transformer Policy Compatibility\n",
    "# Make sure custom transformer can process the 2D obs by running a forward pass \n",
    "# through the extractor to check for shape errors\n",
    "\n",
    "\n",
    "obs = np.random.randn(2, 5*8).astype(np.float32)  # batch=2, window_length=WINDOW_LENGTH, n_features=8\n",
    "# Extractor expects (batch, window_length*n_features), will reshape internally.\n",
    "extractor = TransformerExtractor(\n",
    "    gym.spaces.Box(-np.inf, np.inf, shape=(5*8,), dtype=np.float32), 5, 8\n",
    ")\n",
    "with torch.no_grad():\n",
    "    torch_out = extractor(torch.from_numpy(obs))\n",
    "print(\"Transformer output shape:\", torch_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6a1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Action Space and Reward Consistency\n",
    "# mini-episode ti check action output and cumulative reward:\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "cumulative = 0\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    cumulative += reward\n",
    "    if done:\n",
    "        print(\"Episode finished | Cumulative reward:\", cumulative)\n",
    "        print(\"Info dict:\", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ccdb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode generator determinism OK\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Episode Generator\n",
    "# Check that the same seed produces the same episode list across runs.\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH)\n",
    "seq1 = env.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "env2 = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH)\n",
    "seq2 = env2.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "assert seq1 == seq2, \"Episode sequences should be the same for same seed!\"\n",
    "print(\"Episode generator determinism OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff811f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8: Learnability\n",
    "from src.env.realistic_synthetic_environment import realistic_synthetic_market_sample\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return self.env.action_space.sample(), {}\n",
    "\n",
    "class AlwaysLongAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return 1, {}  # Always go long\n",
    "    \n",
    "def evaluate_baseline_agent(env, agent, n_episodes=20, episode_sequence=None):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86224825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = SequenceAwareCumulativeTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "#env.set_episode_sequence(seq)\n",
    "\n",
    "\n",
    "# Evaluate PPO agent\n",
    "def evaluate_sb3_agent(env, model, n_episodes=10, episode_sequence=None):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7df0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "def c(agent_type, env, window_length, feature_cols, **kwargs):\n",
    "    if agent_type == 'mlp':\n",
    "        return PPO(\"MlpPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type == 'lstm':\n",
    "        return RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "        #return PPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type.startswith('transformer'):\n",
    "        \n",
    "        n_features = len(feature_cols)\n",
    "       \n",
    "        return PPO(\n",
    "            TransformerPolicy,\n",
    "            env,\n",
    "            verbose=0,\n",
    "            policy_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': env.observation_space.shape[1],\n",
    "                'nhead': 2,        # set as desired\n",
    "                'num_layers': 2,   # set as desired\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eddba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_episodes=10, episode_sequence=None, is_sb3=False):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            if is_sb3:\n",
    "                action, _ = agent.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c442b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8151620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: mean -0.0061, std 0.0313\n",
      "Always Long: mean 0.1209, std 0.0132\n",
      "\n",
      "Training mlp agent...\n",
      "mlp agent: mean 0.1221, std 0.0091\n",
      "\n",
      "Training lstm agent...\n",
      "lstm agent: mean 0.0912, std 0.0125\n",
      "\n",
      "Training transformer_single agent...\n",
      "transformer_single agent: mean 0.1230, std 0.0079\n",
      "\n",
      "Training transformer_multi agent...\n",
      "transformer_multi agent: mean 0.1230, std 0.0079\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT AND SEQUENCES ======================\n",
    "df = realistic_synthetic_market_sample(n=200)\n",
    "feature_cols = FEATURE_COLS\n",
    "env = SequenceAwareCumulativeTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "seq = env.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "\n",
    "# Baseline agents =================================\n",
    "random_agent = RandomAgent(env)\n",
    "always_long_agent = AlwaysLongAgent(env)\n",
    "mean_rand, std_rand = evaluate_agent(env, random_agent, n_episodes=20, episode_sequence=seq)\n",
    "mean_long, std_long = evaluate_agent(env, always_long_agent, n_episodes=20, episode_sequence=seq)\n",
    "print(f\"Random: mean {mean_rand:.4f}, std {std_rand:.4f}\")\n",
    "print(f\"Always Long: mean {mean_long:.4f}, std {std_long:.4f}\")\n",
    "\n",
    "# RL agents =======================================\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "#AGENT_TYPES = ['transformer_single', 'transformer_multi']\n",
    "for agent_type in AGENT_TYPES:\n",
    "    print(f\"\\nTraining {agent_type} agent...\")\n",
    "    env = SequenceAwareCumulativeTradingEnv(\n",
    "        df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "    env.set_episode_sequence(seq)\n",
    "    model = make_agent(agent_type, env, window_length=WINDOW_LENGTH, feature_cols=feature_cols, n_steps=EPISODE_LENGTH, batch_size=4)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "    mean_rl, std_rl = evaluate_agent(env, model, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "    print(f\"{agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5789ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training: 1500 episodes, updating every 5 episodes.\n",
      "Episode 5: last episode reward 0.0667\n",
      "Episode 10: last episode reward 0.0058\n",
      "Episode 15: last episode reward -0.0260\n",
      "Episode 20: last episode reward 0.0311\n",
      "Episode 25: last episode reward 0.0301\n",
      "Episode 30: last episode reward -0.0473\n",
      "Episode 35: last episode reward 0.0229\n",
      "Episode 40: last episode reward 0.0261\n",
      "Episode 45: last episode reward 0.0270\n",
      "Episode 50: last episode reward -0.0055\n",
      "Episode 55: last episode reward -0.0027\n",
      "Episode 60: last episode reward 0.0189\n",
      "Episode 65: last episode reward -0.0203\n",
      "Episode 70: last episode reward -0.0175\n",
      "Episode 75: last episode reward -0.0102\n",
      "Episode 80: last episode reward 0.0635\n",
      "Episode 85: last episode reward -0.0586\n",
      "Episode 90: last episode reward -0.0337\n",
      "Episode 95: last episode reward 0.0123\n",
      "Episode 100: last episode reward 0.0293\n",
      "Episode 105: last episode reward 0.0250\n",
      "Episode 110: last episode reward -0.0149\n",
      "Episode 115: last episode reward -0.0592\n",
      "Episode 120: last episode reward 0.0507\n",
      "Episode 125: last episode reward -0.0243\n",
      "Episode 130: last episode reward 0.0151\n",
      "Episode 135: last episode reward -0.0091\n",
      "Episode 140: last episode reward 0.0148\n",
      "Episode 145: last episode reward -0.0602\n",
      "Episode 150: last episode reward 0.0704\n",
      "Episode 155: last episode reward -0.0374\n",
      "Episode 160: last episode reward -0.0134\n",
      "Episode 165: last episode reward 0.0223\n",
      "Episode 170: last episode reward 0.0029\n",
      "Episode 175: last episode reward 0.0305\n",
      "Episode 180: last episode reward -0.0105\n",
      "Episode 185: last episode reward -0.0153\n",
      "Episode 190: last episode reward -0.0276\n",
      "Episode 195: last episode reward -0.0298\n",
      "Episode 200: last episode reward 0.0224\n",
      "Episode 205: last episode reward 0.0179\n",
      "Episode 210: last episode reward 0.0165\n",
      "Episode 215: last episode reward -0.0379\n",
      "Episode 220: last episode reward 0.0029\n",
      "Episode 225: last episode reward 0.0068\n",
      "Episode 230: last episode reward -0.0014\n",
      "Episode 235: last episode reward -0.0450\n",
      "Episode 240: last episode reward 0.0160\n",
      "Episode 245: last episode reward 0.0065\n",
      "Episode 250: last episode reward 0.0614\n",
      "Episode 255: last episode reward 0.0408\n",
      "Episode 260: last episode reward 0.0164\n",
      "Episode 265: last episode reward -0.0327\n",
      "Episode 270: last episode reward 0.0045\n",
      "Episode 275: last episode reward 0.0219\n",
      "Episode 280: last episode reward 0.0386\n",
      "Episode 285: last episode reward -0.0131\n",
      "Episode 290: last episode reward 0.0274\n",
      "Episode 295: last episode reward 0.0291\n",
      "Episode 300: last episode reward -0.0138\n",
      "Episode 305: last episode reward -0.0320\n",
      "Episode 310: last episode reward -0.0673\n",
      "Episode 315: last episode reward 0.0410\n",
      "Episode 320: last episode reward 0.0304\n",
      "Episode 325: last episode reward -0.0277\n",
      "Episode 330: last episode reward -0.0175\n",
      "Episode 335: last episode reward 0.0349\n",
      "Episode 340: last episode reward 0.0015\n",
      "Episode 345: last episode reward -0.0479\n",
      "Episode 350: last episode reward -0.0017\n",
      "Episode 355: last episode reward 0.0426\n",
      "Episode 360: last episode reward -0.0363\n",
      "Episode 365: last episode reward -0.0368\n",
      "Episode 370: last episode reward -0.0617\n",
      "Episode 375: last episode reward 0.0128\n",
      "Episode 380: last episode reward 0.0183\n",
      "Episode 385: last episode reward -0.0200\n",
      "Episode 390: last episode reward 0.0305\n",
      "Episode 395: last episode reward -0.0128\n",
      "Episode 400: last episode reward 0.0259\n",
      "Episode 405: last episode reward -0.0010\n",
      "Episode 410: last episode reward -0.0346\n",
      "Episode 415: last episode reward 0.0491\n",
      "Episode 420: last episode reward -0.0184\n",
      "Episode 425: last episode reward -0.0315\n",
      "Episode 430: last episode reward -0.0153\n",
      "Episode 435: last episode reward 0.0403\n",
      "Episode 440: last episode reward -0.0501\n",
      "Episode 445: last episode reward 0.0239\n",
      "Episode 450: last episode reward 0.0622\n",
      "Episode 455: last episode reward 0.0425\n",
      "Episode 460: last episode reward 0.0468\n",
      "Episode 465: last episode reward -0.0404\n",
      "Episode 470: last episode reward -0.0393\n",
      "Episode 475: last episode reward 0.0305\n",
      "Episode 480: last episode reward 0.0102\n",
      "Episode 485: last episode reward -0.0214\n",
      "Episode 490: last episode reward 0.0347\n",
      "Episode 495: last episode reward -0.0556\n",
      "Episode 500: last episode reward 0.0185\n",
      "Episode 505: last episode reward -0.0152\n",
      "Episode 510: last episode reward -0.0018\n",
      "Episode 515: last episode reward -0.0175\n",
      "Episode 520: last episode reward 0.0152\n",
      "Episode 525: last episode reward -0.0027\n",
      "Episode 530: last episode reward -0.0306\n",
      "Episode 535: last episode reward 0.0569\n",
      "Episode 540: last episode reward 0.0175\n",
      "Episode 545: last episode reward -0.0406\n",
      "Episode 550: last episode reward -0.0735\n",
      "Episode 555: last episode reward -0.0136\n",
      "Episode 560: last episode reward -0.0052\n",
      "Episode 565: last episode reward 0.0016\n",
      "Episode 570: last episode reward -0.0239\n",
      "Episode 575: last episode reward -0.0071\n",
      "Episode 580: last episode reward -0.0212\n",
      "Episode 585: last episode reward 0.0189\n",
      "Episode 590: last episode reward -0.0092\n",
      "Episode 595: last episode reward -0.0645\n",
      "Episode 600: last episode reward 0.0067\n",
      "Episode 605: last episode reward 0.0200\n",
      "Episode 610: last episode reward -0.0348\n",
      "Episode 615: last episode reward 0.0165\n",
      "Episode 620: last episode reward 0.0107\n",
      "Episode 625: last episode reward 0.0125\n",
      "Episode 630: last episode reward 0.0095\n",
      "Episode 635: last episode reward -0.0233\n",
      "Episode 640: last episode reward -0.0515\n",
      "Episode 645: last episode reward -0.0125\n",
      "Episode 650: last episode reward -0.0334\n",
      "Episode 655: last episode reward 0.0703\n",
      "Episode 660: last episode reward -0.0095\n",
      "Episode 665: last episode reward -0.0336\n",
      "Episode 670: last episode reward 0.0178\n",
      "Episode 675: last episode reward 0.0037\n",
      "Episode 680: last episode reward 0.0467\n",
      "Episode 685: last episode reward -0.0477\n",
      "Episode 690: last episode reward 0.0147\n",
      "Episode 695: last episode reward -0.0166\n",
      "Episode 700: last episode reward -0.0410\n",
      "Episode 705: last episode reward 0.0479\n",
      "Episode 710: last episode reward 0.0136\n",
      "Episode 715: last episode reward 0.0473\n",
      "Episode 720: last episode reward 0.0158\n",
      "Episode 725: last episode reward 0.0222\n",
      "Episode 730: last episode reward -0.0213\n",
      "Episode 735: last episode reward -0.0164\n",
      "Episode 740: last episode reward 0.0161\n",
      "Episode 745: last episode reward -0.0452\n",
      "Episode 750: last episode reward 0.0126\n",
      "Episode 755: last episode reward 0.0096\n",
      "Episode 760: last episode reward -0.0480\n",
      "Episode 765: last episode reward -0.0444\n",
      "Episode 770: last episode reward -0.0275\n",
      "Episode 775: last episode reward 0.0457\n",
      "Episode 780: last episode reward -0.0352\n",
      "Episode 785: last episode reward 0.0030\n",
      "Episode 790: last episode reward -0.0316\n",
      "Episode 795: last episode reward 0.0472\n",
      "Episode 800: last episode reward -0.0226\n",
      "Episode 805: last episode reward -0.0050\n",
      "Episode 810: last episode reward 0.0325\n",
      "Episode 815: last episode reward -0.0164\n",
      "Episode 820: last episode reward 0.0376\n",
      "Episode 825: last episode reward 0.0247\n",
      "Episode 830: last episode reward -0.0043\n",
      "Episode 835: last episode reward -0.0387\n",
      "Episode 840: last episode reward 0.0026\n",
      "Episode 845: last episode reward 0.0030\n",
      "Episode 850: last episode reward -0.0147\n",
      "Episode 855: last episode reward -0.0617\n",
      "Episode 860: last episode reward -0.0702\n",
      "Episode 865: last episode reward -0.0144\n",
      "Episode 870: last episode reward 0.0033\n",
      "Episode 875: last episode reward -0.0167\n",
      "Episode 880: last episode reward -0.0301\n",
      "Episode 885: last episode reward -0.0715\n",
      "Episode 890: last episode reward 0.0064\n",
      "Episode 895: last episode reward -0.0090\n",
      "Episode 900: last episode reward -0.0169\n",
      "Episode 905: last episode reward -0.0515\n",
      "Episode 910: last episode reward -0.0198\n",
      "Episode 915: last episode reward -0.0150\n",
      "Episode 920: last episode reward 0.0193\n",
      "Episode 925: last episode reward -0.0311\n",
      "Episode 930: last episode reward -0.0487\n",
      "Episode 935: last episode reward 0.0304\n",
      "Episode 940: last episode reward -0.0319\n",
      "Episode 945: last episode reward 0.0285\n",
      "Episode 950: last episode reward 0.0012\n",
      "Episode 955: last episode reward 0.0448\n",
      "Episode 960: last episode reward -0.0121\n",
      "Episode 965: last episode reward 0.0069\n",
      "Episode 970: last episode reward -0.0414\n",
      "Episode 975: last episode reward 0.0178\n",
      "Episode 980: last episode reward -0.0134\n",
      "Episode 985: last episode reward 0.0032\n",
      "Episode 990: last episode reward -0.0325\n",
      "Episode 995: last episode reward -0.0225\n",
      "Episode 1000: last episode reward -0.0100\n",
      "Episode 1005: last episode reward 0.0344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1010: last episode reward -0.0041\n",
      "Episode 1015: last episode reward -0.0189\n",
      "Episode 1020: last episode reward -0.0278\n",
      "Episode 1025: last episode reward -0.0109\n",
      "Episode 1030: last episode reward -0.0105\n",
      "Episode 1035: last episode reward -0.0282\n",
      "Episode 1040: last episode reward -0.0130\n",
      "Episode 1045: last episode reward -0.0089\n",
      "Episode 1050: last episode reward 0.0103\n",
      "Episode 1055: last episode reward -0.0059\n",
      "Episode 1060: last episode reward 0.0085\n",
      "Episode 1065: last episode reward 0.0092\n",
      "Episode 1070: last episode reward -0.0776\n",
      "Episode 1075: last episode reward -0.0452\n",
      "Episode 1080: last episode reward -0.0315\n",
      "Episode 1085: last episode reward 0.0260\n",
      "Episode 1090: last episode reward -0.0067\n",
      "Episode 1095: last episode reward 0.0025\n",
      "Episode 1100: last episode reward 0.0024\n",
      "Episode 1105: last episode reward -0.0389\n",
      "Episode 1110: last episode reward 0.0134\n",
      "Episode 1115: last episode reward 0.0297\n",
      "Episode 1120: last episode reward 0.0300\n",
      "Episode 1125: last episode reward -0.0154\n",
      "Episode 1130: last episode reward 0.0087\n",
      "Episode 1135: last episode reward 0.0432\n",
      "Episode 1140: last episode reward 0.0415\n",
      "Episode 1145: last episode reward 0.0389\n",
      "Episode 1150: last episode reward -0.0127\n",
      "Episode 1155: last episode reward -0.0129\n",
      "Episode 1160: last episode reward -0.0007\n",
      "Episode 1165: last episode reward -0.0372\n",
      "Episode 1170: last episode reward 0.0094\n",
      "Episode 1175: last episode reward 0.0176\n",
      "Episode 1180: last episode reward -0.0249\n",
      "Episode 1185: last episode reward 0.0530\n",
      "Episode 1190: last episode reward 0.0196\n",
      "Episode 1195: last episode reward -0.0268\n",
      "Episode 1200: last episode reward 0.0118\n",
      "Episode 1205: last episode reward -0.0027\n",
      "Episode 1210: last episode reward 0.0071\n",
      "Episode 1215: last episode reward 0.0045\n",
      "Episode 1220: last episode reward -0.0225\n",
      "Episode 1225: last episode reward 0.0050\n",
      "Episode 1230: last episode reward 0.0173\n",
      "Episode 1235: last episode reward -0.0197\n",
      "Episode 1240: last episode reward -0.0300\n",
      "Episode 1245: last episode reward 0.0120\n",
      "Episode 1250: last episode reward 0.0405\n",
      "Episode 1255: last episode reward 0.0305\n",
      "Episode 1260: last episode reward -0.0383\n",
      "Episode 1265: last episode reward -0.0450\n",
      "Episode 1270: last episode reward -0.0525\n",
      "Episode 1275: last episode reward -0.0148\n",
      "Episode 1280: last episode reward 0.0075\n",
      "Episode 1285: last episode reward -0.0611\n",
      "Episode 1290: last episode reward -0.0495\n",
      "Episode 1295: last episode reward -0.0231\n",
      "Episode 1300: last episode reward 0.0187\n",
      "Episode 1305: last episode reward -0.0140\n",
      "Episode 1310: last episode reward -0.0328\n",
      "Episode 1315: last episode reward -0.0117\n",
      "Episode 1320: last episode reward -0.0565\n",
      "Episode 1325: last episode reward -0.0270\n",
      "Episode 1330: last episode reward -0.0232\n",
      "Episode 1335: last episode reward -0.0254\n",
      "Episode 1340: last episode reward 0.0527\n",
      "Episode 1345: last episode reward 0.0547\n",
      "Episode 1350: last episode reward 0.0248\n",
      "Episode 1355: last episode reward -0.0017\n",
      "Episode 1360: last episode reward -0.0526\n",
      "Episode 1365: last episode reward -0.0254\n",
      "Episode 1370: last episode reward -0.0021\n",
      "Episode 1375: last episode reward -0.0392\n",
      "Episode 1380: last episode reward 0.0021\n",
      "Episode 1385: last episode reward 0.0056\n",
      "Episode 1390: last episode reward -0.0531\n",
      "Episode 1395: last episode reward 0.0328\n",
      "Episode 1400: last episode reward -0.0036\n",
      "Episode 1405: last episode reward 0.0253\n",
      "Episode 1410: last episode reward -0.0007\n",
      "Episode 1415: last episode reward -0.0530\n",
      "Episode 1420: last episode reward -0.0213\n",
      "Episode 1425: last episode reward 0.0180\n",
      "Episode 1430: last episode reward 0.0133\n",
      "Episode 1435: last episode reward 0.0000\n",
      "Episode 1440: last episode reward 0.0384\n",
      "Episode 1445: last episode reward 0.0130\n",
      "Episode 1450: last episode reward 0.0355\n",
      "Episode 1455: last episode reward -0.0177\n",
      "Episode 1460: last episode reward -0.0231\n",
      "Episode 1465: last episode reward -0.0614\n",
      "Episode 1470: last episode reward -0.0077\n",
      "Episode 1475: last episode reward -0.0851\n",
      "Episode 1480: last episode reward 0.0015\n",
      "Episode 1485: last episode reward -0.0025\n",
      "Episode 1490: last episode reward -0.0133\n",
      "Episode 1495: last episode reward 0.0023\n",
      "Episode 1500: last episode reward 0.0232\n",
      "episodic_mlp_agent agent: mean -0.1167, std 0.0083\n"
     ]
    }
   ],
   "source": [
    "SEED = 314\n",
    "def env_factory():\n",
    "    return env\n",
    "trainer = EpisodicPPOTrainer(\n",
    "    env_factory=env_factory,\n",
    "    policy=\"MlpPolicy\",              # Can use custom\n",
    "    episode_length=100,\n",
    "    episodes_per_update=5,\n",
    "    total_episodes=1500,\n",
    "    #agent_kwargs=dict(n_epochs=2, learning_rate=2e-4)  \n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "agent_type = \"episodic_mlp_agent\"\n",
    "mean_rl, std_rl = evaluate_agent(env, trainer.agent, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "print(f\"{agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")\n",
    "#print(f\"{agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c8087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodic_mlp_agent agent: mean -0.1073, std 0.0129\n"
     ]
    }
   ],
   "source": [
    "agent_type = \"episodic_mlp_agent\"\n",
    "print(f\"{agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31252dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training: 500 episodes, updating every 4 episodes.\n",
      "Episode 4: last episode reward 0.0138\n",
      "Episode 8: last episode reward -0.0379\n",
      "Episode 12: last episode reward -0.0264\n",
      "Episode 16: last episode reward -0.0138\n",
      "Episode 20: last episode reward -0.0662\n",
      "Episode 24: last episode reward -0.0340\n",
      "Episode 28: last episode reward 0.0027\n",
      "Episode 32: last episode reward 0.0097\n",
      "Episode 36: last episode reward -0.0081\n",
      "Episode 40: last episode reward -0.0006\n",
      "Episode 44: last episode reward -0.0233\n",
      "Episode 48: last episode reward -0.0018\n",
      "Episode 52: last episode reward 0.0105\n",
      "Episode 56: last episode reward -0.0669\n",
      "Episode 60: last episode reward -0.0209\n",
      "Episode 64: last episode reward -0.0527\n",
      "Episode 68: last episode reward 0.0208\n",
      "Episode 72: last episode reward 0.0192\n",
      "Episode 76: last episode reward -0.0009\n",
      "Episode 80: last episode reward 0.0320\n",
      "Episode 84: last episode reward 0.0230\n",
      "Episode 88: last episode reward 0.0332\n",
      "Episode 92: last episode reward -0.0344\n",
      "Episode 96: last episode reward 0.0163\n",
      "Episode 100: last episode reward 0.0378\n",
      "Episode 104: last episode reward -0.0162\n",
      "Episode 108: last episode reward 0.0136\n",
      "Episode 112: last episode reward -0.0012\n",
      "Episode 116: last episode reward 0.0153\n",
      "Episode 120: last episode reward -0.0388\n",
      "Episode 124: last episode reward 0.0073\n",
      "Episode 128: last episode reward -0.0182\n",
      "Episode 132: last episode reward 0.0018\n",
      "Episode 136: last episode reward 0.0258\n",
      "Episode 140: last episode reward 0.0262\n",
      "Episode 144: last episode reward -0.0257\n",
      "Episode 148: last episode reward 0.0283\n",
      "Episode 152: last episode reward 0.0052\n",
      "Episode 156: last episode reward -0.0333\n",
      "Episode 160: last episode reward 0.0043\n",
      "Episode 164: last episode reward 0.0334\n",
      "Episode 168: last episode reward -0.0044\n",
      "Episode 172: last episode reward 0.0187\n",
      "Episode 176: last episode reward -0.0298\n",
      "Episode 180: last episode reward 0.0481\n",
      "Episode 184: last episode reward 0.0468\n",
      "Episode 188: last episode reward 0.0066\n",
      "Episode 192: last episode reward 0.0252\n",
      "Episode 196: last episode reward 0.0055\n",
      "Episode 200: last episode reward 0.0308\n",
      "Episode 204: last episode reward -0.0255\n",
      "Episode 208: last episode reward -0.0040\n",
      "Episode 212: last episode reward 0.0055\n",
      "Episode 216: last episode reward 0.0122\n",
      "Episode 220: last episode reward -0.0310\n",
      "Episode 224: last episode reward -0.0315\n",
      "Episode 228: last episode reward -0.0150\n",
      "Episode 232: last episode reward -0.0314\n",
      "Episode 236: last episode reward 0.0062\n",
      "Episode 240: last episode reward 0.0057\n",
      "Episode 244: last episode reward -0.0678\n",
      "Episode 248: last episode reward 0.0355\n",
      "Episode 252: last episode reward 0.0201\n",
      "Episode 256: last episode reward 0.0319\n",
      "Episode 260: last episode reward 0.0519\n",
      "Episode 264: last episode reward -0.0480\n",
      "Episode 268: last episode reward -0.0165\n",
      "Episode 272: last episode reward -0.0232\n",
      "Episode 276: last episode reward -0.0274\n",
      "Episode 280: last episode reward -0.0150\n",
      "Episode 284: last episode reward -0.0086\n",
      "Episode 288: last episode reward 0.0063\n",
      "Episode 292: last episode reward 0.0106\n",
      "Episode 296: last episode reward 0.0198\n",
      "Episode 300: last episode reward -0.0308\n",
      "Episode 304: last episode reward -0.0086\n",
      "Episode 308: last episode reward -0.0609\n",
      "Episode 312: last episode reward -0.0188\n",
      "Episode 316: last episode reward 0.0105\n",
      "Episode 320: last episode reward -0.0192\n",
      "Episode 324: last episode reward 0.0081\n",
      "Episode 328: last episode reward -0.0089\n",
      "Episode 332: last episode reward 0.0084\n",
      "Episode 336: last episode reward -0.0323\n",
      "Episode 340: last episode reward -0.0111\n",
      "Episode 344: last episode reward 0.0076\n",
      "Episode 348: last episode reward 0.0242\n",
      "Episode 352: last episode reward -0.0155\n",
      "Episode 356: last episode reward -0.0006\n",
      "Episode 360: last episode reward 0.0353\n",
      "Episode 364: last episode reward -0.0654\n",
      "Episode 368: last episode reward -0.0232\n",
      "Episode 372: last episode reward -0.0554\n",
      "Episode 376: last episode reward 0.0058\n",
      "Episode 380: last episode reward -0.0516\n",
      "Episode 384: last episode reward -0.0205\n",
      "Episode 388: last episode reward -0.0376\n",
      "Episode 392: last episode reward -0.0208\n",
      "Episode 396: last episode reward 0.0303\n",
      "Episode 400: last episode reward -0.0409\n",
      "Episode 404: last episode reward 0.0088\n",
      "Episode 408: last episode reward -0.0188\n",
      "Episode 412: last episode reward -0.0518\n",
      "Episode 416: last episode reward 0.0145\n",
      "Episode 420: last episode reward -0.0489\n",
      "Episode 424: last episode reward 0.0162\n",
      "Episode 428: last episode reward -0.0059\n",
      "Episode 432: last episode reward -0.0116\n",
      "Episode 436: last episode reward -0.0224\n",
      "Episode 440: last episode reward -0.0149\n",
      "Episode 444: last episode reward 0.0658\n",
      "Episode 448: last episode reward -0.0208\n",
      "Episode 452: last episode reward 0.0376\n",
      "Episode 456: last episode reward 0.0454\n",
      "Episode 460: last episode reward 0.0179\n",
      "Episode 464: last episode reward 0.0309\n",
      "Episode 468: last episode reward 0.0214\n",
      "Episode 472: last episode reward 0.0048\n",
      "Episode 476: last episode reward -0.0027\n",
      "Episode 480: last episode reward 0.0192\n",
      "Episode 484: last episode reward -0.0336\n",
      "Episode 488: last episode reward 0.0133\n",
      "Episode 492: last episode reward -0.0079\n",
      "Episode 496: last episode reward -0.0529\n",
      "Episode 500: last episode reward 0.0061\n",
      "Eval: mean reward -0.0801, std 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.08014404, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS TEST - Full Episode Learning \n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class EpisodicPPOTrainer:\n",
    "    def __init__(self, \n",
    "                 env_factory,           # callable, returns fresh env instance\n",
    "                 policy=\"MlpPolicy\",    # or custom policy class\n",
    "                 episode_length=100,\n",
    "                 episodes_per_update=4, # number of full episodes per PPO update\n",
    "                 total_episodes=1000,\n",
    "                 verbose=1,\n",
    "                 agent_kwargs=None):\n",
    "        self.env_factory = env_factory\n",
    "        self.episode_length = episode_length\n",
    "        self.episodes_per_update = episodes_per_update\n",
    "        self.total_episodes = total_episodes\n",
    "        self.verbose = verbose\n",
    "        self.policy = policy\n",
    "        #self.agent_kwargs = agent_kwargs or {}\n",
    "\n",
    "        # Build env and agent\n",
    "        self.env = DummyVecEnv([self.env_factory])\n",
    "        self.env = DummyVecEnv([self.env_factory])\n",
    "        steps_per_update = self.episode_length * self.episodes_per_update\n",
    "        self.agent = PPO(\n",
    "            policy, \n",
    "            self.env,\n",
    "            n_steps=steps_per_update,\n",
    "            batch_size=steps_per_update, # so update only at episode boundary\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def train(self, log_callback=None):\n",
    "        episode_count = 0\n",
    "        rewards_log = []\n",
    "        if self.verbose:\n",
    "            print(f\"Training: {self.total_episodes} episodes, updating every {self.episodes_per_update} episodes.\")\n",
    "        while episode_count < self.total_episodes:\n",
    "            all_obs, all_actions, all_rewards, all_dones, all_values, all_logprobs = [], [], [], [], [], []\n",
    "            for ep in range(self.episodes_per_update):\n",
    "                obs = self.env.reset()        # <-- FIX: initialize obs\n",
    "                done = False\n",
    "\n",
    "                ep_obs, ep_actions, ep_rewards, ep_dones, ep_values, ep_logprobs = [], [], [], [], [], []\n",
    "                while not done:\n",
    "                    # Vectorized env returns obs shape (1, obs_dim)\n",
    "                    obs_tensor = torch.from_numpy(obs).float()\n",
    "                    action, _ = self.agent.predict(obs, deterministic=False)\n",
    "                    action_tensor = torch.from_numpy(action).long()\n",
    "                    value = self.agent.policy.predict_values(torch.as_tensor(obs)).detach()\n",
    "                    logprob = self.agent.policy.evaluate_actions(\n",
    "                        torch.as_tensor(obs), torch.as_tensor(action)\n",
    "                    )[1].detach()\n",
    "                    next_obs, reward, done_arr, info = self.env.step(action)\n",
    "                    done = done_arr[0] if isinstance(done_arr, np.ndarray) else done_arr\n",
    "                    reward = reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "                    ep_obs.append(obs)\n",
    "                    ep_actions.append(action)\n",
    "                    ep_rewards.append(reward)\n",
    "                    ep_dones.append(done)\n",
    "                    ep_values.append(value)\n",
    "                    ep_logprobs.append(logprob)\n",
    "                    obs = next_obs\n",
    "                all_obs.extend(ep_obs)\n",
    "                all_actions.extend(ep_actions)\n",
    "                all_rewards.extend(ep_rewards)\n",
    "                all_dones.extend(ep_dones)\n",
    "                all_values.extend(ep_values)\n",
    "                all_logprobs.extend(ep_logprobs)\n",
    "                rewards_log.append(np.sum(ep_rewards))\n",
    "                episode_count += 1\n",
    "\n",
    "                if log_callback is not None:\n",
    "                    log_callback(episode_count, rewards_log)\n",
    "\n",
    "            # Fill rollout buffer with these episodes\n",
    "            self.agent.rollout_buffer.reset()\n",
    "          \n",
    "            for i in range(len(all_obs)):\n",
    "                self.agent.rollout_buffer.add(\n",
    "                    all_obs[i], all_actions[i], all_rewards[i], all_dones[i], all_values[i], all_logprobs[i]\n",
    "                )\n",
    "            # --- Fix logger bug (SB3 expects setup_learn called at least once) ---\n",
    "            if not hasattr(self.agent, \"_logger\"):\n",
    "                self.agent._setup_learn(1)\n",
    "            self.agent.train()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Episode {episode_count}: last episode reward {rewards_log[-1]:.4f}\")\n",
    "\n",
    "        return rewards_log\n",
    "\n",
    "    def evaluate(self, n_episodes=10):\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action, _ = self.agent.predict(obs, deterministic=True)\n",
    "                obs, reward, done,info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "            rewards.append(total_reward)\n",
    "        mean_r = np.mean(rewards)\n",
    "        std_r = np.std(rewards)\n",
    "        if self.verbose:\n",
    "            print(f\"Eval: mean reward {mean_r:.4f}, std {std_r:.4f}\")\n",
    "        return mean_r, std_r\n",
    "\n",
    "# ========== Usage Example ==========\n",
    "\n",
    "# Example env_factory for the env:\n",
    "class EnvFactory:\n",
    "    def __init__(self,\n",
    "                 env_class,\n",
    "                 df,\n",
    "                 feature_cols=None,\n",
    "                 internal_features=None,\n",
    "                 episode_length=100,\n",
    "                 transaction_cost=0.0001,\n",
    "                 seed=314, \n",
    "                 window_length=10,\n",
    "                 return_sequences=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.env_class = env_class\n",
    "        self.feature_cols = feature_cols\n",
    "        self.internal_features=internal_features\n",
    "        self.episode_length=episode_length\n",
    "        self.transaction_cost=transaction_cost\n",
    "        self.seed=seed\n",
    "        self.window_length=window_length\n",
    "        self.return_sequences=return_sequences\n",
    "    \n",
    "    def generate(self):\n",
    "        return self.env_class(self.df.copy(),\n",
    "                              feature_cols=self.feature_cols, \n",
    "                              internal_features=self.internal_features,\n",
    "                              episode_length=self.episode_length, \n",
    "                              window_length=self.window_length, \n",
    "                              transaction_cost=self.transaction_cost,\n",
    "                              seed=self.seed, \n",
    "                 \n",
    "                              return_sequences=self.return_sequences)\n",
    "\n",
    "SEED = 314\n",
    "env_factory = EnvFactory(\n",
    "    SequenceAwareCumulativeTradingEnv,\n",
    "    df, \n",
    "    feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=SEED,return_sequences=True)\n",
    "\n",
    "trainer = EpisodicPPOTrainer(\n",
    "    env_factory=env_factory.generate,\n",
    "    policy=\"MlpPolicy\",              # Can use custom\n",
    "    episode_length=100,\n",
    "    episodes_per_update=4,\n",
    "    total_episodes=500,\n",
    "    #agent_kwargs=dict(n_epochs=2, learning_rate=2e-4) \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4930e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49e51121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21bb0005890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38331a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'tensorboard' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir /logs/custom_agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
