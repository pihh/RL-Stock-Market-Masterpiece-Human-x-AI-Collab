{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495cca82",
   "metadata": {},
   "source": [
    "Absolutely ‚Äî here's the full **README.md** draft that documents the initial version of our **LTM Test Suite**: Learnability, Transferability, Meta-Evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä LTM Test Suite: Learnability, Transferability, and Meta-Evaluation\n",
    "\n",
    "This project establishes the **foundation for evaluating trading agents** based on their ability to:\n",
    "\n",
    "1. **Learn effectively** in specific environments (Learnability)\n",
    "2. **Generalize their knowledge** to new timeframes (Transferability)\n",
    "3. **Be selected or ranked** using meta-features (Meta-Evaluation)\n",
    "\n",
    "This evaluation system is central to our long-term goal of creating **regime-aware, introspective, intelligent agents** that know when and where they can succeed.\n",
    "\n",
    "---\n",
    "\n",
    "## üöß Status\n",
    "\n",
    "‚úÖ **Prototype phase (single-file implementation)**\n",
    "‚¨ú Modular pipeline with full CLI / batch capabilities\n",
    "‚¨ú Meta-learning model integration\n",
    "‚¨ú Curriculum design system\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Core Concepts\n",
    "\n",
    "### üîπ Learnability\n",
    "\n",
    "> Can an agent learn useful trading behavior in a specific environment?\n",
    "\n",
    "* Agent is trained on a single episode (e.g., 1 stock in 1 month)\n",
    "* Evaluated by its normalized episode score (`0‚Äì100`)\n",
    "* Uses fixed seeds and training steps for fair comparison\n",
    "* Logged across multiple runs to assess robustness\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Transferability\n",
    "\n",
    "> Can the knowledge acquired in one environment be applied to the next one?\n",
    "\n",
    "* Agent trained on `Month T`, evaluated (or fine-tuned) on `Month T+1`\n",
    "* Compared against:\n",
    "\n",
    "  * Random agent baseline\n",
    "  * Oracle performance\n",
    "* Transfer success = performance delta relative to training or baseline\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Meta-Evaluation\n",
    "\n",
    "> Can we predict which environments are promising *before* training?\n",
    "\n",
    "* Extracts meta-features from the environment:\n",
    "\n",
    "  * Volatility, momentum, entropy, Hurst, kurtosis, etc.\n",
    "* Generates `meta_df.csv` with:\n",
    "\n",
    "  * `learnability`, `agent advantage`, `transfer_delta` labels\n",
    "* Enables future predictive modeling and curriculum learning\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Test Protocol\n",
    "\n",
    "### Episode Sampling\n",
    "\n",
    "* All episodes:\n",
    "\n",
    "  * Start on Mondays\n",
    "  * Have fixed `n_timesteps`\n",
    "  * Are non-overlapping **or** weekly (depending on config)\n",
    "* Benchmark episodes are stored in:\n",
    "\n",
    "  ```\n",
    "  data/experiments/learnability_test/benchmark_episodes.json\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Agent Setup\n",
    "\n",
    "* Agents trained with PPO (via Stable-Baselines3)\n",
    "* Random agent used as baseline\n",
    "* Oracle score used as upper bound reference\n",
    "* Configurations, seeds, and policies are logged and reproducible\n",
    "\n",
    "---\n",
    "\n",
    "### Logging & Outputs\n",
    "\n",
    "| Output                    | Description                                    |\n",
    "| ------------------------- | ---------------------------------------------- |\n",
    "| `meta_df.csv`             | Meta-features + labels for each run            |\n",
    "| `checkpoints/{id}.zip`    | Trained agents saved with unique config hashes |\n",
    "| `scores/{id}_results.csv` | Per-step and final metrics                     |\n",
    "| `logs/`                   | Training logs and config snapshots             |\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Project Structure (Coming Soon)\n",
    "\n",
    "```\n",
    "ltm_suite/\n",
    "‚îú‚îÄ‚îÄ configs/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ benchmark_episodes.json\n",
    "‚îú‚îÄ‚îÄ benchmarks/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ scores/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ checkpoints/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ logs/\n",
    "‚îú‚îÄ‚îÄ results/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ meta_df.csv\n",
    "‚îú‚îÄ‚îÄ runners/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ run_learnability.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ run_transferability.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ run_meta_evaluation.py\n",
    "‚îú‚îÄ‚îÄ utils/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ env_loader.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metrics.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ logger.py\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Success Criteria\n",
    "\n",
    "* Episode scores consistently above 50% = good learning\n",
    "* Transfer performance better than random baseline = generalization\n",
    "* Meta-features predictive of good environments = meta-learning success\n",
    "* All results are reproducible and statistically valid (multiple seeds)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Next Milestone\n",
    "\n",
    "We now proceed to:\n",
    "\n",
    "* Implement a single-file prototype for **Learnability Test**\n",
    "* Store benchmark episodes\n",
    "* Save logs, scores, meta-data, and model checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to add/change anything before we start the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"core_learnability_test\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = boot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d87a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Running episode from idx 615 with seed 42\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m done, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 74\u001b[0m     action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     76\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "# ltm_test_suite.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv  # assumed to exist\n",
    "from data import sample_valid_episodes, extract_meta_features \n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SCORES_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(BENCHMARK_PATH), exist_ok=True)\n",
    "\n",
    "# ========== UTILITIES ==========\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "# ========== STEP 1: Load Data ==========\n",
    "print(\"[INFO] Loading data...\")\n",
    "# Replace this with real OHLCV loading\n",
    "df = OHLCV_DF.copy()#[OHLCV_DF['symbol']==TICKER].copy()\n",
    "\n",
    "\n",
    "# ========== STEP 2: Sample Benchmark Episodes ==========\n",
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(df, TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # ‚Üê ‚úÖ Convert to list here\n",
    "\n",
    "# ========== STEP 3: Run Learnability Tests ==========\n",
    "meta_records = []\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Running episode from idx {start_idx} with seed {seed}\")\n",
    "\n",
    "        # Prepare Env\n",
    "        env = Monitor(PositionTradingEnv(df, TICKER, N_TIMESTEPS, LOOKBACK, start_idx=start_idx, seed=seed))\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO agent\n",
    "        obs,_ = env.reset()\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done,terminated, info = env.step(action)\n",
    "            \n",
    "            score += reward\n",
    "\n",
    "        # Evaluate random agent\n",
    "        obs,_ = env.reset()\n",
    "        done, rand_score = False, 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done,terminated, info = env.step(action)\n",
    "            rand_score += reward\n",
    "\n",
    "        # Calculate advantage\n",
    "        advantage = score - rand_score\n",
    "\n",
    "        # Log meta-data\n",
    "        start_date = df.loc[start_idx, \"date\"]\n",
    "        end_date = df.loc[start_idx + N_TIMESTEPS - 1, \"date\"]\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        #meta = extract_meta_features(df.iloc[start_idx: start_idx + N_TIMESTEPS])\n",
    "        meta.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score\": score,\n",
    "            \"rand_score\": rand_score,\n",
    "            \"advantage\": advantage,\n",
    "            \"seed\": seed,\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date)\n",
    "        })\n",
    "        meta_records.append(meta)\n",
    "\n",
    "# ========== STEP 4: Save Results ==========\n",
    "pd.DataFrame(meta_records).to_csv(META_PATH, index=False)\n",
    "print(\"[INFO] Learnability test complete. Results saved to:\", META_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12b10a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([216.67], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
