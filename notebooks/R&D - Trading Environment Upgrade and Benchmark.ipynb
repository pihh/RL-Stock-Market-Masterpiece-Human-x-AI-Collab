{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d0c662",
   "metadata": {},
   "source": [
    "Fantastic ‚Äî your answers make it clear: we're building a **free-thinking, risk-aware, market-grounded learning trader**, with the potential to become not just profitable, but deeply **adaptive** and **introspective**. This will be like training a real human trader, starting simple and layering on cognition and tools.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary of Your Vision\n",
    "\n",
    "| Category              | Your Decision                                                          |\n",
    "| --------------------- | ---------------------------------------------------------------------- |\n",
    "| **Self-Awareness**    | Track PnL, trade history, confidence per trade                         |\n",
    "| **Market Context**    | Allow multi-timeframe (1D, 1W, maybe 1H later), and any usable feature |\n",
    "| **External Signals**  | Include events like earnings/fed/news if available                     |\n",
    "| **Risk Management**   | Wants liquidation/capital erosion + learned position sizing (v2+)      |\n",
    "| **Strategy Modeling** | Enable strategy playbooks and adaptive behavior                        |\n",
    "| **Meta-Learning**     | Agent should retain memory of past conditions, learn from meta-signals |\n",
    "| **Limitations**       | No peeking into future ‚Äî only prediction from available past           |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Now Here's the Plan: \"The Trader Intelligence Stack\"\n",
    "\n",
    "We'll organize this into **four layers** that build on each other. Each layer adds trader-like qualities and improves survivability and strategy creation.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 1: Survival & Orientation (v1)**\n",
    "\n",
    "> Minimal working agent that can hold/sell one stock, one timeframe, rewarded by position-based score.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* OHLCV (daily)\n",
    "* Agent‚Äôs current position\n",
    "* Time since position opened\n",
    "* Estimated profit/loss if selling now\n",
    "\n",
    "**Internal features:**\n",
    "\n",
    "* Current PnL (unrealized)\n",
    "* Position duration\n",
    "* Action history (last N actions ‚Äî optional at this stage)\n",
    "\n",
    "**Reward:**\n",
    "\n",
    "* Oracle-relative reward between 0‚Äì100 per episode (‚úÖ already implemented)\n",
    "\n",
    "**Goal:** Learn to enter/exit positions intelligently on one stock.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 2: Market Perception & Meta-Features**\n",
    "\n",
    "> Now the agent *reads the environment*, and we open it to *multi-feature* inputs.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Volatility, momentum, kurtosis, entropy, regime label, VIX, etc.\n",
    "* Optional: add price features from 3-day, 1-week trailing windows\n",
    "\n",
    "**Goal:** Learn to recognize **conditions** that precede profitable trends.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 3: Portfolio & Risk Awareness**\n",
    "\n",
    "> The agent now becomes a risk-aware trader.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Realized volatility, trailing drawdown\n",
    "* Simulated liquidation: episode ends if capital drops below X%\n",
    "* Optional: reward penalty for big drawdowns\n",
    "\n",
    "**Later upgrade:**\n",
    "\n",
    "* Learn dynamic position sizing (0%, 25%, 50%, 100%) or continuous size\n",
    "\n",
    "**Goal:** Survive, control risk, avoid death by bad trades.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 4: Strategic Thinking & Memory**\n",
    "\n",
    "> Agent becomes *introspective* and *adaptive* ‚Äî career-trader-level.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Confidence score (learned or predicted)\n",
    "* Episodic memory (compare current conditions to prior wins/losses)\n",
    "* Strategy archetype detection (trend following, mean reversion, etc.)\n",
    "* Meta-reward: evaluate *how well the agent acted*, not just profit\n",
    "\n",
    "**Goal:** Develop strategic behavior that generalizes to new situations.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 5: Real World Awareness (Optional Later)**\n",
    "\n",
    "> External signals from scraping/news pipelines and macro indicators.\n",
    "\n",
    "* Earnings day, Fed announcement, news sentiment\n",
    "* Sector rotation features (sector-relative strength)\n",
    "* Macro ETFs (SPY, QQQ, TLT, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Next Step: We Start at Layer 1\n",
    "\n",
    "Let‚Äôs:\n",
    "\n",
    "1. **Solidify internal features**: PnL, duration, trade history, position status.\n",
    "2. **Wrap it into the `PositionTradingEnv`** ‚Äî this becomes part of the observation.\n",
    "3. **Optionally**: Add rolling average reward, confidence, or simple position score.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Can I proceed to implement a Layer 1 `env.get_observation()` that includes:\n",
    "\n",
    "* Agent's current position (0 = flat, 1 = long)\n",
    "* Time since entry\n",
    "* Unrealized PnL (oracle-relative)\n",
    "* Normalized current price vs. entry price\n",
    "* One-hot day-of-week (already done)\n",
    "* Rolling average return over past N days\n",
    "* Optional: last 3 actions (as one-hot vectors)\n",
    "\n",
    "?\n",
    "\n",
    "Once this is done, we‚Äôll run the first agent and start logging learnability under **\"Survival Mode\"**.\n",
    "\n",
    "Ready to code this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from environments import PositionTradingEnv\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"trading_environment_development\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b875016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionTradingEnvV1(PositionTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.last_actions = [0, 0, 0]  # history of last 3 actions\n",
    "        self.entry_price = 0\n",
    "        self.holding_time = 0\n",
    "\n",
    "        obs_dim = 5 + 5 + 6  # core + day_of_week (5) + last_actions (3 x 2 one-hot)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.last_actions = [0, 0, 0]\n",
    "        self.entry_price = self.prices[self.step_idx]\n",
    "        self.holding_time = 0\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        if action == 1:\n",
    "            if self.position == 0:  # Enter long\n",
    "                self.entry_price = self.prices[self.step_idx]\n",
    "                self.holding_time = 0\n",
    "            else:  # Exit\n",
    "                self.entry_price = 0\n",
    "                self.holding_time = 0\n",
    "        elif self.position == 1:\n",
    "            self.holding_time += 1\n",
    "\n",
    "        self.last_actions = self.last_actions[1:] + [action]\n",
    "        return self._get_observation(), reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        price_now = self.prices[self.step_idx]\n",
    "\n",
    "        # If in position, use true entry price; otherwise, use current price as neutral baseline\n",
    "        entry_price = self.entry_price if self.position else price_now\n",
    "\n",
    "        # Safe PNL and ratio computation\n",
    "        if entry_price > 0:\n",
    "            pnl = (price_now - entry_price) / entry_price\n",
    "            price_ratio = price_now / entry_price\n",
    "        else:\n",
    "            pnl = 0.0\n",
    "            price_ratio = 1.0\n",
    "\n",
    "        # Rolling return\n",
    "        window_start = max(0, self.step_idx - 5)\n",
    "        if self.step_idx > window_start:\n",
    "            price_slice = self.prices[window_start:self.step_idx + 1]\n",
    "            rolling_ret = np.mean(np.diff(price_slice) / price_slice[:-1])\n",
    "        else:\n",
    "            rolling_ret = 0.0\n",
    "\n",
    "        # One-hot encode day of week\n",
    "        day = int(self.episode_df.iloc[self.step_idx][\"day_of_week\"])\n",
    "        day_one_hot = np.zeros(5)\n",
    "        day_one_hot[day] = 1\n",
    "\n",
    "        # One-hot encode last 3 actions\n",
    "        action_onehots = []\n",
    "        for a in self.last_actions:\n",
    "            onehot = np.zeros(2)\n",
    "            onehot[a] = 1\n",
    "            action_onehots.extend(onehot)\n",
    "\n",
    "        obs = np.array([\n",
    "            self.position,\n",
    "            self.holding_time,\n",
    "            pnl,\n",
    "            price_ratio,\n",
    "            rolling_ret,\n",
    "            *day_one_hot,\n",
    "            *action_onehots\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b1f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049cbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv\n",
    "from data import extract_meta_features\n",
    "\n",
    "def compute_additional_metrics(env):\n",
    "    if hasattr(env, \"env\"):  # unwrap Monitor\n",
    "        env = env.env\n",
    "    values = np.array(env.values)\n",
    "    rewards = np.array(env.rewards)\n",
    "    actions = np.array(env.actions)\n",
    "\n",
    "    returns = pd.Series(values).pct_change().dropna()\n",
    "    volatility = returns.std()\n",
    "    entropy = -np.sum(np.bincount(actions, minlength=2)/len(actions) * np.log2(np.bincount(actions, minlength=2)/len(actions) + 1e-9))\n",
    "    max_drawdown = (values / np.maximum.accumulate(values)).min() - 1\n",
    "    sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "    sortino = returns.mean() / (returns[returns < 0].std() + 1e-9) * np.sqrt(252)\n",
    "    calmar = returns.mean() / abs(max_drawdown + 1e-9)\n",
    "    success_trades = np.sum((np.diff(values) > 0) & (actions[1:] == 1)) + np.sum((np.diff(values) < 0) & (actions[1:] == 0))\n",
    "\n",
    "    return {\n",
    "        \"volatility\": volatility,\n",
    "        \"entropy\": entropy,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"calmar\": calmar,\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(actions == 0),\n",
    "        \"action_long_ratio\": np.mean(actions == 1)\n",
    "    }\n",
    "\n",
    "def formalized_transferability_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    env_cls: Callable = PositionTradingEnv,\n",
    "    env_version: str = \"v1\",\n",
    "    benchmark_path: str = \"data/experiments/learnability_test/benchmark_episodes.json\",\n",
    "    result_path: str = \"data/experiments/learnability_test/meta_df_transfer.csv\",\n",
    "    timesteps: int = 10_000,\n",
    "    n_timesteps: int = 60,\n",
    "    lookback: int = 0,\n",
    "    seeds: list = [42, 52, 62],\n",
    "    checkpoint_dir: str = \"data/experiments/learnability_test/checkpoints\",\n",
    "    agent_cls: Callable = PPO,\n",
    "    agent_name: str = \"PPO\",\n",
    "    agent_config: dict = None,\n",
    "    env_config: dict = None\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    def generate_config_hash(config):\n",
    "        raw = json.dumps(config, sort_keys=True)\n",
    "        return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "    def save_model(model, config_full, config_hash):\n",
    "        path = os.path.join(checkpoint_dir, f\"agent_{config_hash}.zip\")\n",
    "        model.save(path)\n",
    "        with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "            json.dump(config_full, f, indent=2)\n",
    "\n",
    "    print(\"[INFO] Loading benchmark episodes...\")\n",
    "    with open(benchmark_path) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "    \n",
    "    meta_records = []\n",
    "    df_ticker = df[df['symbol'] == ticker].reset_index(drop=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        existing = pd.read_csv(result_path)\n",
    "        seen_hashes = set(existing['config_hash'].unique())\n",
    "    else:\n",
    "        seen_hashes = set()\n",
    "  \n",
    "    for seed in seeds:\n",
    "        for start_idx in benchmark_episodes:\n",
    "            \n",
    "            test_idx = start_idx + n_timesteps\n",
    "            if test_idx + n_timesteps >= len(df_ticker):\n",
    "                print(\"[WARN] Skipping episode ‚Äî test idx out of range\")\n",
    "                continue\n",
    "\n",
    "            config = {\n",
    "                \"ticker\": ticker,\n",
    "                \"train_idx\": int(start_idx),\n",
    "                \"test_idx\": int(test_idx),\n",
    "                \"timesteps\": timesteps,\n",
    "                \"seed\": seed,\n",
    "                \"env_version\": env_version,\n",
    "                \"env_config\": env_config,\n",
    "                \"agent_name\": agent_name,\n",
    "                \"agent_config\": agent_config\n",
    "            }\n",
    "            config_hash = generate_config_hash(config)\n",
    "            if config_hash in seen_hashes:\n",
    "                print(f\"[INFO] Skipping previously completed run: {config_hash}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Transferability: seed={seed}, start_idx={start_idx}, config_hash={config_hash}\")\n",
    "\n",
    "            env_train = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=start_idx, **(env_config or {})))\n",
    "            model = agent_cls(\"MlpPolicy\", env_train, verbose=0, seed=seed, **(agent_config or {}))\n",
    "            model.learn(total_timesteps=timesteps)\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, score_train = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                score_train += reward\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, rand_train = False, 0\n",
    "            while not done:\n",
    "                action = env_train.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                rand_train += reward\n",
    "\n",
    "            env_test = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=test_idx, **(env_config or {})))\n",
    "            obs, _ = env_test.reset()\n",
    "            done, score_test = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                score_test += reward\n",
    "\n",
    "            obs, _ = env_test.reset()\n",
    "            done, rand_test = False, 0\n",
    "            while not done:\n",
    "                action = env_test.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                rand_test += reward\n",
    "\n",
    "            advantage_train = score_train - rand_train\n",
    "            advantage_test = score_test - rand_test\n",
    "            transfer_delta = score_test - score_train\n",
    "\n",
    "            save_model(model, config, config_hash)\n",
    "\n",
    "            meta = extract_meta_features(df_ticker.iloc[start_idx:start_idx + n_timesteps])\n",
    "            diagnostics = compute_additional_metrics(env_test)\n",
    "\n",
    "            meta.update({\n",
    "                \"config_hash\": config_hash,\n",
    "                \"env_version\": env_version,\n",
    "                \"agent_name\": agent_name,\n",
    "                \"score_train\": score_train,\n",
    "                \"score_test\": score_test,\n",
    "                \"advantage_train\": advantage_train,\n",
    "                \"advantage_test\": advantage_test,\n",
    "                \"transfer_delta\": transfer_delta,\n",
    "                \"transfer_success\": int(transfer_delta > 0),\n",
    "                \"ticker\": ticker,\n",
    "                \"config\":json.dumps(config),\n",
    "                \"seed\": seed,\n",
    "                **diagnostics\n",
    "            })\n",
    "            meta_records.append(meta)\n",
    "\n",
    "    result_df = pd.DataFrame(meta_records)\n",
    "    if os.path.exists(result_path):\n",
    "        result_df = pd.concat([pd.read_csv(result_path), result_df], ignore_index=True)\n",
    "    result_df.to_csv(result_path, index=False)\n",
    "    print(\"[INFO] Transferability test complete. Results saved to:\", result_path)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94462f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/experiments/trading_environment_development/benchmark_episodes.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BENCHMARK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fa80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epis√≥dios de benchmark salvos em: data/experiments/trading_environment_development/benchmark_episodes.json\n",
      "[INFO] Loading benchmark episodes...\n",
      "[INFO] Transferability: seed=42, start_idx=615, config_hash=d209da2d5e9c2c270138c7ac8b78de26010774788594fbb9a2e190c4429b8d34\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(OHLCV_DF[OHLCV_DF['symbol']==TICKER], TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # ‚Üê ‚úÖ Convert to list here\n",
    "\n",
    "print(\"[INFO] Epis√≥dios de benchmark salvos em:\", BENCHMARK_PATH)\n",
    "result_df = formalized_transferability_evaluation(\n",
    "    df=OHLCV_DF.copy(),\n",
    "    ticker=\"AAPL\",\n",
    "    env_cls=PositionTradingEnv,\n",
    "    env_version=\"0\",  # useful if you upgrade the environment logic later\n",
    "    benchmark_path=DEFAULT_PATH+\"/benchmark_episodes.json\",\n",
    "    result_path=DEFAULT_PATH+\"/meta_df_transfer.csv\",\n",
    "    timesteps=100_000,\n",
    "    n_timesteps=60,\n",
    "    lookback=0,\n",
    "    seeds=[42, 52, 62],  # or just [42] for quick run\n",
    "    checkpoint_dir=DEFAULT_PATH+\"/checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28118cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "def compare_environments(result_df,env_version_a=\"v0\",env_version_b = \"v1\"):\n",
    "    \n",
    "\n",
    "    summary = result_df.groupby(\"env_version\")[[\n",
    "        \"score_train\", \"score_test\", \"advantage_train\", \"advantage_test\",\n",
    "        \"transfer_delta\", \"success_trades\", \"sharpe\", \"sortino\", \"calmar\",\n",
    "        \"max_drawdown\", \"volatility\", \"action_hold_ratio\", \"action_long_ratio\"\n",
    "    ]].agg([\"mean\", \"std\", \"median\"]).T\n",
    "    \n",
    "\n",
    "    mean_df = summary.xs('mean', level=1)\n",
    "    # Compute absolute difference between env_version 1 and 0\n",
    "    diffs = (mean_df['v1'] - mean_df['v0']).abs().sort_values(ascending=False)\n",
    "    print(diffs)\n",
    "    # Plot using this sorted order\n",
    "    mean_df.loc[diffs.index].plot.bar(\n",
    "        figsize=(14, 6),\n",
    "        title=\"Env v1 vs v0 ‚Äì Mean metric comparison (sorted by difference)\",\n",
    "        ylabel=\"Mean Value\"\n",
    "    )\n",
    "    metrics = [\"score_test\", \"advantage_test\", \"transfer_delta\", \"sharpe\", \"sortino\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        v0 = result_df[result_df.env_version == env_version_a][metric]\n",
    "        v1 = result_df[result_df.env_version == env_version_b][metric]\n",
    "        stat, pval = ttest_ind(v0, v1)\n",
    "        print(f\"{metric}: p={pval:.4f} | {env_version_a}_mean={v0.mean():.3f}, {env_version_b}_mean={v1.mean():.3f}\")\n",
    "\n",
    "    for metric in metrics:\n",
    "        sns.boxplot(data=result_df, x=\"env_version\", y=metric)\n",
    "        plt.title(f\"{metric} by Environment Version\")\n",
    "        plt.show()\n",
    "        \n",
    "    result_df['composite_score'] = (\n",
    "        result_df['advantage_test'] +\n",
    "        result_df['transfer_delta'] +\n",
    "        result_df['sharpe'] * 5 -\n",
    "        result_df['max_drawdown'] * 10\n",
    "    )\n",
    "\n",
    "    return result_df,result_df.groupby(\"env_version\")[\"composite_score\"].mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e260e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary = compare_environments(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87353e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b47e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
