{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d0c662",
   "metadata": {},
   "source": [
    "Fantastic ‚Äî your answers make it clear: we're building a **free-thinking, risk-aware, market-grounded learning trader**, with the potential to become not just profitable, but deeply **adaptive** and **introspective**. This will be like training a real human trader, starting simple and layering on cognition and tools.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary of Your Vision\n",
    "\n",
    "| Category              | Your Decision                                                          |\n",
    "| --------------------- | ---------------------------------------------------------------------- |\n",
    "| **Self-Awareness**    | Track PnL, trade history, confidence per trade                         |\n",
    "| **Market Context**    | Allow multi-timeframe (1D, 1W, maybe 1H later), and any usable feature |\n",
    "| **External Signals**  | Include events like earnings/fed/news if available                     |\n",
    "| **Risk Management**   | Wants liquidation/capital erosion + learned position sizing (v2+)      |\n",
    "| **Strategy Modeling** | Enable strategy playbooks and adaptive behavior                        |\n",
    "| **Meta-Learning**     | Agent should retain memory of past conditions, learn from meta-signals |\n",
    "| **Limitations**       | No peeking into future ‚Äî only prediction from available past           |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Now Here's the Plan: \"The Trader Intelligence Stack\"\n",
    "\n",
    "We'll organize this into **four layers** that build on each other. Each layer adds trader-like qualities and improves survivability and strategy creation.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 1: Survival & Orientation (v1)**\n",
    "\n",
    "> Minimal working agent that can hold/sell one stock, one timeframe, rewarded by position-based score.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* OHLCV (daily)\n",
    "* Agent‚Äôs current position\n",
    "* Time since position opened\n",
    "* Estimated profit/loss if selling now\n",
    "\n",
    "**Internal features:**\n",
    "\n",
    "* Current PnL (unrealized)\n",
    "* Position duration\n",
    "* Action history (last N actions ‚Äî optional at this stage)\n",
    "\n",
    "**Reward:**\n",
    "\n",
    "* Oracle-relative reward between 0‚Äì100 per episode (‚úÖ already implemented)\n",
    "\n",
    "**Goal:** Learn to enter/exit positions intelligently on one stock.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 2: Market Perception & Meta-Features**\n",
    "\n",
    "> Now the agent *reads the environment*, and we open it to *multi-feature* inputs.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Volatility, momentum, kurtosis, entropy, regime label, VIX, etc.\n",
    "* Optional: add price features from 3-day, 1-week trailing windows\n",
    "\n",
    "**Goal:** Learn to recognize **conditions** that precede profitable trends.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 3: Portfolio & Risk Awareness**\n",
    "\n",
    "> The agent now becomes a risk-aware trader.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Realized volatility, trailing drawdown\n",
    "* Simulated liquidation: episode ends if capital drops below X%\n",
    "* Optional: reward penalty for big drawdowns\n",
    "\n",
    "**Later upgrade:**\n",
    "\n",
    "* Learn dynamic position sizing (0%, 25%, 50%, 100%) or continuous size\n",
    "\n",
    "**Goal:** Survive, control risk, avoid death by bad trades.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 4: Strategic Thinking & Memory**\n",
    "\n",
    "> Agent becomes *introspective* and *adaptive* ‚Äî career-trader-level.\n",
    "\n",
    "**Additions:**\n",
    "\n",
    "* Confidence score (learned or predicted)\n",
    "* Episodic memory (compare current conditions to prior wins/losses)\n",
    "* Strategy archetype detection (trend following, mean reversion, etc.)\n",
    "* Meta-reward: evaluate *how well the agent acted*, not just profit\n",
    "\n",
    "**Goal:** Develop strategic behavior that generalizes to new situations.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Layer 5: Real World Awareness (Optional Later)**\n",
    "\n",
    "> External signals from scraping/news pipelines and macro indicators.\n",
    "\n",
    "* Earnings day, Fed announcement, news sentiment\n",
    "* Sector rotation features (sector-relative strength)\n",
    "* Macro ETFs (SPY, QQQ, TLT, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Next Step: We Start at Layer 1\n",
    "\n",
    "Let‚Äôs:\n",
    "\n",
    "1. **Solidify internal features**: PnL, duration, trade history, position status.\n",
    "2. **Wrap it into the `PositionTradingEnv`** ‚Äî this becomes part of the observation.\n",
    "3. **Optionally**: Add rolling average reward, confidence, or simple position score.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Can I proceed to implement a Layer 1 `env.get_observation()` that includes:\n",
    "\n",
    "* Agent's current position (0 = flat, 1 = long)\n",
    "* Time since entry\n",
    "* Unrealized PnL (oracle-relative)\n",
    "* Normalized current price vs. entry price\n",
    "* One-hot day-of-week (already done)\n",
    "* Rolling average return over past N days\n",
    "* Optional: last 3 actions (as one-hot vectors)\n",
    "\n",
    "?\n",
    "\n",
    "Once this is done, we‚Äôll run the first agent and start logging learnability under **\"Survival Mode\"**.\n",
    "\n",
    "Ready to code this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"trading_environment_development\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "MARKET_FEATURES = ['close']\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "MARKET_FEATURES.sort()\n",
    "SEEDS.sort()\n",
    "\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3276b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f2334534",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv\n",
    "from data import extract_meta_features\n",
    "\n",
    "def compute_additional_metrics(env):\n",
    "    if hasattr(env, \"env\"):  # unwrap Monitor\n",
    "        env = env.env\n",
    "    values = np.array(env.values)\n",
    "    rewards = np.array(env.rewards)\n",
    "    actions = np.array(env.actions)\n",
    "\n",
    "    returns = pd.Series(values).pct_change().dropna()\n",
    "    volatility = returns.std()\n",
    "    entropy = -np.sum(np.bincount(actions, minlength=2)/len(actions) * np.log2(np.bincount(actions, minlength=2)/len(actions) + 1e-9))\n",
    "    max_drawdown = (values / np.maximum.accumulate(values)).min() - 1\n",
    "    sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "    sortino = returns.mean() / (returns[returns < 0].std() + 1e-9) * np.sqrt(252)\n",
    "    calmar = returns.mean() / abs(max_drawdown + 1e-9)\n",
    "    success_trades = np.sum((np.diff(values) > 0) & (actions[1:] == 1)) + np.sum((np.diff(values) < 0) & (actions[1:] == 0))\n",
    "\n",
    "    return {\n",
    "        \"volatility\": volatility,\n",
    "        \"entropy\": entropy,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"calmar\": calmar,\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(actions == 0),\n",
    "        \"action_long_ratio\": np.mean(actions == 1)\n",
    "    }\n",
    "\n",
    "def formalized_transferability_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    env_cls: Callable = PositionTradingEnv,\n",
    "    env_version: str = \"v1\",\n",
    "    benchmark_path: str = \"data/experiments/learnability_test/benchmark_episodes.json\",\n",
    "    result_path: str = \"data/experiments/learnability_test/meta_df_transfer.csv\",\n",
    "    timesteps: int = 10_000,\n",
    "    n_timesteps: int = 60,\n",
    "    lookback: int = 0,\n",
    "    seeds: list = [42, 52, 62],\n",
    "    checkpoint_dir: str = \"data/experiments/learnability_test/checkpoints\",\n",
    "    agent_cls: Callable = PPO,\n",
    "    \n",
    "    agent_config: dict = None,\n",
    "    env_config: dict = None\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    agent_name: str = agent_cls.__name__\n",
    "\n",
    "    def generate_config_hash(config):\n",
    "        raw = json.dumps(config, sort_keys=True)\n",
    "        return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "    def save_model(model, config_full, config_hash):\n",
    "        path = os.path.join(checkpoint_dir, f\"agent_{config_hash}.zip\")\n",
    "        model.save(path)\n",
    "        with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "            json.dump(config_full, f, indent=2)\n",
    "\n",
    "    print(\"[INFO] Loading benchmark episodes...\")\n",
    "    with open(benchmark_path) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "    \n",
    "    meta_records = []\n",
    "    df_ticker = df[df['symbol'] == ticker].reset_index(drop=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        existing = pd.read_csv(result_path)\n",
    "        seen_hashes = set(existing['config_hash'].unique())\n",
    "    else:\n",
    "        seen_hashes = set()\n",
    "  \n",
    "    for seed in seeds:\n",
    "        for start_idx in benchmark_episodes:\n",
    "            \n",
    "            test_idx = start_idx + n_timesteps\n",
    "            if test_idx + n_timesteps >= len(df_ticker):\n",
    "                print(\"[WARN] Skipping episode ‚Äî test idx out of range\")\n",
    "                continue\n",
    "\n",
    "            config = {\n",
    "                \"ticker\": ticker,\n",
    "                \"train_idx\": int(start_idx),\n",
    "                \"test_idx\": int(test_idx),\n",
    "                \"timesteps\": timesteps,\n",
    "                \"seed\": seed,\n",
    "                \"env_version\": env_version,\n",
    "                \"env_config\": env_config,\n",
    "                \"agent_name\": agent_name,\n",
    "                \"agent_config\": agent_config,\n",
    "            }\n",
    "            config_hash = generate_config_hash(config)\n",
    "            if config_hash in seen_hashes:\n",
    "                print(f\"[INFO] Skipping previously completed run: {config_hash}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Transferability: seed={seed}, start_idx={start_idx}, config_hash={config_hash}\")\n",
    "\n",
    " \n",
    "            env_train = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=start_idx, **(env_config or {})))\n",
    "            model = agent_cls(\"MlpPolicy\", env_train, verbose=0, seed=seed, **(agent_config or {}))\n",
    "            model.learn(total_timesteps=timesteps)\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, score_train = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                score_train += reward\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, rand_train = False, 0\n",
    "            while not done:\n",
    "                action = env_train.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                rand_train += reward\n",
    "\n",
    "            env_test = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=test_idx, **(env_config or {})))\n",
    "            obs, _ = env_test.reset()\n",
    "            done, score_test = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                score_test += reward\n",
    "\n",
    "            obs, _ = env_test.reset()\n",
    "            done, rand_test = False, 0\n",
    "            while not done:\n",
    "                action = env_test.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                rand_test += reward\n",
    "\n",
    "            advantage_train = score_train - rand_train\n",
    "            advantage_test = score_test - rand_test\n",
    "            transfer_delta = score_test - score_train\n",
    "\n",
    "            save_model(model, config, config_hash)\n",
    "\n",
    "            meta = extract_meta_features(df_ticker.iloc[start_idx:start_idx + n_timesteps])\n",
    "            diagnostics = compute_additional_metrics(env_test)\n",
    "\n",
    "            meta.update({\n",
    "                \"config_hash\": config_hash,\n",
    "                \"env_version\": env_version,\n",
    "                \"agent_name\": agent_name,\n",
    "                \"score_train\": score_train,\n",
    "                \"score_test\": score_test,\n",
    "                \"advantage_train\": advantage_train,\n",
    "                \"advantage_test\": advantage_test,\n",
    "                \"transfer_delta\": transfer_delta,\n",
    "                \"transfer_success\": int(transfer_delta > 0),\n",
    "                \"ticker\": ticker,\n",
    "                \"config\":json.dumps(config),\n",
    "                \"seed\": seed,\n",
    "                **diagnostics\n",
    "            })\n",
    "            meta_records.append(meta)\n",
    "\n",
    "    result_df = pd.DataFrame(meta_records)\n",
    "    if os.path.exists(result_path):\n",
    "        result_df = pd.concat([pd.read_csv(result_path), result_df], ignore_index=True)\n",
    "    result_df.to_csv(result_path, index=False)\n",
    "    print(\"[INFO] Transferability test complete. Results saved to:\", result_path)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21afd882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/experiments/trading_environment_development/benchmark_episodes.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BENCHMARK_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88c4d83",
   "metadata": {},
   "source": [
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(OHLCV_DF[OHLCV_DF['symbol']==TICKER], TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # ‚Üê ‚úÖ Convert to list here\n",
    "\n",
    "print(\"[INFO] Epis√≥dios de benchmark salvos em:\", BENCHMARK_PATH)\n",
    "result_df = formalized_transferability_evaluation(\n",
    "    df=OHLCV_DF.copy(),\n",
    "    ticker=\"AAPL\",\n",
    "    env_cls=PositionTradingEnvV1,\n",
    "    env_version=\"v1\",  # useful if you upgrade the environment logic later\n",
    "    benchmark_path=DEFAULT_PATH+\"/benchmark_episodes.json\",\n",
    "    result_path=DEFAULT_PATH+\"/meta_df_transfer.csv\",\n",
    "    timesteps=100_000,\n",
    "    n_timesteps=60,\n",
    "    lookback=0,\n",
    "    seeds=[42, 52, 62],  # or just [42] for quick run\n",
    "    checkpoint_dir=DEFAULT_PATH+\"/checkpoints\",\n",
    "    env_config={\"market_features\":MARKET_FEATURES}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af0d9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionTradingEnvV1.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf9e9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionTradingEnv.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8c7823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<environments.PositionTradingEnv at 0x2d47fd3e910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = PositionTradingEnv(OHLCV_DF[OHLCV_DF['symbol']==TICKER], ticker=TICKER, seed=42, start_idx=4)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b169740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price 172.19\n",
      "price 175.08 reward 0.03755481697733883\n",
      "price 175.53 reward 0.000910531535531262\n",
      "price 172.19 reward -0.05016062023591644\n"
     ]
    }
   ],
   "source": [
    "e.reset()\n",
    "print('price', e.prices[e.step_idx])\n",
    "a,b,c,d,_ =e.step(1)\n",
    "print('price',e.prices[e.step_idx], 'reward',b)\n",
    "a,b,c,d,_ =e.step(1)\n",
    "print('price',e.prices[e.step_idx], 'reward',b)\n",
    "a,b,c,d,_ =e.step(1)\n",
    "print('price',e.prices[e.step_idx], 'reward',b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314a17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.19\n",
      "-0.03755481697733883 175.08\n",
      "-0.000910531535531262 175.53\n",
      "0.05016062023591644 172.19\n"
     ]
    }
   ],
   "source": [
    "e.reset()\n",
    "print(e.prices[e.step_idx])\n",
    "a,b,c,d,_ =e.step(0)\n",
    "print(b, e.prices[e.step_idx])\n",
    "a,b,c,d,_ =e.step(0)\n",
    "print(b,e.prices[e.step_idx])\n",
    "a,b,c,d,_ =e.step(0)\n",
    "print(b,e.prices[e.step_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173877bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of raw rel returns: 0.8977696292553151\n",
      "Sum of normalized weights: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "e = PositionTradingEnvV1(OHLCV_DF[OHLCV_DF['symbol']==TICKER], ticker=TICKER, seed=42, start_idx=4)\n",
    "print(\"Sum of raw rel returns:\", np.sum([\n",
    "    abs((e.prices[i + 1] - e.prices[i]) / e.prices[i])\n",
    "    for i in range(len(e.prices) - 1)\n",
    "]))\n",
    "\n",
    "print(\"Sum of normalized weights:\", np.sum(e.step_weights))  # This should be 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978973e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b174b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".2* np.sign(10.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d0f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
