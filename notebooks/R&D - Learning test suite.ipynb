{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a06666",
   "metadata": {},
   "source": [
    "Absolutely â€” here's the full **README.md** draft that documents the initial version of our **LTM Test Suite**: Learnability, Transferability, Meta-Evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“Š LTM Test Suite: Learnability, Transferability, and Meta-Evaluation\n",
    "\n",
    "This project establishes the **foundation for evaluating trading agents** based on their ability to:\n",
    "\n",
    "1. **Learn effectively** in specific environments (Learnability)\n",
    "2. **Generalize their knowledge** to new timeframes (Transferability)\n",
    "3. **Be selected or ranked** using meta-features (Meta-Evaluation)\n",
    "\n",
    "This evaluation system is central to our long-term goal of creating **regime-aware, introspective, intelligent agents** that know when and where they can succeed.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš§ Status\n",
    "\n",
    "âœ… **Prototype phase (single-file implementation)**\n",
    "â¬œ Modular pipeline with full CLI / batch capabilities\n",
    "â¬œ Meta-learning model integration\n",
    "â¬œ Curriculum design system\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Core Concepts\n",
    "\n",
    "### ðŸ”¹ Learnability\n",
    "\n",
    "> Can an agent learn useful trading behavior in a specific environment?\n",
    "\n",
    "* Agent is trained on a single episode (e.g., 1 stock in 1 month)\n",
    "* Evaluated by its normalized episode score (`0â€“100`)\n",
    "* Uses fixed seeds and training steps for fair comparison\n",
    "* Logged across multiple runs to assess robustness\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Transferability\n",
    "\n",
    "> Can the knowledge acquired in one environment be applied to the next one?\n",
    "\n",
    "* Agent trained on `Month T`, evaluated (or fine-tuned) on `Month T+1`\n",
    "* Compared against:\n",
    "\n",
    "  * Random agent baseline\n",
    "  * Oracle performance\n",
    "* Transfer success = performance delta relative to training or baseline\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Meta-Evaluation\n",
    "\n",
    "> Can we predict which environments are promising *before* training?\n",
    "\n",
    "* Extracts meta-features from the environment:\n",
    "\n",
    "  * Volatility, momentum, entropy, Hurst, kurtosis, etc.\n",
    "* Generates `meta_df.csv` with:\n",
    "\n",
    "  * `learnability`, `agent advantage`, `transfer_delta` labels\n",
    "* Enables future predictive modeling and curriculum learning\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Test Protocol\n",
    "\n",
    "### Episode Sampling\n",
    "\n",
    "* All episodes:\n",
    "\n",
    "  * Start on Mondays\n",
    "  * Have fixed `n_timesteps`\n",
    "  * Are non-overlapping **or** weekly (depending on config)\n",
    "* Benchmark episodes are stored in:\n",
    "\n",
    "  ```\n",
    "  data/experiments/learnability_test/benchmark_episodes.json\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Agent Setup\n",
    "\n",
    "* Agents trained with PPO (via Stable-Baselines3)\n",
    "* Random agent used as baseline\n",
    "* Oracle score used as upper bound reference\n",
    "* Configurations, seeds, and policies are logged and reproducible\n",
    "\n",
    "---\n",
    "\n",
    "### Logging & Outputs\n",
    "\n",
    "| Output                    | Description                                    |\n",
    "| ------------------------- | ---------------------------------------------- |\n",
    "| `meta_df.csv`             | Meta-features + labels for each run            |\n",
    "| `checkpoints/{id}.zip`    | Trained agents saved with unique config hashes |\n",
    "| `scores/{id}_results.csv` | Per-step and final metrics                     |\n",
    "| `logs/`                   | Training logs and config snapshots             |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Project Structure (Coming Soon)\n",
    "\n",
    "```\n",
    "ltm_suite/\n",
    "â”œâ”€â”€ configs/\n",
    "â”‚   â””â”€â”€ benchmark_episodes.json\n",
    "â”œâ”€â”€ benchmarks/\n",
    "â”‚   â””â”€â”€ scores/\n",
    "â”‚   â””â”€â”€ checkpoints/\n",
    "â”‚   â””â”€â”€ logs/\n",
    "â”œâ”€â”€ results/\n",
    "â”‚   â””â”€â”€ meta_df.csv\n",
    "â”œâ”€â”€ runners/\n",
    "â”‚   â”œâ”€â”€ run_learnability.py\n",
    "â”‚   â”œâ”€â”€ run_transferability.py\n",
    "â”‚   â””â”€â”€ run_meta_evaluation.py\n",
    "â”œâ”€â”€ utils/\n",
    "â”‚   â””â”€â”€ env_loader.py\n",
    "â”‚   â””â”€â”€ metrics.py\n",
    "â”‚   â””â”€â”€ logger.py\n",
    "â””â”€â”€ README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Success Criteria\n",
    "\n",
    "* Episode scores consistently above 50% = good learning\n",
    "* Transfer performance better than random baseline = generalization\n",
    "* Meta-features predictive of good environments = meta-learning success\n",
    "* All results are reproducible and statistically valid (multiple seeds)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Next Milestone\n",
    "\n",
    "We now proceed to:\n",
    "\n",
    "* Implement a single-file prototype for **Learnability Test**\n",
    "* Store benchmark episodes\n",
    "* Save logs, scores, meta-data, and model checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to add/change anything before we start the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"core_learnability_test\"\n",
    "DEFAULT_PATH = \"/data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = boot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d87a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Sampling benchmark episodes...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Sampling benchmark episodes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m benchmark_episodes \u001b[38;5;241m=\u001b[39m sample_valid_episodes(df, TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(BENCHMARK_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     56\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(benchmark_episodes, f)\n",
      "File \u001b[1;32m~\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\data.py:31\u001b[0m, in \u001b[0;36msample_valid_episodes\u001b[1;34m(df, ticker, n_timesteps, lookback, episodes, seed)\u001b[0m\n\u001b[0;32m     28\u001b[0m         valid_starts\u001b[38;5;241m.\u001b[39mappend(start_idx)\n\u001b[0;32m     30\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed)\n\u001b[1;32m---> 31\u001b[0m sampled_starts \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mchoice(valid_starts, size\u001b[38;5;241m=\u001b[39mepisodes, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sampled_starts\n",
      "File \u001b[1;32m_generator.pyx:729\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# ltm_test_suite.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv  # assumed to exist\n",
    "from data import sample_valid_episodes, extract_meta_features \n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SCORES_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(BENCHMARK_PATH), exist_ok=True)\n",
    "\n",
    "# ========== UTILITIES ==========\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "# ========== STEP 1: Load Data ==========\n",
    "print(\"[INFO] Loading data...\")\n",
    "# Replace this with real OHLCV loading\n",
    "df = OHLCV_DF[OHLCV_DF['symbol']==TICKER].copy()\n",
    "\n",
    "\n",
    "# ========== STEP 2: Sample Benchmark Episodes ==========\n",
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(df, TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes, f)\n",
    "\n",
    "# ========== STEP 3: Run Learnability Tests ==========\n",
    "meta_records = []\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Running episode from idx {start_idx} with seed {seed}\")\n",
    "\n",
    "        # Prepare Env\n",
    "        env = Monitor(PositionTradingEnv(df, TICKER, N_TIMESTEPS, LOOKBACK, start_idx=start_idx, seed=seed))\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO agent\n",
    "        obs = env.reset()\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "        # Evaluate random agent\n",
    "        obs = env.reset()\n",
    "        done, rand_score = False, 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rand_score += reward\n",
    "\n",
    "        # Calculate advantage\n",
    "        advantage = score - rand_score\n",
    "\n",
    "        # Log meta-data\n",
    "        start_date = df.loc[start_idx, \"date\"]\n",
    "        end_date = df.loc[start_idx + N_TIMESTEPS - 1, \"date\"]\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        #meta = extract_meta_features(df.iloc[start_idx: start_idx + N_TIMESTEPS])\n",
    "        meta.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score\": score,\n",
    "            \"rand_score\": rand_score,\n",
    "            \"advantage\": advantage,\n",
    "            \"seed\": seed,\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date)\n",
    "        })\n",
    "        meta_records.append(meta)\n",
    "\n",
    "# ========== STEP 4: Save Results ==========\n",
    "pd.DataFrame(meta_records).to_csv(META_PATH, index=False)\n",
    "print(\"[INFO] Learnability test complete. Results saved to:\", META_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f613996",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "* if price goes up and agent is holding, his reward = % price up / max_ep_reward\n",
    "* Best possible score = 1\n",
    "* Worst = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
