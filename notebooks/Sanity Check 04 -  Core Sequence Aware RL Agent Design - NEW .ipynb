{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "#  Core Sequence-Aware Agent Design v2\n",
    "\n",
    "This experiment explores a transformer-based recurrent PPO agent for financial trading. The environment is sequence-aware and includes both regime-based augmentation and per-episode reward normalization. The agent is evaluated across top 2 stocks in each sector using structured episode sequences to assess learning generalization.\n",
    "\n",
    "---\n",
    "\n",
    "##  Experiment Configuration\n",
    "\n",
    "| Parameter               | Value                         |\n",
    "|-------------------------|-------------------------------|\n",
    "| Agent                   | Recurrent PPO + Transformer   |\n",
    "| Env Wrapper             | RegimeAugmentingWrapper + PerEpisodeRewardNormalizer |\n",
    "| Episode Length          | 100                           |\n",
    "| Episodes                | 20                            |\n",
    "| Eval Episodes           | 3 per iteration               |\n",
    "| Steps per Update        | 800                           |\n",
    "| Batch Size              | 100                           |\n",
    "| Total Timesteps         | 20,000                        |\n",
    "| Learning Rate           | 0.0003                        |\n",
    "| Entropy Coefficient     | 0.005                         |\n",
    "| Value Function Coeff    | 0.5                           |\n",
    "| Max Gradient Norm       | 0.5                           |\n",
    "| Normalize Advantage     | True                          |\n",
    "| Optimizer               | Adam                          |\n",
    "| Transformer d_model     | 64                            |\n",
    "| Heads                  | 4                             |\n",
    "| Layers                 | 2                             |\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- **Train Set:** 2023-01-01 → 2023-07-01\n",
    "- **Test Set:** 2023-07-01 → 2024-01-01\n",
    "- **Assets:** Top 2 stocks by sector\n",
    "- **Sequence Split:** 80% train / 20% eval sequences\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent Architecture\n",
    "\n",
    "- **Feature Extractor:** Transformer encoder with causal mask and learnable positional encoding.\n",
    "- **Policy Class:** Custom `TransformerPolicy` extending `RecurrentActorCriticPolicy`.\n",
    "- **Reward Normalization:** Online normalization within episodes.\n",
    "- **Regime Augmentation:** Appends one-hot encoded market regime to each timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Summary (Selected Stats)\n",
    "\n",
    "| Timesteps | Ep Rew Mean | Explained Variance | Value Loss | Policy Grad Loss |\n",
    "|-----------|-------------|--------------------|------------|------------------|\n",
    "|  8000     | 3.21        | 0.15               | 4.68       | -0.00277         |\n",
    "| 14400     | 2.90        | 0.645              | 1.71       |  0.00163         |\n",
    "| 20000     | **3.79**    | **0.751**          | 1.59       | -0.00012         |\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Snapshots\n",
    "\n",
    "| Timestep | Mean Reward | Std Dev | Eval Length |\n",
    "|----------|-------------|---------|-------------|\n",
    "| 5000     | -8.17       | ±8.72   | 102         |\n",
    "| 10000    | -4.43       | ±7.59   | 102         |\n",
    "| 15000    | -6.17       | ±4.92   | 102         |\n",
    "| 20000    | **0.24**    | ±2.86   | 102         |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "### Paired t-test and Mann-Whitney U-test\n",
    "\n",
    "| Metric       | t-test p-value | Mann-Whitney p-value |\n",
    "|--------------|----------------|-----------------------|\n",
    "| `total_reward` | 0.0300         | 0.0075                |\n",
    "| `calmar`       | 0.0132         | 0.0075                |\n",
    "\n",
    "✅ Both `total_reward` and `calmar ratio` show **statistically significant** improvements compared to the baseline.  \n",
    "Especially, Calmar implies **more stable and risk-adjusted returns**.\n",
    "\n",
    "> Note: `sharpe`, `sortino`, and `final_wealth` were skipped due to empty values in the evaluation logs. Ensure metrics are logged and valid across all test episodes to include them.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Fix missing metrics** (`sharpe`, `sortino`, etc.) in the logging pipeline.\n",
    "2. **Plot distribution comparisons** (boxplots, histograms) for rewards and risk-adjusted returns.\n",
    "3. **Run ablation**:\n",
    "   - Without regime augmentation\n",
    "   - Without reward normalization\n",
    "   - With simpler agents (e.g., MLP or LSTM)\n",
    "4. **Test in unseen market conditions** or during volatility spikes to check robustness.\n",
    "\n",
    "---\n",
    "\n",
    "_Logged using `ExperimentTracker` — Run Hash: `${experiment_tracker.run_hash}`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "from src.env.step_rewards import reward_sharpe,reward_sortino,reward_drawdown,reward_alpha,reward_cumulative,reward_calmar,reward_hybrid\n",
    "\n",
    "class BaseSequenceAwareTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Flexible RL Trading Env with windowed sequence obs (Transformer/LSTM/MLP-ready).\n",
    "    - Set `return_sequences=True` for (window_length, obs_dim) obs (for transformers).\n",
    "    - Set `return_sequences=False` for flat obs (classic RL, SB3 LSTM/MLP).\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, df, feature_cols=None, reward_fn=None, internal_features=None,\n",
    "        episode_length=100, transaction_cost=0.0001, seed=314, window_length=10, return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols or []\n",
    "        self.internal_features = internal_features or [\n",
    "            \"position\", \"holding_period\", \"cumulative_reward\", \"pct_time\",\n",
    "            \"drawdown\", \"rel_perf\", \"unrealized_pnl\", \"entry_price\", \"time_in_position\"\n",
    "        ]\n",
    "        self.obs_dim = len(self.feature_cols) + len(self.internal_features)\n",
    "        self.episode_length = episode_length +2\n",
    "        self.window_length = max(1, window_length)\n",
    "        self.return_sequences = return_sequences  # True: (window, obs_dim), False: flat\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.seed = seed\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        counts = df['symbol'].value_counts()\n",
    "        eligible = counts[counts >= episode_length].index\n",
    "        self.stocks = df[df['symbol'].isin(eligible)]['symbol'].unique()\n",
    "        self.episode_df = df.copy()\n",
    "        self.reward_fn = reward_fn or self.default_reward_fn\n",
    "\n",
    "        # Set observation space\n",
    "        if self.return_sequences:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length, self.obs_dim), dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length * self.obs_dim,), dtype=np.float32\n",
    "            )\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "    \n",
    "    def get_current_regime(self):\n",
    "        \"\"\"\n",
    "        Detects current regime based on recent return volatility and trend.\n",
    "        Returns:\n",
    "            0 = Bull, 1 = Bear, 2 = Sideways\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_length:\n",
    "            return 2  # Not enough data, assume sideways\n",
    "\n",
    "        # Use recent price changes to detect regime\n",
    "        returns = self.episode_df['return_1d'].iloc[self.current_step - self.window_length:self.current_step].values\n",
    "        mean_return = returns.mean()\n",
    "        std_return = returns.std()\n",
    "\n",
    "        # Thresholds can be tuned\n",
    "        if mean_return > 0.001 and std_return < 0.01:\n",
    "            return 0  # Bull\n",
    "        elif mean_return < -0.001 and std_return < 0.01:\n",
    "            return 1  # Bear\n",
    "        else:\n",
    "            return 2  # Sideways\n",
    "        \n",
    "    def default_reward_fn(self, position, price_change, **kwargs):\n",
    "        return position * price_change\n",
    "\n",
    "    def set_episode_sequence(self, sequence):\n",
    "        self.episode_sequence = sequence\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def generate_episode_sequences_v1(self, train_steps=10000):\n",
    "        dataset_length = len(self.df)\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        ticker = self.df['symbol'].unique()[0]\n",
    "        min_start = 0\n",
    "        max_start = dataset_length - self.episode_length - 2\n",
    "        for i in range(episodes):\n",
    "            episode_sequences.append((ticker, np.random.randint(0, max_start)))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def generate_episode_sequences(self, train_steps=10000):\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        for _ in range(episodes):\n",
    "            ticker = np.random.choice(self.stocks)\n",
    "            stock_df = self.df[self.df['symbol'] == ticker].reset_index(drop=True)\n",
    "            max_start = len(stock_df) - self.episode_length - 2\n",
    "            if max_start <= 0:\n",
    "                continue  # skip if not enough data\n",
    "            start = np.random.randint(0, max_start)\n",
    "            episode_sequences.append((ticker, start))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def reset(self, seed=None, options=None, start_index=None):\n",
    "        self.entry_step = None\n",
    "        self.unrealized_pnl = 0\n",
    "        self.relative_perf = 0\n",
    "        self.drawdown = 0\n",
    "        self.time_in_position = 0\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "            \n",
    "        symbol, start_idx = self.episode_sequence[self.episode_counter]\n",
    "        #print(symbol,start_idx,self.df['symbol'].unique())\n",
    "        symbol_df = self.df[self.df['symbol'] == symbol].reset_index(drop=True)\n",
    "        #print(len(symbol_df))\n",
    "        \n",
    "        if start_idx + self.episode_length > len(symbol_df):\n",
    "            print(f\"[WARN] Episode too short for {symbol} at {start_idx}, skipping...\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()  # tenta o próximo episódio\n",
    "\n",
    "        # ✅ Extração segura\n",
    "        #self.episode_df = symbol_df.iloc[start_idx : start_idx + self.episode_length].copy()\n",
    "        end = start_idx + self.episode_length + 1\n",
    "        if end > len(symbol_df):\n",
    "            print(f\"[WARN] Not enough data for {symbol} from {start_idx}, skipping.\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()\n",
    "        self.episode_df = symbol_df.iloc[start_idx:end].copy()\n",
    "        \n",
    "\n",
    "        # Move to next episode (with wrap-around)\n",
    "        self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "        \"\"\"\n",
    "        for _ in range(10):  # Try up to 10 times to get a valid episode\n",
    "            stock = self.stocks[0]\n",
    "            if hasattr(self, \"episode_sequence\"):\n",
    "                if self.episode_counter >= len(self.episode_sequence):\n",
    "                    self.episode_counter = 0\n",
    "                _, start = self.episode_sequence[self.episode_counter]\n",
    "                self.episode_counter += 1\n",
    "            else:\n",
    "                stock = np.random.choice(self.stocks)\n",
    "                stock_df = self.df[self.df['symbol'] == stock].reset_index(drop=True)\n",
    "                max_start = len(stock_df) - self.episode_length\n",
    "                if max_start <= 0:\n",
    "                    continue  # Try another stock\n",
    "                start = np.random.randint(0, max_start + 1)\n",
    "\n",
    "            self.stock = stock\n",
    "            stock_df = self.df[self.df['symbol'] == self.stock].reset_index(drop=True)\n",
    "            self.episode_df = stock_df.iloc[int(start):int(start) + int(self.episode_length + 2)].reset_index(drop=True)\n",
    "\n",
    "            if len(self.episode_df) >= self.window_length:\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"Failed to sample a valid episode with sufficient data.\")\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.entry_price = None\n",
    "        self.position = 0\n",
    "        self.holding_period = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.returns_history = []\n",
    "        self.reward_history = []\n",
    "        self.episode_pct_changes = self.episode_df['return_1d'].values\n",
    "        self.max_possible_reward = np.sum(np.abs(self.episode_pct_changes))\n",
    "        self.current_wealth = 1.0\n",
    "        self.peak_wealth = 1.0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Returns a rolling window of observations (2D or flattened)\n",
    "        obs_list = []\n",
    "        #for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "        #    idx = max(i, 0)  # pad with earliest available step\n",
    "        #    features = self.episode_df.iloc[idx][self.feature_cols].values.astype(np.float32)\n",
    "        for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "            if 0 <= i < len(self.episode_df):\n",
    "                features = self.episode_df.iloc[i][self.feature_cols].values.astype(np.float32)\n",
    "            else:\n",
    "                features = np.zeros(len(self.feature_cols), dtype=np.float32)  # zero padding\n",
    "            internal_state = {\n",
    "                \"position\": self.position,\n",
    "                \"holding_period\": self.holding_period,\n",
    "                \"cumulative_reward\": self.cumulative_reward,\n",
    "                \"pct_time\": self.current_step / self.episode_length,\n",
    "                \"drawdown\": self.drawdown,\n",
    "                \"rel_perf\": self.relative_perf,\n",
    "                \"unrealized_pnl\": self.unrealized_pnl,\n",
    "                \"entry_price\": self.entry_price if self.entry_price is not None else 0.0,\n",
    "                \"time_in_position\": self.time_in_position,\n",
    "            }\n",
    "            internal = np.array([internal_state[name] for name in self.internal_features], dtype=np.float32)\n",
    "            obs = np.concatenate([features, internal])\n",
    "            obs_list.append(obs)\n",
    "        obs_window = np.stack(obs_list)  # shape: (window_length, obs_dim)\n",
    "        if self.return_sequences:\n",
    "            return obs_window  # shape: (window_length, obs_dim)\n",
    "        else:\n",
    "            return obs_window.flatten()  # shape: (window_length * obs_dim,)\n",
    "        \n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        #print(self.current_step,self.episode_length,len(self.episode_df))\n",
    "        done = self.current_step >= self.episode_length - 1\n",
    "        current_row = self.episode_df.iloc[self.current_step]\n",
    "\n",
    "        # Protege contra acesso fora dos limites\n",
    "        if self.current_step + 1 < len(self.episode_df):\n",
    "            next_row = self.episode_df.iloc[self.current_step + 1]\n",
    "        else:\n",
    "            next_row = current_row.copy()  # fallback seguro\n",
    "\n",
    "        price_change = next_row['return_1d']\n",
    "        prev_position = self.position\n",
    "        reward = 0\n",
    "        cost = 0\n",
    "\n",
    "        self.action_counts[action] += 1\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if self.position != 1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = 1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if self.position != -1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = -1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        if self.position != 0:\n",
    "            self.holding_period += 1\n",
    "\n",
    "        step_return = self.position * price_change\n",
    "        self.returns_history.append(step_return)\n",
    "        self.current_wealth *= (1 + step_return)\n",
    "        if self.current_wealth > self.peak_wealth:\n",
    "            self.peak_wealth = self.current_wealth\n",
    "        self.drawdown = 1 - self.current_wealth / self.peak_wealth\n",
    "\n",
    "        if self.position != 0 and self.entry_price is not None:\n",
    "            current_price = next_row['close']\n",
    "            self.unrealized_pnl = (current_price - self.entry_price) * self.position / self.entry_price\n",
    "            self.time_in_position = self.current_step - self.entry_step\n",
    "        else:\n",
    "            self.unrealized_pnl = 0\n",
    "            self.time_in_position = 0\n",
    "\n",
    "        if 'market_return_1d' in self.episode_df.columns:\n",
    "            self.relative_perf = price_change - next_row['market_return_1d']\n",
    "        else:\n",
    "            self.relative_perf = 0\n",
    "\n",
    "        reward = self.reward_fn(\n",
    "            position=self.position,\n",
    "            price_change=price_change,\n",
    "            prev_position=prev_position,\n",
    "            env=self\n",
    "        )\n",
    "        reward -= cost\n",
    "        self.reward_history.append(reward)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        self.current_step += 1\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        info[\"regime\"] = self.get_current_regime()\n",
    "\n",
    "        # Calcula métricas no final do episódio\n",
    "        if done:\n",
    "            returns = np.array(self.returns_history)\n",
    "            mean = np.median(returns) if len(returns) > 0 else np.nan\n",
    "            std = returns.std() if len(returns) > 1 else np.nan\n",
    "            downside = returns[returns < 0]\n",
    "            downside_std = downside.std() if len(downside) > 1 else np.nan\n",
    "\n",
    "            sharpe = mean / std if (std is not None and std > 0 and not np.isnan(std)) else np.nan\n",
    "            sortino = mean / downside_std if (downside_std is not None and downside_std > 0 and not np.isnan(downside_std)) else np.nan\n",
    "\n",
    "            wealth_curve = np.cumprod(1 + returns) if len(returns) > 0 else np.array([])\n",
    "            peak_wealth = np.maximum.accumulate(wealth_curve) if len(wealth_curve) > 0 else np.array([])\n",
    "            drawdowns = (wealth_curve - peak_wealth) / (peak_wealth + 1e-8) if len(wealth_curve) > 0 else np.array([])\n",
    "            max_drawdown = np.abs(drawdowns.min()) if len(drawdowns) > 0 else np.nan\n",
    "            calmar = ((wealth_curve[-1] - 1) / max_drawdown) if (len(wealth_curve) > 0 and max_drawdown and not np.isnan(max_drawdown) and max_drawdown > 0) else np.nan\n",
    "            cum_return = wealth_curve[-1] - 1 if len(wealth_curve) > 0 else np.nan\n",
    "            final_wealth = wealth_curve[-1] if len(wealth_curve) > 0 else np.nan\n",
    "\n",
    "            # Trade-level metrics\n",
    "            trades = []\n",
    "            trade_profits = []\n",
    "            prev = 0\n",
    "            for i, ret in enumerate(returns):\n",
    "                if prev == 0 and ret != 0:\n",
    "                    entry_idx = i\n",
    "                    entry_dir = np.sign(ret)\n",
    "                elif prev != 0 and (ret == 0 or np.sign(ret) != np.sign(prev)):\n",
    "                    if 'entry_idx' in locals():\n",
    "                        trade = returns[entry_idx:i+1]\n",
    "                        trade_profits.append(np.sum(trade))\n",
    "                        del entry_idx\n",
    "                prev = ret\n",
    "            win_rate = np.median(np.array(trade_profits) > 0) if trade_profits else np.nan\n",
    "\n",
    "            if 'market_return_1d' in self.episode_df.columns:\n",
    "                market_returns = self.episode_df['market_return_1d'].values[1:self.episode_length]\n",
    "                market_wealth_curve = np.cumprod(1 + market_returns) if len(market_returns) > 0 else np.array([])\n",
    "                market_cum_return = market_wealth_curve[-1] - 1 if len(market_wealth_curve) > 0 else np.nan\n",
    "                alpha = cum_return - market_cum_return if cum_return is not None and not np.isnan(cum_return) and market_cum_return is not None and not np.isnan(market_cum_return) else np.nan\n",
    "            else:\n",
    "                alpha = np.nan\n",
    "\n",
    "            info.update({\n",
    "                \"episode_sharpe\": sharpe,\n",
    "                \"episode_sortino\": sortino,\n",
    "                \"episode_total_reward\": np.sum(self.reward_history) if len(self.reward_history) > 0 else np.nan,\n",
    "                \"cumulative_return\": cum_return,\n",
    "                \"calmar\": calmar,\n",
    "                \"max_drawdown\": max_drawdown,\n",
    "                \"win_rate\": win_rate,\n",
    "                \"alpha\": alpha,\n",
    "                \"returns\": returns,\n",
    "                \"market_returns\": market_returns if 'market_returns' in locals() else [],\n",
    "                \"downside\": downside,\n",
    "                \"regime\": self.get_current_regime(),\n",
    "                \"final_wealth\": final_wealth,\n",
    "                \"action_hold_count\": self.action_counts[0],\n",
    "                \"action_buy_count\": self.action_counts[1],\n",
    "                \"action_sell_count\": self.action_counts[2]\n",
    "            })\n",
    "\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step} | Pos: {self.position} | Hold: {self.holding_period} | CumRew: {self.cumulative_reward:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "class SequenceAwareSharpeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sharpe, **kwargs)\n",
    "\n",
    "class SequenceAwareSortinoTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sortino, **kwargs)\n",
    "\n",
    "class SequenceAwareAlphaTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_alpha, **kwargs)\n",
    "\n",
    "class SequenceAwareDrawdownTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_drawdown, **kwargs)\n",
    "\n",
    "class SequenceAwareCumulativeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_cumulative, **kwargs)\n",
    "\n",
    "class SequenceAwareCalmarTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_calmar, **kwargs)\n",
    "\n",
    "class SequenceAwareHybridTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_hybrid, **kwargs)\n",
    "\n",
    "class SequenceAwareBaselineTradingAgent:\n",
    "    def __init__(self,df,feature_cols=[],\n",
    "            episode_length=100, seed=314,set_episode_sequence=[]):\n",
    "    \n",
    "        self.env = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols,\n",
    "            episode_length=episode_length, seed=seed)\n",
    "        self.env.set_episode_sequence(set_episode_sequence)\n",
    "        \n",
    "    def predict(self,obs,*args,**kwargs):\n",
    "        #print(self.env.stocks,'xxxxxxxxxxx')\n",
    "        return self.env.action_space.sample(),{}\n",
    "    \n",
    "    def set_episode_sequence(self,seq):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # should return 0, 1, or 2\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design_v2\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "WINDOW_LENGTH = 10  \n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 800,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"project_name\":EXPERIENCE_NAME,\n",
    "    \"environment\": \"SequenceAwareCumulativeTradingEnv\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data ==================================\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# Experiment tracker ========================= \n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "experiment_tracker.set_hash(CONFIG)\n",
    "\n",
    "# Files ======================================\n",
    "checkpoint_path = \"data/checkpoint\" \n",
    "checkpoint_name = \"-8377624099423380081\"#experiment_tracker.run_hash\n",
    "checkpoint_preffix = f\"{checkpoint_name}--checkpoint\"\n",
    "checkpoint_best_model=f\"{checkpoint_path}/{checkpoint_name}--best_model\"\n",
    "log_path=\"data/logs\"\n",
    "save_path= f\"{checkpoint_path}/{checkpoint_name}--final\"\n",
    "print(checkpoint_name)\n",
    "#data/checkpoint/-8377624099423380081--final\n",
    "#data/checkpoint/-3848392742194634112--best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 100\n",
    "MAX_LENGTH = 200\n",
    "SAVE_FREQ=10000\n",
    "EVAL_FREQ=5000\n",
    "TOTAL_TIMESTEPS=200000\n",
    "#TOTAL_TIMESTEPS=1000\n",
    "EPISODES_PER_UPDATE = 8          # ~how many episodes before PPO updates\n",
    "EPISODES_PER_BATCH = 1           # number of full episodes per batch\n",
    "\n",
    "# === Auto-derive PPO settings ===\n",
    "N_STEPS = EPISODE_LENGTH * EPISODES_PER_UPDATE\n",
    "BATCH_SIZE = EPISODE_LENGTH * EPISODES_PER_BATCH\n",
    "\n",
    "ENV_CLASS = SequenceAwareCumulativeTradingEnv\n",
    "\n",
    "n = Notify(experiment_tracker.project)\n",
    "n.info('START')\n",
    "print(N_STEPS,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e51121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "# Causal Mask Function ============================\n",
    "def generate_causal_mask(seq_len):\n",
    "    return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# Transformer Feature Extractor ===================\n",
    "class TransformerFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=64, n_heads=4, n_layers=2, max_len=MAX_LENGTH):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "        input_dim = observation_space.shape[-1]\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward_v1(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "        return x[:, -1]  # return the last token output\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, seq_len, input_dim)\n",
    "        #print(\">>> [Transformer] Input shape:\", obs.shape)\n",
    "\n",
    "        x = self.input_proj(obs)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len]\n",
    "\n",
    "        causal_mask = generate_causal_mask(seq_len).to(x.device)\n",
    "        x = self.transformer(x, mask=causal_mask)\n",
    "\n",
    "        pooled_output = x[:, -1]\n",
    "        #print(\">>> [Transformer] Pooled output mean/std:\", pooled_output.mean().item(), pooled_output.std().item())\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "# Transformer Policy ===================================\n",
    "class TransformerPolicy(RecurrentActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs,\n",
    "                         features_extractor_class=TransformerFeatureExtractor,\n",
    "                         features_extractor_kwargs=dict(\n",
    "                             d_model=64, n_heads=4, n_layers=2, max_len=32\n",
    "                         ))\n",
    "        #self._build(self.lr_schedule)\n",
    "\n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)\n",
    "\n",
    "class PerEpisodeRewardNormalizer(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_rewards = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean = np.mean(self.episode_rewards)\n",
    "        std = np.std(self.episode_rewards) + 1e-8\n",
    "        norm_reward = (reward - mean) / std\n",
    "        return obs, norm_reward, done, truncated, info\n",
    "# Training =============================================================\n",
    "train_df = ohlcv_df[(ohlcv_df['date']>=\"2023-01-01\") & (ohlcv_df['date']<\"2023-07-01\")]\n",
    "test_df = ohlcv_df[(ohlcv_df['date']>=\"2023-07-01\") & (ohlcv_df['date']<\"2024-01-01\")]\n",
    "train_df = train_df[train_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "test_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "# Train on few episodes to prove a point only\n",
    "train_seq = train_env.generate_episode_sequences(TOTAL_TIMESTEPS)\n",
    "_test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS))\n",
    "episodes = _test_seq\n",
    "unique_episodes = {}\n",
    "for ticker, start in episodes:\n",
    "    if ticker not in unique_episodes:\n",
    "        unique_episodes[ticker] = start\n",
    "# Convert back to a list of tuples\n",
    "test_seq = [(ticker, start) for ticker, start in unique_episodes.items()]\n",
    "\n",
    "print(f\"Training on {len(train_seq)} different episodes accross the top 2 stocks for each sector\")\n",
    "print(f\"Testing on {len(test_seq)} different episodes accross the top 2 stocks for each sector\")\n",
    "def train_agent():\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_env =ENV_CLASS(train_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    train_env.set_episode_sequence(train_seq)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    \n",
    "    train_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(train_env))\n",
    "    eval_env = PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env))\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=SAVE_FREQ, save_path=checkpoint_path, name_prefix=checkpoint_preffix\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, best_model_save_path=checkpoint_best_model,\n",
    "        log_path=log_path, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    model = RecurrentPPO(\n",
    "        policy=TransformerPolicy,\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        #tensorboard_log=\"./tensorboard_logs\",\n",
    "        n_steps=N_STEPS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.005,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs=dict(share_features_extractor=True)\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, eval_callback])\n",
    "    model.save(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38331a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Notify(experiment_tracker.project)\n",
    "n.info('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8550deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketVersusWalletHistoryTracker:\n",
    "    def __init__(self, initial_wallet=1.0):\n",
    "        self.wallet_value = initial_wallet\n",
    "        self.prev_wallet_value = initial_wallet\n",
    "        self.wallet_locked = False\n",
    "        self.buy_price = None\n",
    "        self.market_entry_price = None\n",
    "        self.last_price = None\n",
    "        self.has_opened_position = False  # NEW: ensure proper update after first buy\n",
    "\n",
    "        self.wallet_history = []\n",
    "        self.market_history = []\n",
    "        self.price_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset(self, initial_price):\n",
    "        self.__init__(initial_wallet=1.0)\n",
    "        self.market_entry_price = initial_price\n",
    "        self.last_price = initial_price\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.market_history.append(1.0)\n",
    "        self.price_history.append(initial_price)\n",
    "        self.action_history.append(0)\n",
    "\n",
    "    def step(self, action, current_price):\n",
    "        self.price_history.append(current_price)\n",
    "        agent_action = 0\n",
    "\n",
    "        # === 1. Update market benchmark ===\n",
    "        market_perf = current_price / self.market_entry_price\n",
    "        self.market_history.append(market_perf)\n",
    "\n",
    "        # === 2. Update wallet value ===\n",
    "        if self.wallet_locked and self.has_opened_position:\n",
    "            self.wallet_value *= current_price / self.last_price\n",
    "\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.prev_wallet_value = self.wallet_value\n",
    "        self.last_price = current_price  # must be set after wallet update!\n",
    "\n",
    "        # === 3. Process Action ===\n",
    "        if action == 1 and not self.wallet_locked:\n",
    "            self.buy_price = current_price\n",
    "            self.wallet_locked = True\n",
    "            self.has_opened_position = True\n",
    "            agent_action = 1\n",
    "\n",
    "        elif action == 2 and self.wallet_locked:\n",
    "            self.wallet_locked = False\n",
    "            self.buy_price = None\n",
    "            self.has_opened_position = False\n",
    "            agent_action = 2\n",
    "\n",
    "        self.action_history.append(agent_action)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"wallet_history\": self.wallet_history,\n",
    "            \"market_history\": self.market_history,\n",
    "            \"market_price_history\": self.price_history,\n",
    "            \"performed_action_history\": self.action_history\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "#from src.env.base_timeseries_trading_env import SequenceAwareCumulativeTradingEnv\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "#from src.env.base_timeseries_trading_env import RegimeAugmentingWrapper\n",
    "\n",
    "ENV_CLASS=SequenceAwareCumulativeTradingEnv\n",
    "# === Config ===\n",
    "N_EVAL_EPISODES = 5\n",
    "MODEL_PATH = save_path\n",
    "\n",
    "# === Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "test_df = ohlcv_df[(ohlcv_df['date'] >= \"2023-07-01\") & (ohlcv_df['date'] < \"2024-01-01\")]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "\n",
    "# === Evaluation Logic ===\n",
    "def evaluate_agent(agent, env, n_episodes=22):\n",
    "    episode_metrics = []\n",
    "    episode_infos = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating Agent\"):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        # === Tracker setup ===\n",
    "        tracker = MarketVersusWalletHistoryTracker()\n",
    "        initial_price = env.env.env.episode_df.iloc[0]['close']\n",
    "        tracker.reset(initial_price)\n",
    "\n",
    "        while not done:\n",
    "            action, state = agent.predict(obs, state=state, deterministic=True)\n",
    "            action = int(action)\n",
    "            current_price = env.env.env.episode_df.iloc[env.env.env.current_step]['close']\n",
    "\n",
    "            # Step the wallet tracker\n",
    "            tracker.step(action, current_price)\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "\n",
    "        # === Episode summary ===\n",
    "        _env = env.env.env\n",
    "        agent_wealth = infos[-1].get(\"final_wealth\", np.nan)\n",
    "        market_wealth = np.prod(1 + _env.episode_df['market_return_1d'].values)\n",
    "        alpha = agent_wealth - market_wealth\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": agent_wealth,\n",
    "            \"market_wealth\": market_wealth,\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "            \"alpha\": alpha,\n",
    "            \"episode_id\": _env.episode_counter,\n",
    "            \"regime\": infos[-1].get(\"regime\", np.nan)\n",
    "        }\n",
    "\n",
    "        tracker_data = tracker.export()\n",
    "        info[\"ticker\"] = _env.episode_df.iloc[0]['symbol']\n",
    "        info[\"wallet_history\"] = tracker_data[\"wallet_history\"]\n",
    "        info[\"market_history\"] = tracker_data[\"market_history\"]\n",
    "        info[\"market_price_history\"] = tracker_data[\"market_price_history\"]\n",
    "        info[\"performed_action_history\"] = tracker_data[\"performed_action_history\"]\n",
    "        episode_infos.append(info)\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics), episode_infos\n",
    "\n",
    "def evaluate_random_agent( env, n_episodes=22):\n",
    "    episode_metrics = []\n",
    "    episode_infos = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating Agent\"):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        # === Tracker setup ===\n",
    "        tracker = MarketVersusWalletHistoryTracker()\n",
    "        initial_price = env.env.env.episode_df.iloc[0]['close']\n",
    "        tracker.reset(initial_price)\n",
    "        initial_symbol =env.env.env.episode_df.iloc[0]['symbol']\n",
    "        #print(initial_symbol,env.env.env.episode_df.iloc[0]['date'],len(env.env.env.episode_df),env.env.env.episode_counter,env.env.env.episode_sequence)\n",
    "        while not done:\n",
    "            action=  env.action_space.sample()\n",
    "            action = int(action)\n",
    "            current_price = env.env.env.episode_df.iloc[env.env.env.current_step]['close']\n",
    "            current_symbol = env.env.env.episode_df.iloc[env.env.env.current_step]['symbol']\n",
    "            if(current_symbol != initial_symbol):\n",
    "                print('EPISODE SWITCHED', initial_symbol,current_symbol)\n",
    "                initial_symbol = current_symbol\n",
    "            # Step the wallet tracker\n",
    "            tracker.step(action, current_price)\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "        #print(current_symbol)\n",
    "        # === Episode summary ===\n",
    "        _env = env.env.env\n",
    "        agent_wealth = infos[-1].get(\"final_wealth\", np.nan)\n",
    "        market_wealth = np.prod(1 + _env.episode_df['market_return_1d'].values)\n",
    "        alpha = agent_wealth - market_wealth\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": agent_wealth,\n",
    "            \"market_wealth\": market_wealth,\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "            \"alpha\": alpha,\n",
    "            \"episode_id\": _env.episode_counter,\n",
    "            \"regime\": infos[-1].get(\"regime\", np.nan)\n",
    "        }\n",
    "\n",
    "        tracker_data = tracker.export()\n",
    "        info[\"ticker\"] = _env.episode_df.iloc[0]['symbol']\n",
    "        info[\"wallet_history\"] = tracker_data[\"wallet_history\"]\n",
    "        info[\"market_history\"] = tracker_data[\"market_history\"]\n",
    "        info[\"market_price_history\"] = tracker_data[\"market_price_history\"]\n",
    "        info[\"performed_action_history\"] = tracker_data[\"performed_action_history\"]\n",
    "        episode_infos.append(info)\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics), episode_infos\n",
    "\n",
    "\n",
    "# === Run Evaluation ===\n",
    "\n",
    "model = RecurrentPPO.load(MODEL_PATH)\n",
    "\n",
    "def make_test_env():\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    return PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env)) #RegimeAugmentingWrapper(ENV_CLASS(test_df, episode_length=EPISODE_LENGTH,feature_cols=FEATURE_COLS))\n",
    "\n",
    "\n",
    "ppo_agent_df, ppo_agent_infos = evaluate_agent(model, make_test_env(), n_episodes=22)\n",
    "random_agent_df, random_agent_infos = evaluate_random_agent(make_test_env(), n_episodes=22)\n",
    "\n",
    "ppo_agent_df[\"agent\"] = \"recurrent_ppo\"\n",
    "random_agent_df[\"agent\"] = \"random\"\n",
    "results_df = pd.concat([ppo_agent_df, random_agent_df])\n",
    "\n",
    "\n",
    "# === Plotting ===\n",
    "melted = results_df.melt(id_vars=\"agent\", var_name=\"metric\", value_name=\"value\")\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(data=melted, x=\"metric\", y=\"value\", hue=\"agent\")\n",
    "plt.title(\"Agent Performance Comparison (Random vs Recurrent PPO)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Statistical Tests ===\n",
    "comparison_results = []\n",
    "\n",
    "for metric in ppo_agent_df.columns[:-1]:  # exclude 'agent'\n",
    "    a = ppo_agent_df[metric].dropna()\n",
    "    b = random_agent_df[metric].dropna()\n",
    "\n",
    "    # Skip if either is empty\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        print(f\"Skipping metric {metric}: empty values\")\n",
    "        continue\n",
    "\n",
    "    t_stat, p_val_t = ttest_ind(a, b)\n",
    "    u_stat, p_val_u = mannwhitneyu(a, b, alternative='two-sided')\n",
    "    comparison_results.append({\n",
    "        \"metric\": metric,\n",
    "        \"t-test p-value\": p_val_t,\n",
    "        \"mann-whitney p-value\": p_val_u\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ab0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_test_env().env.env.episode_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bf41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=ppo_agent_df, x='market_wealth', y='alpha', hue='regime', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e283144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.scatterplot(data=ppo_agent_df, x='market_wealth', y='alpha', hue='agent')\n",
    "plt.axhline(0, linestyle='--', color='red', label='Alpha = 0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent_df['alpha_ratio'] = ppo_agent_df['alpha'] / np.abs(ppo_agent_df['market_wealth'] + 1e-8)\n",
    "sns.boxplot(data=ppo_agent_df, x='agent', y='alpha_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09420a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episode_complete_results = []\n",
    "for i in range(len(ppo_agent_infos)):# or select based on alpha, reward, etc.\n",
    "#for i in range(5):# or select based on alpha, reward, etc.\n",
    "    info = ppo_agent_infos[i]\n",
    "    info_ = random_agent_infos[i]\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(info['market_history'], label='Market')\n",
    "    plt.plot(info['wallet_history'], label='Wallet (Agent)')\n",
    "    plt.plot(info_['wallet_history'], label='Wallet (Random)')\n",
    "    plt.title(f\"Episode {info.get('episode_id')} - {info.get('ticker')}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    episode_complete_results.append({\n",
    "        \"agent_wallet\":info['wallet_history'][-1],\n",
    "        \"random_wallet\":info_['wallet_history'][-1],\n",
    "        \"market_wallet\":info_['market_history'][-1],\n",
    "                                   })\n",
    "episode_complete_results_df = pd.DataFrame(episode_complete_results)\n",
    "episode_complete_results_df['agent>random']=episode_complete_results_df['agent_wallet']>episode_complete_results_df['random_wallet']\n",
    "episode_complete_results_df['agent>market']=episode_complete_results_df['agent_wallet']>episode_complete_results_df['market_wallet']\n",
    "print(\"AGENT > RANDOM value counts\",episode_complete_results_df['agent>random'].value_counts())\n",
    "print(\"AGENT > MARKET value counts\",episode_complete_results_df['agent>market'].value_counts())\n",
    "episode_complete_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3686c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test\n",
    "\n",
    "n_wins = episode_complete_results_df['agent>random'].sum()\n",
    "n_trials = len(episode_complete_results_df)\n",
    "\n",
    "p_val = binom_test(n_wins, n_trials, p=0.5, alternative='greater')\n",
    "print(f\"PPO wins over Random: {n_wins}/{n_trials}, binomial p-value = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c36ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p_df = pd.DataFrame({\"wallet\":ppo_agent_infos[0][\"wallet_history\"],\"price\":ppo_agent_infos[0][\"market_price_history\"],\"performed_action\":ppo_agent_infos[0][\"performed_action_history\"]})\n",
    "for i in range(len(w_p_df)):\n",
    "    print({\"i\":i,\"wallet\":w_p_df.iloc[i][\"wallet\"], \"price\":w_p_df.iloc[i][\"price\"], \"performed_action\":w_p_df.iloc[i][\"performed_action\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016aa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppo_agent_infos[0][\"performed_action_history\"]),len(ppo_agent_infos[0][\"market_price_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d4010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
