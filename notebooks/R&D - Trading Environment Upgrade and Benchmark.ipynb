{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d0c662",
   "metadata": {},
   "source": [
    "\n",
    "## üìò **Notebook Summary**: Environment Upgrade and Benchmark\n",
    "\n",
    "### üéØ **Objective**\n",
    "\n",
    "To evaluate whether the upgraded environment `PositionTradingEnvV1` (which includes internal features like position, holding time, PnL, etc.) improves agent performance and learnability compared to the original `PositionTradingEnv`.\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è **Structure of the Notebook**\n",
    "\n",
    "#### 1. **Imports and Setup**\n",
    "\n",
    "* Essential packages are imported: `stable_baselines3`, `gym`, `numpy`, `pandas`, etc.\n",
    "* A2C and PPO agents are prepared for training.\n",
    "\n",
    "#### 2. **Environment Definitions**\n",
    "\n",
    "* You define `PositionTradingEnv` and the upgraded version `PositionTradingEnvV1`.\n",
    "* `PositionTradingEnvV1` includes richer observations:\n",
    "\n",
    "  * `position`, `time_in_position`, `unrealized_pnl`, `price_vs_entry`, `rolling_return`\n",
    "  * One-hot encoding for weekday\n",
    "  * One-hot action history (3 last actions)\n",
    "\n",
    "#### 3. **Training Procedure**\n",
    "\n",
    "* Agents are trained on both v0 and v1 environments.\n",
    "* You use `run_learning_evaluation()` which:\n",
    "\n",
    "  * Trains each agent on a sample episode (fixed seed)\n",
    "  * Evaluates on both training and test episodes\n",
    "  * Compares PPO and A2C to random policy\n",
    "  * Logs detailed metrics: reward, advantage, transferability, etc.\n",
    "\n",
    "#### 4. **Metric Aggregation**\n",
    "\n",
    "* Results are stored and aggregated in a DataFrame\n",
    "* Metrics include:\n",
    "\n",
    "  * `score_train`, `score_test` (normalized episode score)\n",
    "  * `advantage_train`, `advantage_test` (agent - random)\n",
    "  * `transfer_delta` (test - train)\n",
    "  * `success_trades`, `action ratios`, Sharpe, Sortino, etc.\n",
    "\n",
    "#### 5. **Visualization / Tables**\n",
    "\n",
    "* Grouped comparison tables are printed\n",
    "* Showing `agent_name` √ó `env_version` (v0, v1) across all metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Key Results**\n",
    "\n",
    "| Metric              | A2C v1 vs v0   | PPO v1 vs v0 | Conclusion                                    |\n",
    "| ------------------- | -------------- | ------------ | --------------------------------------------- |\n",
    "| `score_train`       | +0.22          | **+0.49** ‚úÖ  | Both improved significantly on training       |\n",
    "| `advantage_train`   | +0.22          | **+0.48** ‚úÖ  | Better advantage signal learning              |\n",
    "| `score_test`        | ‚àí0.01          | **‚àí0.53** ‚ùå  | PPO overfits? Generalization worsened         |\n",
    "| `advantage_test`    | ‚àí0.58          | **+0.14** ‚úÖ  | PPO still beats random more often             |\n",
    "| `transfer_delta`    | ‚àí0.21          | **‚àí1.02** ‚ùå  | PPO v1 transfers worse ‚Äî needs regularization |\n",
    "| `action_hold_ratio` | \\~ +0.02       | +0.02        | Agents learned to hold longer                 |\n",
    "| `success_trades`    | Roughly stable | Slightly ‚Üì   | Behaviorally still consistent                 |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Conclusion**\n",
    "\n",
    "* `PositionTradingEnvV1` **improves learning** and agent awareness during training.\n",
    "* PPO agents **learn to trade better** in v1, with higher training scores and advantage.\n",
    "* However, **generalization and transferability dropped**, suggesting:\n",
    "\n",
    "  * Overfitting to internal agent features\n",
    "  * Lack of training diversity\n",
    "  * No regularization (yet)\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **Next Steps (Suggested)**\n",
    "\n",
    "1. Add **dropout or entropy tuning** to avoid overfitting in PPO.\n",
    "2. Add **training diversity** (tickers/months) or curriculum.\n",
    "3. Consider **meta-learning memory** (comparing past episode conditions).\n",
    "4. Benchmark with larger set of episodes and visualize **success vs failure cases**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "from src.defaults import RANDOM_SEEDS\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"trading_environment_development\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 100_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = RANDOM_SEEDS\n",
    "MARKET_FEATURES = ['close']\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "MARKET_FEATURES.sort()\n",
    "SEEDS.sort()\n",
    "\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()\n",
    "\n",
    "NOTIFICATION = Notify(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d2e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c261190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "from stable_baselines3 import PPO,A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv\n",
    "from data import extract_meta_features\n",
    "\n",
    "def compute_additional_metrics(env):\n",
    "    if hasattr(env, \"env\"):  # unwrap Monitor\n",
    "        env = env.env\n",
    "    values = np.array(env.values)\n",
    "    rewards = np.array(env.rewards)\n",
    "    actions = np.array(env.actions)\n",
    "\n",
    "    returns = pd.Series(values).pct_change().dropna()\n",
    "    volatility = returns.std()\n",
    "    entropy = -np.sum(np.bincount(actions, minlength=2)/len(actions) * np.log2(np.bincount(actions, minlength=2)/len(actions) + 1e-9))\n",
    "    max_drawdown = (values / np.maximum.accumulate(values)).min() - 1\n",
    "    sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "    sortino = returns.mean() / (returns[returns < 0].std() + 1e-9) * np.sqrt(252)\n",
    "    calmar = returns.mean() / abs(max_drawdown + 1e-9)\n",
    "    success_trades = np.sum((np.diff(values) > 0) & (actions[1:] == 1)) + np.sum((np.diff(values) < 0) & (actions[1:] == 0))\n",
    "\n",
    "    return {\n",
    "        \"volatility\": volatility,\n",
    "        \"entropy\": entropy,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"calmar\": calmar,\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(actions == 0),\n",
    "        \"action_long_ratio\": np.mean(actions == 1)\n",
    "    }\n",
    "\n",
    "def formalized_transferability_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    env_cls: Callable = PositionTradingEnv,\n",
    "    benchmark_path: str = \"data/experiments/learnability_test/benchmark_episodes.json\",\n",
    "    result_path: str = \"data/experiments/learnability_test/meta_df_transfer.csv\",\n",
    "    timesteps: int = 10_000,\n",
    "    n_timesteps: int = 60,\n",
    "    lookback: int = 0,\n",
    "    seeds: list = [42, 52, 62],\n",
    "    checkpoint_dir: str = \"data/experiments/learnability_test/checkpoints\",\n",
    "    agent_cls: Callable = PPO,\n",
    "    \n",
    "    agent_config: dict = None,\n",
    "    env_config: dict = None\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    agent_name: str = agent_cls.__name__\n",
    "    env_version: str = f\"v{env_cls.__version__}\"\n",
    "        \n",
    "    def generate_config_hash(config):\n",
    "        raw = json.dumps(config, sort_keys=True)\n",
    "        return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "    def save_model(model, config_full, config_hash):\n",
    "        path = os.path.join(checkpoint_dir, f\"agent_{config_hash}.zip\")\n",
    "        model.save(path)\n",
    "        with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "            json.dump(config_full, f, indent=2)\n",
    "\n",
    "    print(\"[INFO] Loading benchmark episodes...\")\n",
    "    with open(benchmark_path) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "    \n",
    "    meta_records = []\n",
    "    df_ticker = df[df['symbol'] == ticker].reset_index(drop=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        existing = pd.read_csv(result_path)\n",
    "        #seen_hashes = set(existing['config_hash'].unique())\n",
    "        seen_hashes = set(zip(existing['config_hash'], existing['agent_name'], existing['seed']))\n",
    "    else:\n",
    "        seen_hashes = set()\n",
    "  \n",
    "    for seed in seeds:\n",
    "        for start_idx in benchmark_episodes:\n",
    "            \n",
    "            test_idx = start_idx + n_timesteps\n",
    "            if test_idx + n_timesteps >= len(df_ticker):\n",
    "                print(\"[WARN] Skipping episode ‚Äî test idx out of range\")\n",
    "                continue\n",
    "\n",
    "            config = {\n",
    "                \"ticker\": ticker,\n",
    "                \"train_idx\": int(start_idx),\n",
    "                \"test_idx\": int(test_idx),\n",
    "                \"timesteps\": timesteps,\n",
    "                \"episode_steps\":n_timesteps,\n",
    "                #\"seed\": seed,\n",
    "                \"env_version\": env_version,\n",
    "                \"env_config\": env_config,\n",
    "                #\"agent_name\": agent_name,\n",
    "                \"agent_config\": agent_config,\n",
    "            }\n",
    "            config_hash = generate_config_hash(config)\n",
    "            #if config_hash in seen_hashes:\n",
    "            if (config_hash, agent_name, seed) in seen_hashes:\n",
    "                print(f\"[INFO] Skipping previously completed run: {config_hash} for {agent_name} and seed = {seed}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Transferability: seed={seed}, start_idx={start_idx}, config_hash={config_hash}\")\n",
    "\n",
    " \n",
    "            env_train = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=start_idx, **(env_config or {})))\n",
    "            model = agent_cls(\"MlpPolicy\", env_train, verbose=0, seed=seed, **(agent_config or {}))\n",
    "            model.learn(total_timesteps=timesteps)\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, score_train = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                score_train += reward\n",
    "\n",
    "            obs, _ = env_train.reset()\n",
    "            done, rand_train = False, 0\n",
    "            while not done:\n",
    "                action = env_train.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_train.step(action)\n",
    "                rand_train += reward\n",
    "\n",
    "            env_test = Monitor(env_cls(df_ticker, ticker=ticker, seed=seed, start_idx=test_idx, **(env_config or {})))\n",
    "            obs, _ = env_test.reset()\n",
    "            done, score_test = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                score_test += reward\n",
    "\n",
    "            obs, _ = env_test.reset()\n",
    "            done, rand_test = False, 0\n",
    "            while not done:\n",
    "                action = env_test.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_test.step(action)\n",
    "                rand_test += reward\n",
    "\n",
    "            advantage_train = score_train - rand_train\n",
    "            advantage_test = score_test - rand_test\n",
    "            transfer_delta = score_test - score_train\n",
    "\n",
    "            save_model(model, config, config_hash)\n",
    "\n",
    "            meta = extract_meta_features(df_ticker.iloc[start_idx:start_idx + n_timesteps])\n",
    "            diagnostics = compute_additional_metrics(env_test)\n",
    "\n",
    "            meta.update({\n",
    "                \"config_hash\": config_hash,\n",
    "                \"env_version\": env_version,\n",
    "                \"agent_name\": agent_name,\n",
    "                \"score_train\": score_train,\n",
    "                \"score_test\": score_test,\n",
    "                \"advantage_train\": advantage_train,\n",
    "                \"advantage_test\": advantage_test,\n",
    "                \"transfer_delta\": transfer_delta,\n",
    "                \"transfer_success\": int(transfer_delta > 0),\n",
    "                \"ticker\": ticker,\n",
    "                \"config\":json.dumps(config),\n",
    "                \"seed\": seed,\n",
    "                \"ticker\": ticker,\n",
    "                \"train_idx\": int(start_idx),\n",
    "                \"test_idx\": int(test_idx),\n",
    "                \"timesteps\": timesteps,\n",
    "                \"episode_steps\":n_timesteps,\n",
    "                \"seed\": seed,\n",
    "                **diagnostics\n",
    "            })\n",
    "            meta_records.append(meta)\n",
    "\n",
    "    result_df = pd.DataFrame(meta_records)\n",
    "    if os.path.exists(result_path):\n",
    "        result_df = pd.concat([pd.read_csv(result_path), result_df], ignore_index=True)\n",
    "    result_df.to_csv(result_path, index=False)\n",
    "    print(\"[INFO] Transferability test complete. Results saved to:\", result_path)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e65932",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epis√≥dios de benchmark salvos em: data/experiments/trading_environment_development/benchmark_episodes.json\n",
      "v2\n",
      "[INFO] Loading benchmark episodes...\n",
      "v2 04df18149363d56c1044f3e177eeb496085628466862e032c01ceb8476f714da PPO\n",
      "[INFO] Transferability: seed=644267, start_idx=615, config_hash=04df18149363d56c1044f3e177eeb496085628466862e032c01ceb8476f714da\n",
      "1 {'market_features': ['close']}\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(OHLCV_DF[OHLCV_DF['symbol']==TICKER], TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # ‚Üê ‚úÖ Convert to list here\n",
    "\n",
    "print(\"[INFO] Epis√≥dios de benchmark salvos em:\", BENCHMARK_PATH)\n",
    "#for env_cls in [PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2]:\n",
    "for env_cls in [PositionTradingEnvV2]:\n",
    "    for agent_cls in [PPO,A2C]:\n",
    "        try:\n",
    "            NOTIFICATION.info('Started training new agent')\n",
    "            result_df = formalized_transferability_evaluation(\n",
    "                df=OHLCV_DF.copy(),\n",
    "                ticker=TICKER,\n",
    "                env_cls=env_cls,\n",
    "                agent_cls=agent_cls,\n",
    "                benchmark_path=DEFAULT_PATH+\"/benchmark_episodes.json\",\n",
    "                result_path=DEFAULT_PATH+\"/meta_df_transfer.csv\",\n",
    "                timesteps=TIMESTEPS,\n",
    "                n_timesteps=N_TIMESTEPS,\n",
    "                lookback=LOOKBACK,\n",
    "                seeds=[SEEDS[0]],  # or just [42] for quick run\n",
    "                checkpoint_dir=DEFAULT_PATH+\"/checkpoints\",\n",
    "                env_config={\"market_features\":MARKET_FEATURES}\n",
    "            )\n",
    "            NOTIFICATION.info('Test complete')\n",
    "        except:\n",
    "            NOTIFICATION.danger('Error on train')\n",
    "            \n",
    "NOTIFICATION.success('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a059ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(DEFAULT_PATH+\"/meta_df_transfer.csv\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea5fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28118cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "def compare_environments(result_df,env_version_a=\"v0\",env_version_b = \"v1\"):\n",
    "    \n",
    "\n",
    "    summary = result_df.groupby(\"env_version\")[[\n",
    "        \"score_train\", \"score_test\", \"advantage_train\", \"advantage_test\",\n",
    "        \"transfer_delta\", \"success_trades\", \"sharpe\", \"sortino\", \"calmar\",\n",
    "        \"max_drawdown\", \"volatility\", \"action_hold_ratio\", \"action_long_ratio\"\n",
    "    ]].agg([\"mean\", \"std\", \"median\"]).T\n",
    "    \n",
    "\n",
    "    mean_df = summary.xs('mean', level=1)\n",
    "    # Compute absolute difference between env_version 1 and 0\n",
    "    diffs = (mean_df[env_version_a] - mean_df[env_version_b]).abs().sort_values(ascending=False)\n",
    "  \n",
    "    # Plot using this sorted order\n",
    "    mean_df.loc[diffs.index].plot.bar(\n",
    "        figsize=(14, 6),\n",
    "        title=f\"Env {env_version_a} vs {env_version_b} ‚Äì Mean metric comparison (sorted by difference)\",\n",
    "        ylabel=\"Mean Value\"\n",
    "    )\n",
    "    metrics = [\"score_test\", \"advantage_test\", \"transfer_delta\", \"sharpe\", \"sortino\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        v0 = result_df[result_df.env_version == env_version_a][metric]\n",
    "        v1 = result_df[result_df.env_version == env_version_b][metric]\n",
    "        stat, pval = ttest_ind(v0, v1)\n",
    "        print(f\"{metric}: p={pval:.4f} | {env_version_a}_mean={v0.mean():.3f}, {env_version_b}_mean={v1.mean():.3f}\")\n",
    "\n",
    "    for metric in metrics:\n",
    "        sns.boxplot(data=result_df, x=\"env_version\", y=metric)\n",
    "        plt.title(f\"{metric} by Environment Version\")\n",
    "        plt.show()\n",
    "        \n",
    "    result_df['composite_score'] = (\n",
    "        result_df['advantage_test'] +\n",
    "        result_df['transfer_delta'] +\n",
    "        result_df['sharpe'] * 5 -\n",
    "        result_df['max_drawdown'] * 10\n",
    "    )\n",
    "\n",
    "    return result_df,result_df.groupby(\"env_version\")[\"composite_score\"].mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby([\"agent_name\",\"env_version\"])[[\n",
    "        \"score_train\", \"score_test\", \"advantage_train\", \"advantage_test\",\n",
    "        \"transfer_delta\", \"success_trades\", \"sharpe\", \"sortino\", \"calmar\",\n",
    "        \"max_drawdown\", \"volatility\", \"action_hold_ratio\", \"action_long_ratio\"\n",
    "    ]].agg([\"mean\", \"std\", \"median\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df['agent_name'].unique()\n",
    "#existing = pd.read_csv(result_path)\n",
    "        #seen_hashes = set(existing['config_hash'].unique())\n",
    "seen_hashes = set(zip(result_df['config_hash'], result_df['agent_name']))\n",
    "#seen_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b230b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#summary = compare_environments(result_df)\n",
    "#summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['env_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DEFAULT_PATH+\"/meta_df_transfer.csv\")\n",
    "results.groupby('env_version').mean(numeric_only=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b4e08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ac057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121580cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = Monitor(PositionTradingEnvV2(OHLCV_DF, ticker=\"AAPL\", seed=42, start_idx=100, market_features=[\"close\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7323fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ba931",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env_train, verbose=0, seed=42)\n",
    "model.learn(total_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56029a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f91e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
