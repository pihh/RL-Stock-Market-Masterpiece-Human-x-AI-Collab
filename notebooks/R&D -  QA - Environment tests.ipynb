{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ddaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837b0196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco Sá\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.defaults import RANDOM_SEEDS\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV1,PositionTradingEnvV2\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"qa__trading_environments\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "MARKET_FEATURES = ['close']\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "\n",
    "MARKET_FEATURES.sort()\n",
    "SEEDS.sort()\n",
    "\n",
    "DEVICE = boot()\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d80aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PositionTradingEnvV3(PositionTradingEnvV2):\n",
    "    \"\"\"\n",
    "    PositionTradingEnvV3\n",
    "    ---------------------\n",
    "\n",
    "    A human-inspired, curriculum-learning environment for reinforcement learning agents in trading.\n",
    "    This version introduces Kai's \"School Reward Curriculum\" — a staged reward system designed to mimic \n",
    "    how we teach children to explore, persist through failure, and progressively master difficult tasks.\n",
    "\n",
    "    ---------------------\n",
    "    Why This Matters:\n",
    "    ---------------------\n",
    "    Traditional RL assumes agents can survive cold optimization. But we’re building an intelligent, self-reflective system. \n",
    "    And like all intelligent learners, it must be nurtured.\n",
    "\n",
    "    So instead of punishing early failure or passivity too harshly, we reward **meaningful attempts to act**.\n",
    "    This fosters early exploration, builds confidence, and allows the agent to discover structure in the market \n",
    "    before we tighten expectations.\n",
    "\n",
    "    ---------------------\n",
    "    The School Reward Curriculum:\n",
    "    ---------------------\n",
    "\n",
    "    ◉ Phase 1: Exploration Over Inaction\n",
    "        - Reward is generous toward action.\n",
    "        - Foresight bonus: If a position switch *happened to be well-timed*, the agent gets extra points.\n",
    "        - Exploration bonus: Trying new positions is encouraged — even if the immediate outcome isn't profitable.\n",
    "        - Goal: Reward **trying**, not just winning. Build initiative.\n",
    "\n",
    "    ◉ Phase 2: Mastery Emerges\n",
    "        - Bonuses are gradually decayed.\n",
    "        - Agent must begin to **sustain good decisions**, not just get lucky.\n",
    "        - Less encouragement for randomness; more weight on consistent performance.\n",
    "        - Goal: Build **skill**, not just courage.\n",
    "\n",
    "    ◉ Phase 3: Graduation\n",
    "        - Return to strict oracle-relative reward.\n",
    "        - No more bonuses: the agent is ready for the real world.\n",
    "        - Encourage specialization — regime-awareness, style, timeframe expertise.\n",
    "        - Goal: Become a **professional**.\n",
    "\n",
    "    ---------------------\n",
    "    Usage:\n",
    "    ---------------------\n",
    "    Use the `reward_phase` parameter to set the phase manually, or optionally let the system \n",
    "    transition automatically after N episodes.\n",
    "\n",
    "    Available Phases:\n",
    "        - \"exploration\"\n",
    "        - \"mastery\"\n",
    "        - \"strict\"\n",
    "\n",
    "    ---------------------\n",
    "    Designed With ❤️ by Pi & Kai\n",
    "    ---------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, reward_phase=\"exploration\", foresight_bonus=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reward_phase = reward_phase\n",
    "        self.foresight_bonus = foresight_bonus\n",
    "        self.prev_position = 0\n",
    "\n",
    "    def _step_reward(self, action: int, price_change: float, oracle_action: int) -> float:\n",
    "        # Base reward: oracle-relative\n",
    "        base_reward = 0\n",
    "        if action == oracle_action:\n",
    "            base_reward = 1 * abs(price_change)\n",
    "        elif action != 0:\n",
    "            base_reward = -1 * abs(price_change)\n",
    "\n",
    "        bonus = 0\n",
    "\n",
    "        # --- Phase-specific logic ---\n",
    "        if self.reward_phase == \"exploration\":\n",
    "            if action != self.prev_position:\n",
    "                # Position switch bonus\n",
    "                if np.sign(price_change) == (1 if action == 1 else -1):\n",
    "                    bonus += self.foresight_bonus * abs(price_change)\n",
    "        elif self.reward_phase == \"mastery\":\n",
    "            if action != self.prev_position:\n",
    "                if np.sign(price_change) == (1 if action == 1 else -1):\n",
    "                    bonus += 0.5 * self.foresight_bonus * abs(price_change)\n",
    "        # \"strict\" phase does not add bonus\n",
    "\n",
    "        self.prev_position = action\n",
    "        return base_reward + bonus\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        obs, price_change, done, truncated,  _ = super().step(action)\n",
    "        oracle_action = action\n",
    "        if price_change < 0 :\n",
    "            oracle_action = abs(action-1)\n",
    "        reward = self._step_reward(action, price_change, oracle_action)\n",
    "        return obs, reward, done, truncated, {}\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.prev_position = 0\n",
    "        return super().reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3387f6b",
   "metadata": {},
   "source": [
    "# TESTS \n",
    "\n",
    "\n",
    "## Core categories:\n",
    "1. **Data** \n",
    "   * _prepare_ticker_df\n",
    "2. **Episode** \n",
    "   * _resample_episode\n",
    "   * _precompute_step_weights\n",
    "3. **Reward**\n",
    "   * Oracle computation\n",
    "   * Expected reward for action/step\n",
    "4. **Money** \n",
    "   * market_progress\n",
    "   * wallet_progress\n",
    "   * alpha_progress\n",
    "5. **Position**\n",
    "   * position updates\n",
    "6. **Observation**\n",
    "   * Check get observation - if matches expected reality\n",
    "7. **Behaviours**\n",
    "   * later\n",
    "8. **Edge cases**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87c2b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from your_module import PositionTradingEnvV1  # Replace with actual path\n",
    "\n",
    "class TestPositionTradingEnvSynthetic(unittest.TestCase):\n",
    "    def __init__(self,env_class):\n",
    "        self.env_class= env_class\n",
    "        self.setUp()\n",
    "        \n",
    "    def setUp(self):\n",
    "        # Synthetic constant uptrend (price +1 per step)\n",
    "        dates = pd.date_range(\"2024-01-01\", periods=10, freq=\"B\")\n",
    "        self.df = pd.DataFrame({\n",
    "           \"date\": pd.date_range(\"2024-01-01\", periods=10, freq=\"B\"),  # 10 business days\n",
    "            \"symbol\": [\"TEST\"] * 10,\n",
    "            \"close\": [100, 102, 101, 103, 104, 105, 107, 106, 108, 110],\n",
    "            \"day_of_week\": list(range(5)) * 2,                         # fake, for observations\n",
    "        })\n",
    "        self.env_kwargs = {\n",
    "            \"full_df\": self.df,\n",
    "            \"ticker\": \"TEST\",\n",
    "            \"market_features\": [\"close\"],\n",
    "            \"n_timesteps\": 5,\n",
    "            \"lookback\": 0,\n",
    "            \"seed\": 0,\n",
    "            \"start_idx\": 0\n",
    "        }\n",
    "\n",
    "    def get_env(self):\n",
    "        env = self.env_class(**self.env_kwargs)\n",
    "        env.reset()\n",
    "        return env\n",
    "        \n",
    "    def test_oracle_reward_sum(self):\n",
    "        env = self.env_class(**self.env_kwargs)\n",
    "        env.reset()\n",
    "        oracle_reward = 0\n",
    "        for i in range(env.n_timesteps - 1):\n",
    "            price_diff = env.prices[i + 1] - env.prices[i]\n",
    "            weight = env.step_weights[i]\n",
    "            reward = abs(weight * np.sign(price_diff)  * 100)\n",
    "            oracle_reward += reward\n",
    "        \n",
    "        self.assertAlmostEqual(oracle_reward, 100.0, places=4)\n",
    "\n",
    "    def test_episode_prices(self):\n",
    "        env = self.env_class(**self.env_kwargs)\n",
    "        obs, _ = env.reset()\n",
    "       \n",
    "        expected_prices = [100, 102, 101, 103, 104]\n",
    "        self.assertTrue((env.prices == expected_prices).all())\n",
    "        \n",
    "    def test_position_updates(self):\n",
    "        env = self.env_class(**self.env_kwargs)\n",
    "        obs, _ = env.reset()\n",
    "        print(env.position)\n",
    "        self.assertTrue(env.position== 0)\n",
    "\n",
    "        # Buy\n",
    "        obs, reward, terminated, truncated, _ = env.step(1)\n",
    "        self.assertTrue(env.position== 1)\n",
    "        self.assertTrue(env.entry_price > 0)\n",
    "\n",
    "        # Hold\n",
    "        obs, reward, terminated, truncated, _ = env.step(1)\n",
    "        self.assertTrue(env.position== 1)\n",
    "        self.assertGreater(env.holding_time, 0)\n",
    "\n",
    "        # Sell\n",
    "        obs, reward, terminated, truncated, _ = env.step(0)\n",
    "        self.assertTrue(env.position== 0)\n",
    "        self.assertTrue(env.entry_price== 0)\n",
    "\n",
    "    def test_scaled_reward_behavior(self):\n",
    "        env_kwargs = self.env_kwargs\n",
    "        env_kwargs['start_idx'] = 2\n",
    "        env = self.env_class(**env_kwargs)\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        rewards = []\n",
    "        for _ in range(env.n_timesteps - 1):\n",
    "            obs, reward, terminated, truncated, _ = env.step(1)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        self.assertTrue(all([r > 0 for r in rewards if r != 0]))  # uptrend\n",
    "\n",
    "    def test_flat_position_gets_penalized(self):\n",
    "        env = self.env_class(**self.env_kwargs)\n",
    "        obs, _ = env.reset()\n",
    "        rewards = []\n",
    "        for _ in range(env.n_timesteps - 1):\n",
    "            obs, reward, terminated, truncated, _ = env.step(0)\n",
    "            rewards.append(reward)\n",
    "        self.assertTrue(all([r <= 0 for r in rewards]))\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    unittest.main()\n",
    "t = TestPositionTradingEnvSynthetic(PositionTradingEnvV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81b53b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#np.sum(t.get_env().step_weights)\n",
    "t.test_oracle_reward_sum()\n",
    "t.test_episode_prices()\n",
    "t.test_position_updates()\n",
    "t.test_scaled_reward_behavior()\n",
    "t.test_flat_position_gets_penalized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ca98afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',\n",
       "               '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10',\n",
       "               '2024-01-11', '2024-01-12'],\n",
       "              dtype='datetime64[ns]', freq='B')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range(\"2024-01-01\", periods=10, freq=\"B\")\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2087ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
