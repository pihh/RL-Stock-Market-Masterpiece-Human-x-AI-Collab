{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a429826b",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Summary of the Task\n",
    "\n",
    "**Goal**: Given a sequence of discrete 6D word vectors (representing an unknown language), predict whether the **next word will increase or decrease the sentiment** of the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Step-by-Step Plan\n",
    "\n",
    "### 1. **Synthetic Dataset Creation**\n",
    "\n",
    "* Build a vocabulary of discrete 6D word vectors.\n",
    "* Define a simple but meaningful **sentiment function** over sequences (e.g., a non-linear function of vector mean or weighted sum).\n",
    "* For each sequence, compute sentiment before and after adding the next word ‚Üí label: `‚Üë` or `‚Üì`.\n",
    "\n",
    "### 2. **Input Preprocessing**\n",
    "\n",
    "Since vectors are discrete:\n",
    "\n",
    "* Normalize: ‚úÖ (to remove scale issues).\n",
    "* Quantize: ‚ùå probably not needed unless you're feeding integer tokens. Transformers work better with continuous embeddings.\n",
    "\n",
    "So: **normalize the vectors (e.g., MinMax or z-score)**, but don‚Äôt quantize yet ‚Äî we‚Äôre not tokenizing.\n",
    "\n",
    "### 3. **Model Architecture**\n",
    "\n",
    "* Transformer encoder for the **sequence** of vectors.\n",
    "* Final vector (to be predicted) can be:\n",
    "\n",
    "  * Appended (and let the model learn the delta)\n",
    "  * OR input separately and compared with sentence encoding\n",
    "\n",
    "**Output**: Binary classification (‚Üë or ‚Üì in sentiment)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513e3698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706dac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.defaults import RANDOM_SEEDS, TOP2_STOCK_BY_SECTOR\n",
    "from tracker import OHLCV_DF, EpisodeTracker, EnvironmentTracker, AgentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c51865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>sp500</th>\n",
       "      <th>vix</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>candle_size</th>\n",
       "      <th>candle_relative_size</th>\n",
       "      <th>candle_upper</th>\n",
       "      <th>candle_body</th>\n",
       "      <th>candle_lower</th>\n",
       "      <th>candle_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>182.640</td>\n",
       "      <td>182.94</td>\n",
       "      <td>179.120</td>\n",
       "      <td>179.70</td>\n",
       "      <td>106090378.0</td>\n",
       "      <td>831898.0</td>\n",
       "      <td>47.9354</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>-0.026600</td>\n",
       "      <td>3.820</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>0.078534</td>\n",
       "      <td>0.769634</td>\n",
       "      <td>0.151832</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>179.610</td>\n",
       "      <td>180.17</td>\n",
       "      <td>174.640</td>\n",
       "      <td>174.92</td>\n",
       "      <td>95142198.0</td>\n",
       "      <td>848518.0</td>\n",
       "      <td>47.0058</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>-0.026600</td>\n",
       "      <td>5.530</td>\n",
       "      <td>0.030789</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>172.700</td>\n",
       "      <td>175.30</td>\n",
       "      <td>171.640</td>\n",
       "      <td>172.00</td>\n",
       "      <td>103899632.0</td>\n",
       "      <td>960344.0</td>\n",
       "      <td>46.9605</td>\n",
       "      <td>0.1961</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>3.660</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.710383</td>\n",
       "      <td>0.191257</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>172.890</td>\n",
       "      <td>174.14</td>\n",
       "      <td>171.030</td>\n",
       "      <td>172.17</td>\n",
       "      <td>94554334.0</td>\n",
       "      <td>715419.0</td>\n",
       "      <td>46.7703</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>3.110</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>0.401929</td>\n",
       "      <td>0.231511</td>\n",
       "      <td>0.366559</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-10</th>\n",
       "      <td>169.080</td>\n",
       "      <td>172.50</td>\n",
       "      <td>168.170</td>\n",
       "      <td>172.19</td>\n",
       "      <td>117005143.0</td>\n",
       "      <td>956342.0</td>\n",
       "      <td>46.7029</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>4.330</td>\n",
       "      <td>0.025609</td>\n",
       "      <td>0.071594</td>\n",
       "      <td>0.718245</td>\n",
       "      <td>0.210162</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-29</th>\n",
       "      <td>203.575</td>\n",
       "      <td>203.81</td>\n",
       "      <td>198.510</td>\n",
       "      <td>199.95</td>\n",
       "      <td>51477938.0</td>\n",
       "      <td>652509.0</td>\n",
       "      <td>59.1217</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>5.300</td>\n",
       "      <td>0.026035</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.683962</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-30</th>\n",
       "      <td>199.370</td>\n",
       "      <td>201.96</td>\n",
       "      <td>196.780</td>\n",
       "      <td>200.85</td>\n",
       "      <td>70819942.0</td>\n",
       "      <td>605924.0</td>\n",
       "      <td>59.1169</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>5.180</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-02</th>\n",
       "      <td>200.280</td>\n",
       "      <td>202.13</td>\n",
       "      <td>200.120</td>\n",
       "      <td>201.70</td>\n",
       "      <td>35423294.0</td>\n",
       "      <td>501431.0</td>\n",
       "      <td>59.3594</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.213930</td>\n",
       "      <td>0.706468</td>\n",
       "      <td>0.079602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03</th>\n",
       "      <td>201.350</td>\n",
       "      <td>203.77</td>\n",
       "      <td>200.955</td>\n",
       "      <td>203.27</td>\n",
       "      <td>46381567.0</td>\n",
       "      <td>519820.0</td>\n",
       "      <td>59.7037</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>2.815</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.177620</td>\n",
       "      <td>0.682060</td>\n",
       "      <td>0.140320</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-04</th>\n",
       "      <td>202.910</td>\n",
       "      <td>206.24</td>\n",
       "      <td>202.100</td>\n",
       "      <td>202.82</td>\n",
       "      <td>43603985.0</td>\n",
       "      <td>568214.0</td>\n",
       "      <td>59.7081</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>-0.002214</td>\n",
       "      <td>4.140</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               open    high      low   close       volume  trade_count  \\\n",
       "date                                                                     \n",
       "2022-01-04  182.640  182.94  179.120  179.70  106090378.0     831898.0   \n",
       "2022-01-05  179.610  180.17  174.640  174.92   95142198.0     848518.0   \n",
       "2022-01-06  172.700  175.30  171.640  172.00  103899632.0     960344.0   \n",
       "2022-01-07  172.890  174.14  171.030  172.17   94554334.0     715419.0   \n",
       "2022-01-10  169.080  172.50  168.170  172.19  117005143.0     956342.0   \n",
       "...             ...     ...      ...     ...          ...          ...   \n",
       "2025-05-29  203.575  203.81  198.510  199.95   51477938.0     652509.0   \n",
       "2025-05-30  199.370  201.96  196.780  200.85   70819942.0     605924.0   \n",
       "2025-06-02  200.280  202.13  200.120  201.70   35423294.0     501431.0   \n",
       "2025-06-03  201.350  203.77  200.955  203.27   46381567.0     519820.0   \n",
       "2025-06-04  202.910  206.24  202.100  202.82   43603985.0     568214.0   \n",
       "\n",
       "              sp500     vix  return_1d  candle_size  candle_relative_size  \\\n",
       "date                                                                        \n",
       "2022-01-04  47.9354  0.1691  -0.026600        3.820              0.020915   \n",
       "2022-01-05  47.0058  0.1973  -0.026600        5.530              0.030789   \n",
       "2022-01-06  46.9605  0.1961  -0.016693        3.660              0.021193   \n",
       "2022-01-07  46.7703  0.1876   0.000988        3.110              0.017988   \n",
       "2022-01-10  46.7029  0.1940   0.000116        4.330              0.025609   \n",
       "...             ...     ...        ...          ...                   ...   \n",
       "2025-05-29  59.1217  0.1918  -0.002345        5.300              0.026035   \n",
       "2025-05-30  59.1169  0.1857   0.004501        5.180              0.025982   \n",
       "2025-06-02  59.3594  0.1836   0.004232        2.010              0.010036   \n",
       "2025-06-03  59.7037  0.1769   0.007784        2.815              0.013981   \n",
       "2025-06-04  59.7081  0.1761  -0.002214        4.140              0.020403   \n",
       "\n",
       "            candle_upper  candle_body  candle_lower  candle_direction  \n",
       "date                                                                   \n",
       "2022-01-04      0.078534     0.769634      0.151832              -1.0  \n",
       "2022-01-05      0.101266     0.848101      0.050633              -1.0  \n",
       "2022-01-06      0.710383     0.191257      0.098361              -1.0  \n",
       "2022-01-07      0.401929     0.231511      0.366559              -1.0  \n",
       "2022-01-10      0.071594     0.718245      0.210162               1.0  \n",
       "...                  ...          ...           ...               ...  \n",
       "2025-05-29      0.044340     0.683962      0.271698              -1.0  \n",
       "2025-05-30      0.214286     0.285714      0.500000               1.0  \n",
       "2025-06-02      0.213930     0.706468      0.079602               1.0  \n",
       "2025-06-03      0.177620     0.682060      0.140320               1.0  \n",
       "2025-06-04      0.804348     0.021739      0.173913              -1.0  \n",
       "\n",
       "[857 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = OHLCV_DF[OHLCV_DF['symbol']==\"AAPL\"].copy()\n",
    "df = df.set_index('date')[['open','high','low','close','volume','trade_count','sp500','vix', 'return_1d',]]\n",
    "\n",
    "df['candle_size'] = df['high'] - df['low']\n",
    "df['candle_relative_size'] = df['candle_size'] / df['open']\n",
    "\n",
    "df['candle_upper'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['candle_size']\n",
    "df['candle_body'] = np.abs(df['close'] - df['open']) / df['candle_size']\n",
    "df['candle_lower'] = (np.minimum(df['open'], df['close']) - df['low']) / df['candle_size']\n",
    "\n",
    "df['candle_direction'] = np.sign(df['close'] - df['open'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b1be67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Candle Vector Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!--| quarto-html-table-processing: none -->\n",
       "<table id=\"itables_5c17c6bd_9153_4af9_9570_65979d6c636f\"><tbody><tr>\n",
       "    <td style=\"vertical-align:middle; text-align:left\">\n",
       "    <a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
       "    <g style=\"fill:#d9d7fc\">\n",
       "        <path d=\"M100,400H500V357H100Z\" />\n",
       "        <path d=\"M100,300H400V257H100Z\" />\n",
       "        <path d=\"M0,200H400V157H0Z\" />\n",
       "        <path d=\"M100,100H500V57H100Z\" />\n",
       "        <path d=\"M100,350H500V307H100Z\" />\n",
       "        <path d=\"M100,250H400V207H100Z\" />\n",
       "        <path d=\"M0,150H400V107H0Z\" />\n",
       "        <path d=\"M100,50H500V7H100Z\" />\n",
       "    </g>\n",
       "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
       "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"0;0;400\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;300;0\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;400\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
       "            <g transform=\"translate(45 50) rotate(-45)\">\n",
       "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
       "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(450 152)\">\n",
       "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
       "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(50 352)\">\n",
       "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
       "                <polygon points=\"-35,10 0,45 35,10\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(75 250)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(425 250) rotate(180)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "        </g>\n",
       "    </g>\n",
       "</svg>\n",
       "</a>\n",
       "    Loading ITables v2.4.0 from the internet...\n",
       "    (need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
       "    </tr></tbody></table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.3.2/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import { ITable, jQuery as $ } from 'https://www.unpkg.com/dt_for_itables@2.3.2/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_5c17c6bd_9153_4af9_9570_65979d6c636f:not(.dataTable)\").forEach(table => {\n",
       "        if (!(table instanceof HTMLTableElement))\n",
       "            return;\n",
       "\n",
       "        let dt_args = {\"layout\": {\"topStart\": \"pageLength\", \"topEnd\": \"search\", \"bottomStart\": \"info\", \"bottomEnd\": \"paging\"}, \"style\": {\"table-layout\": \"auto\", \"width\": \"auto\", \"margin\": \"auto\", \"caption-side\": \"bottom\"}, \"text_in_header_can_be_selected\": true, \"order\": [], \"classes\": [\"display\", \"nowrap\"], \"table_html\": \"<table><thead>\\n    <tr style=\\\"text-align: right;\\\">\\n      <th></th>\\n      <th>candle_relative_size</th>\\n      <th>candle_upper</th>\\n      <th>candle_body</th>\\n      <th>candle_lower</th>\\n      <th>candle_direction</th>\\n      <th>return_1d</th>\\n    </tr>\\n    <tr>\\n      <th>date</th>\\n      <th></th>\\n      <th></th>\\n      <th></th>\\n      <th></th>\\n      <th></th>\\n      <th></th>\\n    </tr>\\n  </thead></table>\", \"data_json\": \"[[\\\"2022-01-04\\\", -0.064016, -0.909524, 1.129761, -0.57876, -1.10333, -1.442646], [\\\"2022-01-05\\\", 0.716582, -0.796797, 1.420436, -1.060544, -1.10333, -1.442646], [\\\"2022-01-06\\\", -0.042088, 2.22379, -1.012766, -0.833325, -1.10333, -0.911009], [\\\"2022-01-07\\\", -0.295437, 0.694182, -0.863648, 0.443496, -1.10333, 0.037884], [\\\"2022-01-10\\\", 0.30707, -0.943941, 0.939398, -0.301071, 0.908487, -0.008923], [\\\"2022-01-11\\\", 0.282765, -1.185233, 0.623721, 0.336271, 0.908487, 0.885549], [\\\"2022-01-12\\\", -0.658193, 0.92836, -0.795159, 0.130659, -1.10333, 0.122776], [\\\"2022-01-13\\\", 0.454658, -0.44681, 1.039775, -0.90733, -1.10333, -1.036303], [\\\"2022-01-14\\\", -0.476369, 0.009901, 0.661117, -0.859146, 0.908487, 0.259106], [\\\"2022-01-18\\\", -0.27247, 0.33029, 0.299317, -0.701757, -1.10333, -1.029112], [\\\"2022-01-19\\\", 0.672811, -0.25701, 0.995772, -1.032992, -1.10333, -1.143454], [\\\"2022-01-20\\\", 0.886493, 1.13543, -0.05765, -1.01595, -1.10333, -0.570437], [\\\"2022-01-21\\\", 0.220259, 1.05746, 0.121744, -1.171648, -1.10333, -0.700203], [\\\"2022-01-24\\\", 2.037291, -0.855274, -0.941384, 2.030914, 0.908487, -0.276197], [\\\"2022-01-25\\\", 1.136884, 1.275543, -1.204964, 0.32402, 0.908487, -0.626121], [\\\"2022-01-26\\\", 1.459026, -0.6276, 0.427145, 0.053562, -1.10333, -0.045385], [\\\"2022-01-27\\\", 0.98831, -0.05923, 0.430751, -0.496722, -1.10333, -0.173104], [\\\"2022-01-28\\\", 1.884504, -1.285835, 0.545532, 0.533335, 0.908487, 3.729477], [\\\"2022-01-31\\\", 0.833181, -1.100251, 1.396093, -0.737937, 0.908487, 1.386888], [\\\"2022-02-01\\\", -0.568109, -0.848156, -0.842745, 1.897313, 0.908487, -0.067354]]\"};\n",
       "        new ITable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulated OHLCV-like dataframe (for demonstration)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "df['candle_size'] = df['high'] - df['low']\n",
    "df['candle_relative_size'] = df['candle_size'] / df['open']\n",
    "df['candle_upper'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['candle_size']\n",
    "df['candle_body'] = np.abs(df['close'] - df['open']) / df['candle_size']\n",
    "df['candle_lower'] = (np.minimum(df['open'], df['close']) - df['low']) / df['candle_size']\n",
    "df['candle_direction'] = np.sign(df['close'] - df['open'])\n",
    "\n",
    "# Drop rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Select 6D word vector features\n",
    "vector_cols = [\n",
    "    'candle_relative_size',\n",
    "    'candle_upper',\n",
    "    'candle_body',\n",
    "    'candle_lower',\n",
    "    'candle_direction',\n",
    "    'return_1d'\n",
    "]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "df[vector_cols] = scaler.fit_transform(df[vector_cols])\n",
    "\n",
    "# Preview the normalized vector-based dataset\n",
    "import ace_tools_open as tools; tools.display_dataframe_to_user(name=\"Normalized Candle Vector Dataset\", dataframe=df[vector_cols].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e1c8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentShiftDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vector_cols = [\n",
    "            'candle_relative_size',\n",
    "            'candle_upper',\n",
    "            'candle_body',\n",
    "            'candle_lower',\n",
    "            'candle_direction',\n",
    "            'return_1d'\n",
    "        ]\n",
    "        self.data = df[self.vector_cols].values\n",
    "        self.labels = (df['return_1d'].shift(-1) > 0).astype(int).values  # 1 if next return is positive\n",
    "        self.valid_indices = range(len(df) - sequence_length - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        sequence = self.data[start_idx:end_idx]\n",
    "        label = self.labels[end_idx]  # Label is based on return of the day after the sequence\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa90f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 6])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "dataset = SentimentShiftDataset(df, sequence_length=10)\n",
    "\n",
    "# Get one sample\n",
    "x, y = dataset[0]\n",
    "print(\"Input shape:\", x.shape)   # (10, 6)\n",
    "print(\"Label:\", y.item())        # 0 or 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b3bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=6, model_dim=32, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # Input projection to model_dim\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 100, model_dim))  # max_len=100\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(model_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # Binary classification (‚Üë / ‚Üì)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Project input to model_dim\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Transformer expects (seq_len, batch_size, model_dim)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Use the last token (or mean pooling)\n",
    "        x = x[-1, :, :]  # (batch_size, model_dim)\n",
    "\n",
    "        # Classify\n",
    "        out = self.cls_head(x)  # (batch_size, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da9bd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=6, model_dim=32, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 100, model_dim))  # max_len=100\n",
    "\n",
    "        # Define the encoder layer\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(model_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        x = self.input_proj(x)  # (B, T, D)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "\n",
    "        attentions = []\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            # Hook into attention weights via forward pre-hook\n",
    "            def hook(module, input, output):\n",
    "                attn = module.self_attn(input[0], input[0], input[0])[1]\n",
    "                attentions.append(attn.detach().cpu())\n",
    "\n",
    "            handle = layer.register_forward_hook(hook)\n",
    "            x = layer(x)\n",
    "            handle.remove()\n",
    "\n",
    "        cls_out = x[:, -1, :]  # use the last token\n",
    "        logits = self.cls_head(cls_out)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attentions\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5b7b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco S√°\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "# Instantiate dataset\n",
    "dataset = SentimentShiftDataset(df, sequence_length=10)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerSentimentClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                output = model(batch_x)\n",
    "                preds = output.argmax(dim=1)\n",
    "                correct += (preds == batch_y).sum().item()\n",
    "                total += batch_y.size(0)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val Acc: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c66d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7118 | Val Acc: 0.5235\n",
      "Epoch 2 | Loss: 0.6928 | Val Acc: 0.4765\n",
      "Epoch 3 | Loss: 0.6885 | Val Acc: 0.4941\n",
      "Epoch 4 | Loss: 0.6869 | Val Acc: 0.5235\n",
      "Epoch 5 | Loss: 0.6905 | Val Acc: 0.4824\n",
      "Epoch 6 | Loss: 0.6916 | Val Acc: 0.5412\n",
      "Epoch 7 | Loss: 0.6895 | Val Acc: 0.5471\n",
      "Epoch 8 | Loss: 0.6850 | Val Acc: 0.5176\n",
      "Epoch 9 | Loss: 0.6768 | Val Acc: 0.5647\n",
      "Epoch 10 | Loss: 0.6702 | Val Acc: 0.5294\n",
      "Epoch 11 | Loss: 0.6718 | Val Acc: 0.5353\n",
      "Epoch 12 | Loss: 0.6510 | Val Acc: 0.5294\n",
      "Epoch 13 | Loss: 0.6696 | Val Acc: 0.5176\n",
      "Epoch 14 | Loss: 0.6690 | Val Acc: 0.5294\n",
      "Epoch 15 | Loss: 0.6318 | Val Acc: 0.5412\n",
      "Epoch 16 | Loss: 0.6579 | Val Acc: 0.5353\n",
      "Epoch 17 | Loss: 0.6342 | Val Acc: 0.5706\n",
      "Epoch 18 | Loss: 0.6562 | Val Acc: 0.5412\n",
      "Epoch 19 | Loss: 0.6307 | Val Acc: 0.4765\n",
      "Epoch 20 | Loss: 0.6173 | Val Acc: 0.5412\n",
      "Epoch 21 | Loss: 0.6139 | Val Acc: 0.5000\n",
      "Epoch 22 | Loss: 0.5937 | Val Acc: 0.5000\n",
      "Epoch 23 | Loss: 0.5996 | Val Acc: 0.5235\n",
      "Epoch 24 | Loss: 0.5685 | Val Acc: 0.5000\n",
      "Epoch 25 | Loss: 0.5480 | Val Acc: 0.5176\n",
      "Epoch 26 | Loss: 0.5734 | Val Acc: 0.5235\n",
      "Epoch 27 | Loss: 0.5394 | Val Acc: 0.4706\n",
      "Epoch 28 | Loss: 0.5508 | Val Acc: 0.4824\n",
      "Epoch 29 | Loss: 0.5856 | Val Acc: 0.5059\n",
      "Epoch 30 | Loss: 0.5584 | Val Acc: 0.5118\n",
      "Epoch 31 | Loss: 0.4926 | Val Acc: 0.5294\n",
      "Epoch 32 | Loss: 0.4975 | Val Acc: 0.5353\n",
      "Epoch 33 | Loss: 0.4890 | Val Acc: 0.4647\n",
      "Epoch 34 | Loss: 0.4800 | Val Acc: 0.4824\n",
      "Epoch 35 | Loss: 0.4665 | Val Acc: 0.4882\n",
      "Epoch 36 | Loss: 0.4099 | Val Acc: 0.4647\n",
      "Epoch 37 | Loss: 0.4624 | Val Acc: 0.5176\n",
      "Epoch 38 | Loss: 0.4180 | Val Acc: 0.4941\n",
      "Epoch 39 | Loss: 0.4337 | Val Acc: 0.5000\n",
      "Epoch 40 | Loss: 0.4366 | Val Acc: 0.4882\n",
      "Epoch 41 | Loss: 0.4439 | Val Acc: 0.5118\n",
      "Epoch 42 | Loss: 0.4522 | Val Acc: 0.5059\n",
      "Epoch 43 | Loss: 0.3993 | Val Acc: 0.5000\n",
      "Epoch 44 | Loss: 0.3847 | Val Acc: 0.5353\n",
      "Epoch 45 | Loss: 0.3697 | Val Acc: 0.5000\n",
      "Epoch 46 | Loss: 0.3819 | Val Acc: 0.5000\n",
      "Epoch 47 | Loss: 0.4133 | Val Acc: 0.5235\n",
      "Epoch 48 | Loss: 0.4058 | Val Acc: 0.4647\n",
      "Epoch 49 | Loss: 0.3347 | Val Acc: 0.5176\n",
      "Epoch 50 | Loss: 0.3473 | Val Acc: 0.5235\n",
      "Epoch 51 | Loss: 0.3041 | Val Acc: 0.5059\n",
      "Epoch 52 | Loss: 0.2728 | Val Acc: 0.5353\n",
      "Epoch 53 | Loss: 0.3070 | Val Acc: 0.5000\n",
      "Epoch 54 | Loss: 0.2982 | Val Acc: 0.5176\n",
      "Epoch 55 | Loss: 0.2897 | Val Acc: 0.5294\n",
      "Epoch 56 | Loss: 0.2979 | Val Acc: 0.5000\n",
      "Epoch 57 | Loss: 0.3276 | Val Acc: 0.5235\n",
      "Epoch 58 | Loss: 0.3264 | Val Acc: 0.4824\n",
      "Epoch 59 | Loss: 0.2993 | Val Acc: 0.5294\n",
      "Epoch 60 | Loss: 0.2899 | Val Acc: 0.5176\n",
      "Epoch 61 | Loss: 0.3025 | Val Acc: 0.5118\n",
      "Epoch 62 | Loss: 0.2725 | Val Acc: 0.5353\n",
      "Epoch 63 | Loss: 0.2822 | Val Acc: 0.5412\n",
      "Epoch 64 | Loss: 0.2619 | Val Acc: 0.5235\n",
      "Epoch 65 | Loss: 0.2627 | Val Acc: 0.5294\n",
      "Epoch 66 | Loss: 0.2116 | Val Acc: 0.5059\n",
      "Epoch 67 | Loss: 0.2225 | Val Acc: 0.5412\n",
      "Epoch 68 | Loss: 0.2668 | Val Acc: 0.5118\n",
      "Epoch 69 | Loss: 0.2257 | Val Acc: 0.5235\n",
      "Epoch 70 | Loss: 0.1793 | Val Acc: 0.5059\n",
      "Epoch 71 | Loss: 0.2545 | Val Acc: 0.5647\n",
      "Epoch 72 | Loss: 0.2175 | Val Acc: 0.5588\n",
      "Epoch 73 | Loss: 0.2325 | Val Acc: 0.5059\n",
      "Epoch 74 | Loss: 0.2494 | Val Acc: 0.5294\n",
      "Epoch 75 | Loss: 0.2040 | Val Acc: 0.5353\n",
      "Epoch 76 | Loss: 0.2248 | Val Acc: 0.5294\n",
      "Epoch 77 | Loss: 0.1927 | Val Acc: 0.5353\n",
      "Epoch 78 | Loss: 0.1954 | Val Acc: 0.5235\n",
      "Epoch 79 | Loss: 0.1735 | Val Acc: 0.5235\n",
      "Epoch 80 | Loss: 0.2377 | Val Acc: 0.5235\n",
      "Epoch 81 | Loss: 0.2360 | Val Acc: 0.5353\n",
      "Epoch 82 | Loss: 0.2096 | Val Acc: 0.5706\n",
      "Epoch 83 | Loss: 0.1723 | Val Acc: 0.5118\n",
      "Epoch 84 | Loss: 0.1625 | Val Acc: 0.5294\n",
      "Epoch 85 | Loss: 0.2002 | Val Acc: 0.5118\n",
      "Epoch 86 | Loss: 0.2523 | Val Acc: 0.5706\n",
      "Epoch 87 | Loss: 0.1993 | Val Acc: 0.5235\n",
      "Epoch 88 | Loss: 0.1569 | Val Acc: 0.5529\n",
      "Epoch 89 | Loss: 0.1727 | Val Acc: 0.5294\n",
      "Epoch 90 | Loss: 0.1883 | Val Acc: 0.4765\n",
      "Epoch 91 | Loss: 0.1644 | Val Acc: 0.5000\n",
      "Epoch 92 | Loss: 0.1515 | Val Acc: 0.5471\n",
      "Epoch 93 | Loss: 0.1497 | Val Acc: 0.5118\n",
      "Epoch 94 | Loss: 0.1487 | Val Acc: 0.5176\n",
      "Epoch 95 | Loss: 0.1936 | Val Acc: 0.5294\n",
      "Epoch 96 | Loss: 0.2701 | Val Acc: 0.5000\n",
      "Epoch 97 | Loss: 0.1850 | Val Acc: 0.5235\n",
      "Epoch 98 | Loss: 0.1660 | Val Acc: 0.4941\n",
      "Epoch 99 | Loss: 0.2261 | Val Acc: 0.4882\n",
      "Epoch 100 | Loss: 0.3350 | Val Acc: 0.5294\n"
     ]
    }
   ],
   "source": [
    "train(model,train_loader,val_loader,optimizer,criterion,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d1870e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"[‚úì] Held-Out Accuracy: {accuracy:.4f}\")\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "007411f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Held-Out Accuracy: 0.5294\n"
     ]
    }
   ],
   "source": [
    "preds,labels=evaluate_model(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9afb4576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ‚Üì       0.51      0.58      0.54        81\n",
      "           ‚Üë       0.56      0.48      0.52        89\n",
      "\n",
      "    accuracy                           0.53       170\n",
      "   macro avg       0.53      0.53      0.53       170\n",
      "weighted avg       0.53      0.53      0.53       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"‚Üì\", \"‚Üë\"], yticklabels=[\"‚Üì\", \"‚Üë\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(labels, preds, target_names=[\"‚Üì\", \"‚Üë\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6984aae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerSentimentClassifier.forward() got an unexpected keyword argument 'return_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_x, _ \u001b[38;5;241m=\u001b[39m val_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m sample_x \u001b[38;5;241m=\u001b[39m sample_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m logits, attentions \u001b[38;5;241m=\u001b[39m model(sample_x, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Plot attention from last layer and first head\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerSentimentClassifier.forward() got an unexpected keyword argument 'return_attention'"
     ]
    }
   ],
   "source": [
    "sample_x, _ = val_dataset[0]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "logits, attentions = model(sample_x, return_attention=True)\n",
    "\n",
    "# Plot attention from last layer and first head\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attn = attentions[-1][0, 0]  # shape: (seq_len, seq_len)\n",
    "sns.heatmap(attn, cmap='viridis', xticklabels=True, yticklabels=True)\n",
    "plt.title(\"Attention Map (Last Layer, Head 0)\")\n",
    "plt.xlabel(\"Input Candle\")\n",
    "plt.ylabel(\"Attending To\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce588e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
