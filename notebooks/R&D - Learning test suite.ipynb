{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69fe254",
   "metadata": {},
   "source": [
    "Absolutely — here's the full **README.md** draft that documents the initial version of our **LTM Test Suite**: Learnability, Transferability, Meta-Evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 📊 LTM Test Suite: Learnability, Transferability, and Meta-Evaluation\n",
    "\n",
    "This project establishes the **foundation for evaluating trading agents** based on their ability to:\n",
    "\n",
    "1. **Learn effectively** in specific environments (Learnability)\n",
    "2. **Generalize their knowledge** to new timeframes (Transferability)\n",
    "3. **Be selected or ranked** using meta-features (Meta-Evaluation)\n",
    "\n",
    "This evaluation system is central to our long-term goal of creating **regime-aware, introspective, intelligent agents** that know when and where they can succeed.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚧 Status\n",
    "\n",
    "✅ **Prototype phase (single-file implementation)**\n",
    "⬜ Modular pipeline with full CLI / batch capabilities\n",
    "⬜ Meta-learning model integration\n",
    "⬜ Curriculum design system\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Core Concepts\n",
    "\n",
    "### 🔹 Learnability\n",
    "\n",
    "> Can an agent learn useful trading behavior in a specific environment?\n",
    "\n",
    "* Agent is trained on a single episode (e.g., 1 stock in 1 month)\n",
    "* Evaluated by its normalized episode score (`0–100`)\n",
    "* Uses fixed seeds and training steps for fair comparison\n",
    "* Logged across multiple runs to assess robustness\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Transferability\n",
    "\n",
    "> Can the knowledge acquired in one environment be applied to the next one?\n",
    "\n",
    "* Agent trained on `Month T`, evaluated (or fine-tuned) on `Month T+1`\n",
    "* Compared against:\n",
    "\n",
    "  * Random agent baseline\n",
    "  * Oracle performance\n",
    "* Transfer success = performance delta relative to training or baseline\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Meta-Evaluation\n",
    "\n",
    "> Can we predict which environments are promising *before* training?\n",
    "\n",
    "* Extracts meta-features from the environment:\n",
    "\n",
    "  * Volatility, momentum, entropy, Hurst, kurtosis, etc.\n",
    "* Generates `meta_df.csv` with:\n",
    "\n",
    "  * `learnability`, `agent advantage`, `transfer_delta` labels\n",
    "* Enables future predictive modeling and curriculum learning\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Test Protocol\n",
    "\n",
    "### Episode Sampling\n",
    "\n",
    "* All episodes:\n",
    "\n",
    "  * Start on Mondays\n",
    "  * Have fixed `n_timesteps`\n",
    "  * Are non-overlapping **or** weekly (depending on config)\n",
    "* Benchmark episodes are stored in:\n",
    "\n",
    "  ```\n",
    "  data/experiments/learnability_test/benchmark_episodes.json\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Agent Setup\n",
    "\n",
    "* Agents trained with PPO (via Stable-Baselines3)\n",
    "* Random agent used as baseline\n",
    "* Oracle score used as upper bound reference\n",
    "* Configurations, seeds, and policies are logged and reproducible\n",
    "\n",
    "---\n",
    "\n",
    "### Logging & Outputs\n",
    "\n",
    "| Output                    | Description                                    |\n",
    "| ------------------------- | ---------------------------------------------- |\n",
    "| `meta_df.csv`             | Meta-features + labels for each run            |\n",
    "| `checkpoints/{id}.zip`    | Trained agents saved with unique config hashes |\n",
    "| `scores/{id}_results.csv` | Per-step and final metrics                     |\n",
    "| `logs/`                   | Training logs and config snapshots             |\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Project Structure (Coming Soon)\n",
    "\n",
    "```\n",
    "ltm_suite/\n",
    "├── configs/\n",
    "│   └── benchmark_episodes.json\n",
    "├── benchmarks/\n",
    "│   └── scores/\n",
    "│   └── checkpoints/\n",
    "│   └── logs/\n",
    "├── results/\n",
    "│   └── meta_df.csv\n",
    "├── runners/\n",
    "│   ├── run_learnability.py\n",
    "│   ├── run_transferability.py\n",
    "│   └── run_meta_evaluation.py\n",
    "├── utils/\n",
    "│   └── env_loader.py\n",
    "│   └── metrics.py\n",
    "│   └── logger.py\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Success Criteria\n",
    "\n",
    "* Episode scores consistently above 50% = good learning\n",
    "* Transfer performance better than random baseline = generalization\n",
    "* Meta-features predictive of good environments = meta-learning success\n",
    "* All results are reproducible and statistically valid (multiple seeds)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Next Milestone\n",
    "\n",
    "We now proceed to:\n",
    "\n",
    "* Implement a single-file prototype for **Learnability Test**\n",
    "* Store benchmark episodes\n",
    "* Save logs, scores, meta-data, and model checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to add/change anything before we start the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1d6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.utils.system import boot\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from experiments import check_if_experiment_exists, register_experiment ,experiment_hash\n",
    "\n",
    "# ========== SYSTEM BOOT ==========\n",
    "DEVICE = boot()\n",
    "EXPERIMENT_NAME = \"core_learnability_test\"\n",
    "DEFAULT_PATH = \"data/experiments/\" + EXPERIMENT_NAME\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = boot()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OHLCV_DF = load_base_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d87a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Running episode from idx 615 with seed 42\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (23,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     74\u001b[0m     action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m#done = terminated or truncated\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32m~\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\environments.py:307\u001b[0m, in \u001b[0;36mPositionTradingEnvV2.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    306\u001b[0m     price_now \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprices[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_idx]\n\u001b[1;32m--> 307\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\environments.py:207\u001b[0m, in \u001b[0;36mPositionTradingEnvV1.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholding_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_actions[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m [action]\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation(), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\Dev\\RL-Stock-Market-Masterpiece-Human-x-AI-Collab\\notebooks\\environments.py:371\u001b[0m, in \u001b[0;36mPositionTradingEnvV2._get_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m     onehot[a] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    369\u001b[0m     action_onehots\u001b[38;5;241m.\u001b[39mextend(onehot)\n\u001b[1;32m--> 371\u001b[0m obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholding_time,\n\u001b[0;32m    374\u001b[0m     pnl,\n\u001b[0;32m    375\u001b[0m     price_ratio,\n\u001b[0;32m    376\u001b[0m     rolling_ret,\n\u001b[0;32m    377\u001b[0m     volatility,\n\u001b[0;32m    378\u001b[0m     drawdown,\n\u001b[0;32m    379\u001b[0m     regime,\n\u001b[0;32m    380\u001b[0m     confidence,\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39mday_one_hot,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;241m*\u001b[39maction_onehots,\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39mstep_values\n\u001b[0;32m    384\u001b[0m ], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (23,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# ltm_test_suite.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from environments import PositionTradingEnv,PositionTradingEnvV2  # assumed to exist\n",
    "from data import sample_valid_episodes, extract_meta_features \n",
    "\n",
    "# ========== CONFIG ==========\n",
    "TICKER = \"AAPL\"\n",
    "TIMESTEPS = 10_000\n",
    "EVAL_EPISODES = 5\n",
    "N_TIMESTEPS = 60\n",
    "LOOKBACK = 0\n",
    "SEEDS = [42, 52, 62]\n",
    "BENCHMARK_PATH = DEFAULT_PATH+\"/benchmark_episodes.json\"\n",
    "CHECKPOINT_DIR = DEFAULT_PATH+\"/checkpoints\"\n",
    "SCORES_DIR = DEFAULT_PATH+\"/scores\"\n",
    "META_PATH = DEFAULT_PATH+\"/meta_df.csv\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SCORES_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(BENCHMARK_PATH), exist_ok=True)\n",
    "\n",
    "# ========== UTILITIES ==========\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "# ========== STEP 1: Load Data ==========\n",
    "print(\"[INFO] Loading data...\")\n",
    "# Replace this with real OHLCV loading\n",
    "df = OHLCV_DF.copy()#[OHLCV_DF['symbol']==TICKER].copy()\n",
    "\n",
    "\n",
    "# ========== STEP 2: Sample Benchmark Episodes ==========\n",
    "if os.path.exists(BENCHMARK_PATH):\n",
    "    with open(BENCHMARK_PATH) as f:\n",
    "        benchmark_episodes = json.load(f)\n",
    "else:\n",
    "    print(\"[INFO] Sampling benchmark episodes...\")\n",
    "    np.random.seed(0)\n",
    "    benchmark_episodes = sample_valid_episodes(df, TICKER, N_TIMESTEPS, LOOKBACK, EVAL_EPISODES)\n",
    "    with open(BENCHMARK_PATH, \"w\") as f:\n",
    "        json.dump(benchmark_episodes.tolist(), f)  # ← ✅ Convert to list here\n",
    "\n",
    "# ========== STEP 3: Run Learnability Tests ==========\n",
    "meta_records = []\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Running episode from idx {start_idx} with seed {seed}\")\n",
    "\n",
    "        # Prepare Env\n",
    "        env = Monitor(PositionTradingEnvV2(df, TICKER, n_timesteps=N_TIMESTEPS,market_features=[\"price_change\",\"overnight_price_change\",\"volume_change\"], lookback=LOOKBACK, start_idx=start_idx, seed=seed))\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
    "\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO agent\n",
    "        obs,_ = env.reset()\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #done = terminated or truncated\n",
    "            score += reward\n",
    "\n",
    "        # Evaluate random agent\n",
    "        obs= env.reset()\n",
    "        done, rand_score = False, 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #done = terminated or truncated\n",
    "            rand_score += reward\n",
    "\n",
    "        # Calculate advantage\n",
    "        advantage = score - rand_score\n",
    "\n",
    "        # Log meta-data\n",
    "        start_date = df.loc[start_idx, \"date\"]\n",
    "        end_date = df.loc[start_idx + N_TIMESTEPS - 1, \"date\"]\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        meta = extract_meta_features(df.iloc[start_idx: start_idx + N_TIMESTEPS])\n",
    "        meta.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score\": score,\n",
    "            \"rand_score\": rand_score,\n",
    "            \"advantage\": advantage,\n",
    "            \"seed\": seed,\n",
    "            \"ticker\": TICKER,\n",
    "            \"start_date\": str(start_date),\n",
    "            \"end_date\": str(end_date)\n",
    "        })\n",
    "        meta_records.append(meta)\n",
    "\n",
    "# ========== STEP 4: Save Results ==========\n",
    "pd.DataFrame(meta_records).to_csv(META_PATH, index=False)\n",
    "print(\"[INFO] Learnability test complete. Results saved to:\", META_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f63498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.        ,  2.        , -0.01098445,  0.9890156 , -0.01098445,\n",
       "         0.        , -0.01098445,  0.        , -0.05486707,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "        -0.01098445,  0.00424609, -0.14707454], dtype=float32),\n",
       " -0.03316697446510403,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = Monitor(PositionTradingEnvV2(df, TICKER, n_timesteps=N_TIMESTEPS,market_features=[\"price_change\",\"overnight_price_change\",\"volume_change\"], lookback=LOOKBACK, start_idx=start_idx, seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(meta_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_config_hash(config):\n",
    "    raw = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha256(raw.encode()).hexdigest()\n",
    "\n",
    "def save_model(model, config_hash):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"agent_{config_hash}.zip\")\n",
    "    model.save(path)\n",
    "    with open(path.replace(\".zip\", \"_config.json\"), \"w\") as f:\n",
    "        json.dump(config_hash, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] Loading benchmark episodes...\")\n",
    "with open(BENCHMARK_PATH) as f:\n",
    "    benchmark_episodes = json.load(f)\n",
    "\n",
    "meta_records = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    for start_idx in benchmark_episodes:\n",
    "        print(f\"[INFO] Transferability episode: seed={seed}, train_idx={start_idx}\")\n",
    "\n",
    "        # TRAIN on Month T\n",
    "        env_train = Monitor(PositionTradingEnv(df[df['symbol'] ==TICKER].reset_index(), TICKER, N_TIMESTEPS, LOOKBACK, seed=seed, start_idx=start_idx))\n",
    "        model = PPO(\"MlpPolicy\", env_train, verbose=0, seed=seed)\n",
    "        model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "        # Evaluate PPO on Month T\n",
    "        obs, _= env_train.reset()\n",
    "        done, score_train = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env_train.step(action)\n",
    "            score_train += reward\n",
    "\n",
    "        # Random agent on Month T\n",
    "        obs, _ = env_train.reset()\n",
    "        done, rand_train = False, 0\n",
    "        while not done:\n",
    "            action = env_train.action_space.sample()\n",
    "            obs, reward, done, _, _ = env_train.step(action)\n",
    "            rand_train += reward\n",
    "\n",
    "        # TEST on Month T+1\n",
    "        test_idx = start_idx + 60  # approx. one month later\n",
    "        if test_idx + N_TIMESTEPS >= len(df):\n",
    "            print(\"[WARN] Skipping episode — test idx out of range\")\n",
    "            continue\n",
    "\n",
    "        env_test = Monitor(PositionTradingEnv(df[df['symbol'] ==TICKER].reset_index(), TICKER, N_TIMESTEPS, LOOKBACK, seed=seed, start_idx=test_idx))\n",
    "\n",
    "        obs, _ = env_test.reset()\n",
    "        done, score_test = False, 0\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env_test.step(action)\n",
    "            score_test += reward\n",
    "\n",
    "        # Random agent on Month T+1\n",
    "        obs, _ = env_test.reset()\n",
    "        done, rand_test = False, 0\n",
    "        while not done:\n",
    "            action = env_test.action_space.sample()\n",
    "            obs, reward, done, _, _ = env_test.step(action)\n",
    "            rand_test += reward\n",
    "\n",
    "        advantage_train = score_train - rand_train\n",
    "        advantage_test = score_test - rand_test\n",
    "        transfer_delta = score_test - score_train\n",
    "\n",
    "        config = {\n",
    "            \"ticker\": TICKER,\n",
    "            \"train_idx\": int(start_idx),\n",
    "            \"test_idx\": int(test_idx),\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"seed\": seed\n",
    "        }\n",
    "        config_hash = generate_config_hash(config)\n",
    "        save_model(model, config_hash)\n",
    "\n",
    "        features = extract_meta_features(df.iloc[start_idx:start_idx + N_TIMESTEPS])\n",
    "        features.update({\n",
    "            \"config_hash\": config_hash,\n",
    "            \"score_train\": score_train,\n",
    "            \"score_test\": score_test,\n",
    "            \"advantage_train\": advantage_train,\n",
    "            \"advantage_test\": advantage_test,\n",
    "            \"transfer_delta\": transfer_delta,\n",
    "            \"transfer_success\": int(transfer_delta > 0),\n",
    "            \"ticker\": TICKER,\n",
    "            \"seed\": seed\n",
    "        })\n",
    "        meta_records.append(features)\n",
    "\n",
    "pd.DataFrame(meta_records).to_csv(TRANSFER_META_PATH, index=False)\n",
    "print(\"[INFO] Transferability test complete. Results saved to:\", TRANSFER_META_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['symbol']==\"AAPL\"].reset_index().iloc[672 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Union\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example agent registry\n",
    "AGENT_REGISTRY = {\n",
    "    \"ppo\": PPO,\n",
    "    \"a2c\": A2C\n",
    "}\n",
    "\n",
    "def compute_additional_metrics(env):\n",
    "    values = np.array(env.values)\n",
    "    rewards = np.array(env.rewards)\n",
    "    actions = np.array(env.actions)\n",
    "\n",
    "    returns = pd.Series(values).pct_change().dropna()\n",
    "    volatility = returns.std()\n",
    "    entropy = -np.sum(np.bincount(actions, minlength=2)/len(actions) * np.log2(np.bincount(actions, minlength=2)/len(actions) + 1e-9))\n",
    "    max_drawdown = (values / np.maximum.accumulate(values)).min() - 1\n",
    "    sharpe = returns.mean() / (returns.std() + 1e-9) * np.sqrt(252)\n",
    "    sortino = returns.mean() / (returns[returns < 0].std() + 1e-9) * np.sqrt(252)\n",
    "    calmar = returns.mean() / abs(max_drawdown + 1e-9)\n",
    "    success_trades = np.sum((np.diff(values) > 0) & (actions[1:] == 1)) + np.sum((np.diff(values) < 0) & (actions[1:] == 0))\n",
    "\n",
    "    return {\n",
    "        \"volatility\": volatility,\n",
    "        \"entropy\": entropy,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"sortino\": sortino,\n",
    "        \"calmar\": calmar,\n",
    "        \"success_trades\": success_trades,\n",
    "        \"action_hold_ratio\": np.mean(actions == 0),\n",
    "        \"action_long_ratio\": np.mean(actions == 1)\n",
    "    }\n",
    "\n",
    "def formalized_learning_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    ticker: str,\n",
    "    agents: List[str] = [\"ppo\", \"a2c\"],\n",
    "    env_cls: Callable = None,\n",
    "    env_name: str = \"PositionTradingEnv\",\n",
    "    env_version: str = \"v0\",\n",
    "    env_config: Dict = None,\n",
    "    timesteps: int = 10_000,\n",
    "    eval_episodes: int = 10,\n",
    "    n_timesteps: int = 60,\n",
    "    lookback: int = 0,\n",
    "    seed: int = 42,\n",
    "    result_path: str = \"data/eval/ltm_learnability.csv\"\n",
    "):\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    # Load previously completed configs\n",
    "    if os.path.exists(result_path):\n",
    "        past_df = pd.read_csv(result_path)\n",
    "        past_configs = set(past_df['config_hash'].unique())\n",
    "    else:\n",
    "        past_df = pd.DataFrame()\n",
    "        past_configs = set()\n",
    "\n",
    "    # Filter data and sample episodes\n",
    "    df = df[df['symbol'] == ticker].copy().sort_values('date')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    mondays = df[df['date'].dt.weekday == 0]\n",
    "    valid_starts = []\n",
    "    for date in mondays['date']:\n",
    "        start_idx = df.index[df['date'] == date][0]\n",
    "        if start_idx + n_timesteps < len(df):\n",
    "            valid_starts.append(start_idx)\n",
    "    sampled_starts = np.random.default_rng(seed).choice(valid_starts, size=min(eval_episodes, len(valid_starts)), replace=False)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for agent_name in agents:\n",
    "        agent_cls = AGENT_REGISTRY[agent_name]\n",
    "        for start_idx in sampled_starts:\n",
    "            config = {\n",
    "                \"agent\": agent_name,\n",
    "                \"ticker\": ticker,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"timesteps\": timesteps,\n",
    "                \"n_timesteps\": n_timesteps,\n",
    "                \"lookback\": lookback,\n",
    "                \"seed\": seed,\n",
    "                \"env_name\": env_name,\n",
    "                \"env_version\": env_version,\n",
    "                \"env_config\": env_config or {}\n",
    "            }\n",
    "            config_hash = hash(json.dumps(config, sort_keys=True))\n",
    "            if config_hash in past_configs:\n",
    "                continue\n",
    "\n",
    "            env_train = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            env_train = Monitor(env_train)\n",
    "            model = agent_cls(\"MlpPolicy\", env_train, seed=seed, verbose=0)\n",
    "            model.learn(total_timesteps=timesteps)\n",
    "\n",
    "            # Eval PPO\n",
    "            env_eval = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            obs, _ = env_eval.reset()\n",
    "            done, ppo_score = False, 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = env_eval.step(action)\n",
    "                ppo_score += reward\n",
    "\n",
    "            metrics = compute_additional_metrics(env_eval)\n",
    "\n",
    "            # Eval Random\n",
    "            env_rand = env_cls(df, ticker, n_timesteps, lookback, seed=seed, start_idx=start_idx)\n",
    "            obs, _ = env_rand.reset()\n",
    "            done, rand_score = False, 0\n",
    "            while not done:\n",
    "                action = env_rand.action_space.sample()\n",
    "                obs, reward, done, _, _ = env_rand.step(action)\n",
    "                rand_score += reward\n",
    "\n",
    "            row = {\n",
    "                \"config_hash\": config_hash,\n",
    "                \"agent\": agent_name,\n",
    "                \"env_name\": env_name,\n",
    "                \"env_version\": env_version,\n",
    "                \"ticker\": ticker,\n",
    "                \"seed\": seed,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"timesteps\": timesteps,\n",
    "                \"ppo_score\": ppo_score,\n",
    "                \"rand_score\": rand_score,\n",
    "                \"ppo_advantage\": ppo_score - rand_score,\n",
    "                \"ppo_std\": np.std([ppo_score]),\n",
    "                \"rand_std\": np.std([rand_score]),\n",
    "                \"ppo_median\": ppo_score,\n",
    "                \"rand_median\": rand_score,\n",
    "                \"train_start_date\": df.loc[start_idx, 'date'],\n",
    "                \"train_end_date\": df.loc[start_idx + n_timesteps, 'date'],\n",
    "                \"config_json\": json.dumps(config, sort_keys=True),\n",
    "                **metrics\n",
    "            }\n",
    "            all_results.append(row)\n",
    "\n",
    "    if all_results:\n",
    "        new_df = pd.DataFrame(all_results)\n",
    "        combined = pd.concat([past_df, new_df], ignore_index=True)\n",
    "        combined.to_csv(result_path, index=False)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
