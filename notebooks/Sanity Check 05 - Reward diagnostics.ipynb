{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e78cadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "from src.env.step_rewards import reward_sharpe,reward_sortino,reward_drawdown,reward_alpha,reward_cumulative,reward_calmar,reward_hybrid\n",
    "\n",
    "class BaseSequenceAwareTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Flexible RL Trading Env with windowed sequence obs (Transformer/LSTM/MLP-ready).\n",
    "    - Set `return_sequences=True` for (window_length, obs_dim) obs (for transformers).\n",
    "    - Set `return_sequences=False` for flat obs (classic RL, SB3 LSTM/MLP).\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, df, feature_cols=None, reward_fn=None, internal_features=None,\n",
    "        episode_length=100, transaction_cost=0.0001, seed=314, window_length=10, return_sequences=True):\n",
    "        super().__init__()\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols or []\n",
    "        self.internal_features = internal_features or [\n",
    "            \"position\", \"holding_period\", \"cumulative_reward\", \"pct_time\",\n",
    "            \"drawdown\", \"rel_perf\", \"unrealized_pnl\", \"entry_price\", \"time_in_position\"\n",
    "        ]\n",
    "        self.obs_dim = len(self.feature_cols) + len(self.internal_features)\n",
    "        self.episode_length = episode_length +2\n",
    "        self.window_length = max(1, window_length)\n",
    "        self.return_sequences = return_sequences  # True: (window, obs_dim), False: flat\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.seed = seed\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        counts = df['symbol'].value_counts()\n",
    "        eligible = counts[counts >= episode_length].index\n",
    "        self.stocks = df[df['symbol'].isin(eligible)]['symbol'].unique()\n",
    "        self.episode_df = df.copy()\n",
    "        self.reward_fn = reward_fn or self.default_reward_fn\n",
    "\n",
    "        # Set observation space\n",
    "        if self.return_sequences:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length, self.obs_dim), dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=-np.inf, high=np.inf,\n",
    "                shape=(self.window_length * self.obs_dim,), dtype=np.float32\n",
    "            )\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "    \n",
    "    def get_current_regime(self):\n",
    "        \"\"\"\n",
    "        Detects current regime based on recent return volatility and trend.\n",
    "        Returns:\n",
    "            0 = Bull, 1 = Bear, 2 = Sideways\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_length:\n",
    "            return 2  # Not enough data, assume sideways\n",
    "\n",
    "        # Use recent price changes to detect regime\n",
    "        returns = self.episode_df['return_1d'].iloc[self.current_step - self.window_length:self.current_step].values\n",
    "        mean_return = returns.mean()\n",
    "        std_return = returns.std()\n",
    "\n",
    "        # Thresholds can be tuned\n",
    "        if mean_return > 0.001 and std_return < 0.01:\n",
    "            return 0  # Bull\n",
    "        elif mean_return < -0.001 and std_return < 0.01:\n",
    "            return 1  # Bear\n",
    "        else:\n",
    "            return 2  # Sideways\n",
    "        \n",
    "    def default_reward_fn(self, position, price_change, **kwargs):\n",
    "        return position * price_change\n",
    "\n",
    "    def set_episode_sequence(self, sequence):\n",
    "        self.episode_sequence = sequence\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def generate_episode_sequences_v1(self, train_steps=10000):\n",
    "        dataset_length = len(self.df)\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        ticker = self.df['symbol'].unique()[0]\n",
    "        min_start = 0\n",
    "        max_start = dataset_length - self.episode_length - 2\n",
    "        for i in range(episodes):\n",
    "            episode_sequences.append((ticker, np.random.randint(0, max_start)))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def generate_episode_sequences(self, train_steps=10000):\n",
    "        episodes = int(train_steps / self.episode_length) + 1\n",
    "        episode_sequences = []\n",
    "        for _ in range(episodes):\n",
    "            ticker = np.random.choice(self.stocks)\n",
    "            stock_df = self.df[self.df['symbol'] == ticker].reset_index(drop=True)\n",
    "            max_start = len(stock_df) - self.episode_length - 2\n",
    "            if max_start <= 0:\n",
    "                continue  # skip if not enough data\n",
    "            start = np.random.randint(0, max_start)\n",
    "            episode_sequences.append((ticker, start))\n",
    "        np.random.shuffle(episode_sequences)\n",
    "        return episode_sequences\n",
    "\n",
    "    def reset(self, seed=None, options=None, start_index=None):\n",
    "        self.entry_step = None\n",
    "        self.unrealized_pnl = 0\n",
    "        self.relative_perf = 0\n",
    "        self.drawdown = 0\n",
    "        self.time_in_position = 0\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            \n",
    "            \n",
    "        symbol, start_idx = self.episode_sequence[self.episode_counter]\n",
    "        #print(symbol,start_idx,self.df['symbol'].unique())\n",
    "        symbol_df = self.df[self.df['symbol'] == symbol].reset_index(drop=True)\n",
    "        #print(len(symbol_df))\n",
    "        \n",
    "        if start_idx + self.episode_length > len(symbol_df):\n",
    "            print(f\"[WARN] Episode too short for {symbol} at {start_idx}, skipping...\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()  # tenta o próximo episódio\n",
    "\n",
    "        # ✅ Extração segura\n",
    "        #self.episode_df = symbol_df.iloc[start_idx : start_idx + self.episode_length].copy()\n",
    "        end = start_idx + self.episode_length + 1\n",
    "        if end > len(symbol_df):\n",
    "            print(f\"[WARN] Not enough data for {symbol} from {start_idx}, skipping.\")\n",
    "            self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "            return self.reset()\n",
    "        self.episode_df = symbol_df.iloc[start_idx:end].copy()\n",
    "        \n",
    "\n",
    "        # Move to next episode (with wrap-around)\n",
    "        self.episode_counter = (self.episode_counter + 1) % len(self.episode_sequence)\n",
    "        \"\"\"\n",
    "        for _ in range(10):  # Try up to 10 times to get a valid episode\n",
    "            stock = self.stocks[0]\n",
    "            if hasattr(self, \"episode_sequence\"):\n",
    "                if self.episode_counter >= len(self.episode_sequence):\n",
    "                    self.episode_counter = 0\n",
    "                _, start = self.episode_sequence[self.episode_counter]\n",
    "                self.episode_counter += 1\n",
    "            else:\n",
    "                stock = np.random.choice(self.stocks)\n",
    "                stock_df = self.df[self.df['symbol'] == stock].reset_index(drop=True)\n",
    "                max_start = len(stock_df) - self.episode_length\n",
    "                if max_start <= 0:\n",
    "                    continue  # Try another stock\n",
    "                start = np.random.randint(0, max_start + 1)\n",
    "\n",
    "            self.stock = stock\n",
    "            stock_df = self.df[self.df['symbol'] == self.stock].reset_index(drop=True)\n",
    "            self.episode_df = stock_df.iloc[int(start):int(start) + int(self.episode_length + 2)].reset_index(drop=True)\n",
    "\n",
    "            if len(self.episode_df) >= self.window_length:\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"Failed to sample a valid episode with sufficient data.\")\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.entry_price = None\n",
    "        self.position = 0\n",
    "        self.holding_period = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.returns_history = []\n",
    "        self.reward_history = []\n",
    "        self.episode_pct_changes = self.episode_df['return_1d'].values\n",
    "        self.max_possible_reward = np.sum(np.abs(self.episode_pct_changes))\n",
    "        self.current_wealth = 1.0\n",
    "        self.peak_wealth = 1.0\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Returns a rolling window of observations (2D or flattened)\n",
    "        obs_list = []\n",
    "        #for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "        #    idx = max(i, 0)  # pad with earliest available step\n",
    "        #    features = self.episode_df.iloc[idx][self.feature_cols].values.astype(np.float32)\n",
    "        for i in range(self.current_step - self.window_length + 1, self.current_step + 1):\n",
    "            if 0 <= i < len(self.episode_df):\n",
    "                features = self.episode_df.iloc[i][self.feature_cols].values.astype(np.float32)\n",
    "            else:\n",
    "                features = np.zeros(len(self.feature_cols), dtype=np.float32)  # zero padding\n",
    "            internal_state = {\n",
    "                \"position\": self.position,\n",
    "                \"holding_period\": self.holding_period,\n",
    "                \"cumulative_reward\": self.cumulative_reward,\n",
    "                \"pct_time\": self.current_step / self.episode_length,\n",
    "                \"drawdown\": self.drawdown,\n",
    "                \"rel_perf\": self.relative_perf,\n",
    "                \"unrealized_pnl\": self.unrealized_pnl,\n",
    "                \"entry_price\": self.entry_price if self.entry_price is not None else 0.0,\n",
    "                \"time_in_position\": self.time_in_position,\n",
    "            }\n",
    "            internal = np.array([internal_state[name] for name in self.internal_features], dtype=np.float32)\n",
    "            obs = np.concatenate([features, internal])\n",
    "            obs_list.append(obs)\n",
    "        obs_window = np.stack(obs_list)  # shape: (window_length, obs_dim)\n",
    "        if self.return_sequences:\n",
    "            return obs_window  # shape: (window_length, obs_dim)\n",
    "        else:\n",
    "            return obs_window.flatten()  # shape: (window_length * obs_dim,)\n",
    "        \n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        #print(self.current_step,self.episode_length,len(self.episode_df))\n",
    "        done = self.current_step >= self.episode_length - 1\n",
    "        current_row = self.episode_df.iloc[self.current_step]\n",
    "\n",
    "        # Protege contra acesso fora dos limites\n",
    "        if self.current_step + 1 < len(self.episode_df):\n",
    "            next_row = self.episode_df.iloc[self.current_step + 1]\n",
    "        else:\n",
    "            next_row = current_row.copy()  # fallback seguro\n",
    "\n",
    "        price_change = next_row['return_1d']\n",
    "        prev_position = self.position\n",
    "        reward = 0\n",
    "        cost = 0\n",
    "\n",
    "        self.action_counts[action] += 1\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if self.position != 1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = 1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if self.position != -1:\n",
    "                if self.position != 0:\n",
    "                    cost = self.transaction_cost\n",
    "                self.position = -1\n",
    "                self.holding_period = 0\n",
    "                self.entry_price = current_row['close']\n",
    "                self.entry_step = self.current_step\n",
    "\n",
    "        if self.position != 0:\n",
    "            self.holding_period += 1\n",
    "\n",
    "        step_return = self.position * price_change\n",
    "        self.returns_history.append(step_return)\n",
    "        self.current_wealth *= (1 + step_return)\n",
    "        if self.current_wealth > self.peak_wealth:\n",
    "            self.peak_wealth = self.current_wealth\n",
    "        self.drawdown = 1 - self.current_wealth / self.peak_wealth\n",
    "\n",
    "        if self.position != 0 and self.entry_price is not None:\n",
    "            current_price = next_row['close']\n",
    "            self.unrealized_pnl = (current_price - self.entry_price) * self.position / self.entry_price\n",
    "            self.time_in_position = self.current_step - self.entry_step\n",
    "        else:\n",
    "            self.unrealized_pnl = 0\n",
    "            self.time_in_position = 0\n",
    "\n",
    "        if 'market_return_1d' in self.episode_df.columns:\n",
    "            self.relative_perf = price_change - next_row['market_return_1d']\n",
    "        else:\n",
    "            self.relative_perf = 0\n",
    "\n",
    "        reward = self.reward_fn(\n",
    "            position=self.position,\n",
    "            price_change=price_change,\n",
    "            prev_position=prev_position,\n",
    "            env=self\n",
    "        )\n",
    "        reward -= cost\n",
    "        self.reward_history.append(reward)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        self.current_step += 1\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        info[\"regime\"] = self.get_current_regime()\n",
    "\n",
    "        # Calcula métricas no final do episódio\n",
    "        if done:\n",
    "            returns = np.array(self.returns_history)\n",
    "            mean = np.median(returns) if len(returns) > 0 else np.nan\n",
    "            std = returns.std() if len(returns) > 1 else np.nan\n",
    "            downside = returns[returns < 0]\n",
    "            downside_std = downside.std() if len(downside) > 1 else np.nan\n",
    "\n",
    "            sharpe = mean / std if (std is not None and std > 0 and not np.isnan(std)) else np.nan\n",
    "            sortino = mean / downside_std if (downside_std is not None and downside_std > 0 and not np.isnan(downside_std)) else np.nan\n",
    "\n",
    "            wealth_curve = np.cumprod(1 + returns) if len(returns) > 0 else np.array([])\n",
    "            peak_wealth = np.maximum.accumulate(wealth_curve) if len(wealth_curve) > 0 else np.array([])\n",
    "            drawdowns = (wealth_curve - peak_wealth) / (peak_wealth + 1e-8) if len(wealth_curve) > 0 else np.array([])\n",
    "            max_drawdown = np.abs(drawdowns.min()) if len(drawdowns) > 0 else np.nan\n",
    "            calmar = ((wealth_curve[-1] - 1) / max_drawdown) if (len(wealth_curve) > 0 and max_drawdown and not np.isnan(max_drawdown) and max_drawdown > 0) else np.nan\n",
    "            cum_return = wealth_curve[-1] - 1 if len(wealth_curve) > 0 else np.nan\n",
    "            final_wealth = wealth_curve[-1] if len(wealth_curve) > 0 else np.nan\n",
    "\n",
    "            # Trade-level metrics\n",
    "            trades = []\n",
    "            trade_profits = []\n",
    "            prev = 0\n",
    "            for i, ret in enumerate(returns):\n",
    "                if prev == 0 and ret != 0:\n",
    "                    entry_idx = i\n",
    "                    entry_dir = np.sign(ret)\n",
    "                elif prev != 0 and (ret == 0 or np.sign(ret) != np.sign(prev)):\n",
    "                    if 'entry_idx' in locals():\n",
    "                        trade = returns[entry_idx:i+1]\n",
    "                        trade_profits.append(np.sum(trade))\n",
    "                        del entry_idx\n",
    "                prev = ret\n",
    "            win_rate = np.median(np.array(trade_profits) > 0) if trade_profits else np.nan\n",
    "\n",
    "            if 'market_return_1d' in self.episode_df.columns:\n",
    "                market_returns = self.episode_df['market_return_1d'].values[1:self.episode_length]\n",
    "                market_wealth_curve = np.cumprod(1 + market_returns) if len(market_returns) > 0 else np.array([])\n",
    "                market_cum_return = market_wealth_curve[-1] - 1 if len(market_wealth_curve) > 0 else np.nan\n",
    "                alpha = cum_return - market_cum_return if cum_return is not None and not np.isnan(cum_return) and market_cum_return is not None and not np.isnan(market_cum_return) else np.nan\n",
    "            else:\n",
    "                alpha = np.nan\n",
    "\n",
    "            info.update({\n",
    "                \"episode_sharpe\": sharpe,\n",
    "                \"episode_sortino\": sortino,\n",
    "                \"rewards_history\": np.array(self.reward_history),\n",
    "                \"episode_total_reward\": np.sum(self.reward_history) if len(self.reward_history) > 0 else np.nan,\n",
    "                \"cumulative_return\": cum_return,\n",
    "                \"calmar\": calmar,\n",
    "                \"max_drawdown\": max_drawdown,\n",
    "                \"win_rate\": win_rate,\n",
    "                \"alpha\": alpha,\n",
    "                \"returns\": returns,\n",
    "                \"market_returns\": market_returns if 'market_returns' in locals() else [],\n",
    "                \"downside\": downside,\n",
    "                \"regime\": self.get_current_regime(),\n",
    "                \"final_wealth\": final_wealth,\n",
    "                \"action_hold_count\": self.action_counts[0],\n",
    "                \"action_buy_count\": self.action_counts[1],\n",
    "                \"action_sell_count\": self.action_counts[2]\n",
    "            })\n",
    "\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step} | Pos: {self.position} | Hold: {self.holding_period} | CumRew: {self.cumulative_reward:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "class SequenceAwareSharpeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sharpe, **kwargs)\n",
    "\n",
    "class SequenceAwareSortinoTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_sortino, **kwargs)\n",
    "\n",
    "class SequenceAwareAlphaTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_alpha, **kwargs)\n",
    "\n",
    "class SequenceAwareDrawdownTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_drawdown, **kwargs)\n",
    "\n",
    "class SequenceAwareCumulativeTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_cumulative, **kwargs)\n",
    "\n",
    "class SequenceAwareCalmarTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_calmar, **kwargs)\n",
    "\n",
    "class SequenceAwareHybridTradingEnv(BaseSequenceAwareTradingEnv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, reward_fn=reward_hybrid, **kwargs)\n",
    "\n",
    "class SequenceAwareBaselineTradingAgent:\n",
    "    def __init__(self,df,feature_cols=[],\n",
    "            episode_length=100, seed=314,set_episode_sequence=[]):\n",
    "    \n",
    "        self.env = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols,\n",
    "            episode_length=episode_length, seed=seed)\n",
    "        self.env.set_episode_sequence(set_episode_sequence)\n",
    "        \n",
    "    def predict(self,obs,*args,**kwargs):\n",
    "        #print(self.env.stocks,'xxxxxxxxxxx')\n",
    "        return self.env.action_space.sample(),{}\n",
    "    \n",
    "    def set_episode_sequence(self,seq):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # should return 0, 1, or 2\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38331a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime Augmentation Wrapper ===========================\n",
    "class RegimeAugmentingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.regime_dim = 3  # One-hot: bull, bear, sideways\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_shape[0], obs_shape[1] + self.regime_dim),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        regime = self.env.get_current_regime()  # 0,1,2 -> bull,bear,sideways\n",
    "        one_hot = np.zeros(self.regime_dim)\n",
    "        one_hot[regime] = 1.0\n",
    "        one_hot = np.repeat(one_hot[None, :], obs.shape[0], axis=0)\n",
    "        return np.concatenate([obs, one_hot], axis=-1)\n",
    "\n",
    "class PerEpisodeRewardNormalizer(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_rewards = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean = np.mean(self.episode_rewards)\n",
    "        std = np.std(self.episode_rewards) + 1e-8\n",
    "        norm_reward = (reward - mean) / std\n",
    "        return obs, norm_reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9dca395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketVersusWalletHistoryTracker:\n",
    "    def __init__(self, initial_wallet=1.0):\n",
    "        self.wallet_value = initial_wallet\n",
    "        self.prev_wallet_value = initial_wallet\n",
    "        self.wallet_locked = False\n",
    "        self.buy_price = None\n",
    "        self.market_entry_price = None\n",
    "        self.last_price = None\n",
    "        self.has_opened_position = False  # NEW: ensure proper update after first buy\n",
    "\n",
    "        self.wallet_history = []\n",
    "        self.market_history = []\n",
    "        self.price_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset(self, initial_price):\n",
    "        self.__init__(initial_wallet=1.0)\n",
    "        self.market_entry_price = initial_price\n",
    "        self.last_price = initial_price\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.market_history.append(1.0)\n",
    "        self.price_history.append(initial_price)\n",
    "        self.action_history.append(0)\n",
    "\n",
    "    def step(self, action, current_price):\n",
    "        self.price_history.append(current_price)\n",
    "        agent_action = 0\n",
    "\n",
    "        # === 1. Update market benchmark ===\n",
    "        market_perf = current_price / self.market_entry_price\n",
    "        self.market_history.append(market_perf)\n",
    "\n",
    "        # === 2. Update wallet value ===\n",
    "        if self.wallet_locked and self.has_opened_position:\n",
    "            self.wallet_value *= current_price / self.last_price\n",
    "\n",
    "        self.wallet_history.append(self.wallet_value)\n",
    "        self.prev_wallet_value = self.wallet_value\n",
    "        self.last_price = current_price  # must be set after wallet update!\n",
    "\n",
    "        # === 3. Process Action ===\n",
    "        if action == 1 and not self.wallet_locked:\n",
    "            self.buy_price = current_price\n",
    "            self.wallet_locked = True\n",
    "            self.has_opened_position = True\n",
    "            agent_action = 1\n",
    "\n",
    "        elif action == 2 and self.wallet_locked:\n",
    "            self.wallet_locked = False\n",
    "            self.buy_price = None\n",
    "            self.has_opened_position = False\n",
    "            agent_action = 2\n",
    "\n",
    "        self.action_history.append(agent_action)\n",
    "\n",
    "    def export(self):\n",
    "        return {\n",
    "            \"wallet_history\": self.wallet_history,\n",
    "            \"market_history\": self.market_history,\n",
    "            \"market_price_history\": self.price_history,\n",
    "            \"performed_action_history\": self.action_history\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Agent:   9%|▉         | 183/1961 [02:22<24:55,  1.19it/s]"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "#from src.env.base_timeseries_trading_env import SequenceAwareCumulativeTradingEnv\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "#from src.env.base_timeseries_trading_env import RegimeAugmentingWrapper\n",
    "TOTAL_TIMESTEPS=200000\n",
    "ENV_CLASS=SequenceAwareCumulativeTradingEnv\n",
    "# === Config ===\n",
    "N_EVAL_EPISODES = 5\n",
    "\n",
    "\n",
    "# === Load Data ===\n",
    "ohlcv_df = load_base_dataframe()\n",
    "test_df = ohlcv_df[(ohlcv_df['date'] >= \"2023-07-01\") & (ohlcv_df['date'] < \"2024-01-01\")]\n",
    "test_df = test_df[test_df['symbol'].isin(TOP2_STOCK_BY_SECTOR)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "# Train on few episodes to prove a point only\n",
    "\n",
    "test_seq = test_env.generate_episode_sequences(int(TOTAL_TIMESTEPS))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_random_agent( env, n_episodes=22):\n",
    "    episode_metrics = []\n",
    "    episode_infos = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating Agent\"):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        state = None\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        # === Tracker setup ===\n",
    "        tracker = MarketVersusWalletHistoryTracker()\n",
    "        initial_price = env.env.env.episode_df.iloc[0]['close']\n",
    "        tracker.reset(initial_price)\n",
    "        initial_symbol =env.env.env.episode_df.iloc[0]['symbol']\n",
    "        #print(initial_symbol,env.env.env.episode_df.iloc[0]['date'],len(env.env.env.episode_df),env.env.env.episode_counter,env.env.env.episode_sequence)\n",
    "        while not done:\n",
    "            action=  env.action_space.sample()\n",
    "            action = int(action)\n",
    "            current_price = env.env.env.episode_df.iloc[env.env.env.current_step]['close']\n",
    "            current_symbol = env.env.env.episode_df.iloc[env.env.env.current_step]['symbol']\n",
    "            if(current_symbol != initial_symbol):\n",
    "                print('EPISODE SWITCHED', initial_symbol,current_symbol)\n",
    "                initial_symbol = current_symbol\n",
    "            # Step the wallet tracker\n",
    "            tracker.step(action, current_price)\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "        #print(current_symbol)\n",
    "        # === Episode summary ===\n",
    "        _env = env.env.env\n",
    "        agent_wealth = infos[-1].get(\"final_wealth\", np.nan)\n",
    "        market_wealth = np.prod(1 + _env.episode_df['market_return_1d'].values)\n",
    "        alpha = agent_wealth - market_wealth\n",
    "\n",
    "        metrics = {\n",
    "            \"total_reward\": np.sum(rewards),\n",
    "            \"final_wealth\": agent_wealth,\n",
    "            \"market_wealth\": market_wealth,\n",
    "            \"calmar\": infos[-1].get(\"calmar\", np.nan),\n",
    "            \"sharpe\": infos[-1].get(\"episode_sharpe\", np.nan),\n",
    "            \"sortino\": infos[-1].get(\"episode_sortino\", np.nan),\n",
    "            \"alpha\": alpha,\n",
    "            \"episode_id\": _env.episode_counter,\n",
    "            \"regime\": infos[-1].get(\"regime\", np.nan)\n",
    "        }\n",
    "\n",
    "        tracker_data = tracker.export()\n",
    "        info[\"ticker\"] = _env.episode_df.iloc[0]['symbol']\n",
    "        info[\"wallet_history\"] = tracker_data[\"wallet_history\"]\n",
    "        info[\"market_history\"] = tracker_data[\"market_history\"]\n",
    "        info[\"market_price_history\"] = tracker_data[\"market_price_history\"]\n",
    "        info[\"performed_action_history\"] = tracker_data[\"performed_action_history\"]\n",
    "        episode_infos.append(info)\n",
    "        episode_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(episode_metrics), episode_infos\n",
    "\n",
    "\n",
    "\n",
    "def make_test_env():\n",
    "    eval_env =ENV_CLASS(test_df, episode_length=EPISODE_LENGTH, feature_cols=FEATURE_COLS)\n",
    "    eval_env.set_episode_sequence(test_seq)\n",
    "    return PerEpisodeRewardNormalizer(RegimeAugmentingWrapper(eval_env)) #RegimeAugmentingWrapper(ENV_CLASS(test_df, episode_length=EPISODE_LENGTH,feature_cols=FEATURE_COLS))\n",
    "\n",
    "\n",
    "\n",
    "random_agent_df, random_agent_infos = evaluate_random_agent(make_test_env(), n_episodes=len(test_seq))\n",
    "#random_agent_df, random_agent_infos = evaluate_random_agent(make_test_env(), n_episodes=5)\n",
    "\n",
    "#ppo_agent_df[\"agent\"] = \"recurrent_ppo\"\n",
    "random_agent_df[\"agent\"] = \"random\"\n",
    "#results_df = pd.concat([ppo_agent_df, random_agent_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40888291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWARD ANALYSIS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick one episode\n",
    "info = random_agent_infos[0]  # or any i\n",
    "rewards = info[\"episode_total_reward\"]\n",
    "returns = info[\"returns\"]\n",
    "cum_rewards = np.cumsum(info[\"returns\"] * info[\"performed_action_history\"][1:])  # or info['reward_history'] if saved\n",
    "cum_returns = np.cumprod(1 + np.array(returns)) - 1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cum_returns, label=\"Cumulative Return\")\n",
    "plt.plot(np.cumsum(info[\"rewards_history\"]), label=\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Return vs Reward\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(np.cumsum(info[\"rewards_history\"]), cum_returns)\n",
    "plt.xlabel(\"Cumulative Reward\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.title(\"Reward vs Return Correlation\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "return_list = []\n",
    "alpha_list = []\n",
    "\n",
    "for info in random_agent_infos:\n",
    "    reward_list.append(np.sum(info[\"rewards_history\"]))\n",
    "    return_list.append(info[\"final_wealth\"] - 1)\n",
    "    alpha_list.append(info[\"alpha\"])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"reward\": reward_list,\n",
    "    \"return\": return_list,\n",
    "    \"alpha\": alpha_list\n",
    "})\n",
    "\n",
    "print(df.corr())\n",
    "import seaborn as sns\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869898ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec947fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
