{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": [
    "Data splits (walk-forward, same stocks/timeframes)\n",
    "\n",
    "Features (identical input sets)\n",
    "\n",
    "Reward (cumulative)\n",
    "\n",
    "Evaluation (out-of-sample EWM, Sharpe, etc.)\n",
    "\n",
    "\n",
    "That’s exactly the study I’d dream of running at this stage!\n",
    "Let’s set up a true **apples-to-apples benchmark** of agent architectures on our trading pipeline, using the exact same:\n",
    "\n",
    "* Data splits (walk-forward, same stocks/timeframes)\n",
    "* Features (identical input sets)\n",
    "* Reward (cumulative)\n",
    "* Evaluation (out-of-sample EWM, Sharpe, etc.)\n",
    "\n",
    "I’ll outline the practical plan below and can deliver full code scaffolding for each model if you want to go straight to implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Agent Architecture Benchmark Plan**\n",
    "\n",
    "### **1. Baseline PPO-MLP**\n",
    "\n",
    "* **Policy:** Standard multilayer perceptron (MLP)\n",
    "* **Library:** Stable Baselines3 (`PPO`)\n",
    "* **Policy kwargs:** e.g., `[128, 128]` or `[256, 128]`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. LSTM PPO (RecurrentPPO)**\n",
    "\n",
    "* **Policy:** LSTM (single-layer or 2-layer, 128 units)\n",
    "* **Library:** Stable Baselines3-Contrib (`RecurrentPPO`)\n",
    "* **Policy:** `\"MlpLstmPolicy\"`\n",
    "* **Handles sequences natively**\n",
    "* **Extra: Tune sequence/episode length for best results**\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Single-Head Attention Transformer Policy**\n",
    "\n",
    "* **Policy:** Transformer encoder with 1 attention head (minimalist setup)\n",
    "* **Implementation:**\n",
    "\n",
    "  * *Option 1*: Use `stable-baselines3` with a custom policy class (PyTorch).\n",
    "  * *Option 2*: Use SB3 fork/extensions that support transformer policies out-of-the-box (less common; will probably need custom code).\n",
    "* **Goal:** Test transformer’s “pattern memory” edge vs LSTM.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Multi-Head Attention Transformer Policy**\n",
    "\n",
    "* **Policy:** Transformer encoder, e.g., 4–8 heads, 1–2 layers\n",
    "* **Implementation:** Same as above but with multiple heads\n",
    "* **Why:** See if more heads/layers boost performance (at higher compute cost)\n",
    "\n",
    "---\n",
    "\n",
    "## **Benchmarking Protocol**\n",
    "\n",
    "1. **Data**: Use our best meta-selected stocks/timeframes, identical for all runs.\n",
    "2. **Feature set**: Fix features for all models (no advantage to one or another).\n",
    "3. **Hyperparameters**: Tune as fairly as possible (similar total params, same optimizer, batch size, episode length).\n",
    "4. **Evaluation**:\n",
    "\n",
    "   * Out-of-sample EWM cumulative reward\n",
    "   * Sharpe ratio, drawdown, % > market\n",
    "   * Policy entropy, if curious\n",
    "   * 5+ random seeds per setting\n",
    "5. **Logging**: Use Weights & Biases, MLflow, or simple CSVs to compare runs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementation Plan**\n",
    "\n",
    "**A. Write/Adapt Custom Policies**\n",
    "\n",
    "* For LSTM: use `RecurrentPPO` (easy).\n",
    "* For Transformers: extend SB3’s `ActorCriticPolicy` using PyTorch, plug in transformer blocks.\n",
    "\n",
    "**B. Standardized Training Loop**\n",
    "\n",
    "* For each agent: loop over all stocks/timeframes, train, evaluate, record metrics.\n",
    "\n",
    "**C. Result Table**\n",
    "\n",
    "| Model           | Architecture     | Params | Mean EWM Reward | Sharpe | % > Market | Notes       |\n",
    "| --------------- | ---------------- | ------ | --------------- | ------ | ---------- | ----------- |\n",
    "| PPO-MLP         | \\[256,128] MLP   | X      | ...             | ...    | ...        | Baseline    |\n",
    "| PPO-LSTM        | 1x128 LSTM       | Y      | ...             | ...    | ...        | Recurrent   |\n",
    "| PPO-Transformer | 1-head, 1 layer  | Z      | ...             | ...    | ...        | Single head |\n",
    "| PPO-Transformer | 4-head, 2 layers | W      | ...             | ...    | ...        | Multi-head  |\n",
    "\n",
    "---\n",
    "\n",
    "## **Deliverables**\n",
    "\n",
    "* **Scripts for each model type** (ready to run)\n",
    "* **Unified training and eval pipeline** (for apples-to-apples comparison)\n",
    "* **Benchmarking notebooks** for quick result viz\n",
    "* **Markdown summary template** for documentation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco Sá\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# SETUP: Imports & Paths ===========================\n",
    "import jupyter\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.feature_pipeline import basic_chart_features,load_base_dataframe\n",
    "from src.predictability.easiness import rolling_sharpe, rolling_r2, rolling_info_ratio, rolling_autocorr\n",
    "from src.predictability.pipeline import generate_universe_easiness_report\n",
    "from IPython import display\n",
    "\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.config import TOP2_STOCK_BY_SECTOR\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import acf, acovf\n",
    "\n",
    "from src.env.base_trading_env import (\n",
    "    CumulativeTradingEnv,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== IMPORTS & SETUP ==========\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.env.base_trading_env import CumulativeTradingEnv\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.defaults import FEATURE_COLS, EPISODE_LENGTH,EXCLUDED_TICKERS\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"__agent_design_and_benchmark\"\n",
    "\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\":EPISODE_LENGTH, \n",
    "    \"n_steps\":EPISODE_LENGTH*3,\n",
    "    \"total_timesteps\": 1000\n",
    "}\n",
    "\n",
    "# ========== DATA LOAD ==========\n",
    "ohlcv_df = load_base_dataframe()\n",
    "ohlcv_df['date'] = pd.to_datetime(ohlcv_df['date'])\n",
    "# Adjust date range as needed!\n",
    "ohlcv_df['month'] = ohlcv_df['date'].dt.to_period('M')\n",
    "\n",
    "TICKERS = ohlcv_df['symbol'].unique()\n",
    "TICKERS = TICKERS[~np.isin(TICKERS, EXCLUDED_TICKERS)]\n",
    "TICKERS = TOP2_STOCK_BY_SECTOR[:2]\n",
    "# ========== TRACKER ==========\n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "\n",
    "# ========== ENV FACTORY ==========\n",
    "def make_env(df, ticker, feature_cols, episode_length):\n",
    "    df_ticker = df[df['symbol'] == ticker].copy()\n",
    "    env = CumulativeTradingEnv(\n",
    "        df=df_ticker,\n",
    "        feature_cols=feature_cols,\n",
    "        episode_length=episode_length,\n",
    "        transaction_cost=TRANSACTION_COST,\n",
    "        #reward_fn=None,  # use env default\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# ========== BAREBONES TRANSFORMER POLICY ==========\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class TransformerExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, d_model=32, nhead=1, num_layers=1):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.embedding = nn.Linear(observation_space.shape[0], d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    def forward(self, obs):\n",
    "        x = self.embedding(obs)\n",
    "        x = x.unsqueeze(0)  # (seq=1, batch, d_model)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(0)\n",
    "        return x\n",
    "\n",
    "class TransformerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, nhead=1, num_layers=1, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=TransformerExtractor,\n",
    "            features_extractor_kwargs={'d_model': 32, 'nhead': nhead, 'num_layers': num_layers},\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "def is_scalar_series(series):\n",
    "    return series.apply(lambda x: np.isscalar(x) or (isinstance(x, (np.floating, np.integer)))).all()\n",
    "\n",
    "def evaluate_agent(model, env, n_episodes=10):\n",
    "    all_infos = []\n",
    "    all_actions = []\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_actions = []\n",
    "        info = {}\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_actions.append(int(action))\n",
    "        all_infos.append(info)\n",
    "        all_actions.extend(episode_actions)\n",
    "\n",
    "    infos_df = pd.DataFrame(all_infos)\n",
    "    # Only keep scalar columns\n",
    "    scalar_cols = [col for col in infos_df.columns if is_scalar_series(infos_df[col])]\n",
    "    metrics = {f\"mean_{col}\": infos_df[col].median() for col in scalar_cols}\n",
    "    metrics.update({f\"std_{col}\": infos_df[col].std() for col in scalar_cols})\n",
    "\n",
    "    # Optionally: record the *first* (or last) value of array-valued metrics for inspection\n",
    "    array_cols = [col for col in infos_df.columns if col not in scalar_cols]\n",
    "    for col in array_cols:\n",
    "        metrics[f\"{col}_sample\"] = infos_df[col].iloc[0]  # Or another summary\n",
    "\n",
    "    # Action breakdown and entropy\n",
    "    action_counts = pd.Series(all_actions).value_counts(normalize=True).to_dict()\n",
    "    metrics[\"action_counts\"] = action_counts\n",
    "    metrics[\"action_entropy\"] = -sum(p * np.log(p + 1e-8) for p in action_counts.values())\n",
    "    return metrics\n",
    "\n",
    "# --- MAIN BENCHMARK LOOP ---\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "if os.path.exists(RESULTS_PATH):\n",
    "    results_df = pd.read_csv(RESULTS_PATH)\n",
    "    done_keys = set(zip(results_df['ticker'], results_df['agent'], results_df['seed']))\n",
    "    results = results_df.to_dict('records')\n",
    "    print(f\"Loaded {len(done_keys)} previously completed results.\")\n",
    "else:\n",
    "    done_keys = set()\n",
    "    results = []\n",
    "\n",
    "for ticker in tqdm(TICKERS, desc=\"Tickers\"):\n",
    "    df_ticker = ohlcv_df[ohlcv_df['symbol'] == ticker].copy()\n",
    "    env = make_env(ohlcv_df, ticker, FEATURE_COLS, EPISODE_LENGTH)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    for agent_type in AGENT_TYPES:\n",
    "        for seed in range(N_SEEDS):\n",
    "            key = (ticker, agent_type, seed)\n",
    "            if key in done_keys:\n",
    "                print(f\"Skipping already completed: {key}\")\n",
    "                continue\n",
    "                \n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if agent_type == 'mlp':\n",
    "                model = PPO(\n",
    "                    \"MlpPolicy\", vec_env,\n",
    "                    verbose=0, seed=seed,\n",
    "                    batch_size=CONFIG[\"batch_size\"], n_steps=CONFIG['n_steps']\n",
    "                )\n",
    "            elif agent_type == 'lstm':\n",
    "                model = RecurrentPPO(\n",
    "                    \"MlpLstmPolicy\", vec_env,\n",
    "                    verbose=0, seed=seed,\n",
    "                    batch_size=CONFIG[\"batch_size\"], n_steps=CONFIG['n_steps']\n",
    "                )\n",
    "            elif agent_type == 'transformer_single':\n",
    "                model = PPO(\n",
    "                    TransformerPolicy, vec_env,\n",
    "                    policy_kwargs={'nhead': 1, 'num_layers': 1},\n",
    "                    verbose=0, seed=seed,\n",
    "                    batch_size=CONFIG[\"batch_size\"], n_steps=CONFIG['n_steps']\n",
    "                )\n",
    "            elif agent_type == 'transformer_multi':\n",
    "                model = PPO(\n",
    "                    TransformerPolicy, vec_env,\n",
    "                    policy_kwargs={'nhead': 4, 'num_layers': 2},\n",
    "                    verbose=0, seed=seed,\n",
    "                    batch_size=CONFIG[\"batch_size\"], n_steps=CONFIG['n_steps']\n",
    "                )\n",
    "            key = (ticker, agent_type, seed)\n",
    "            if key in done_keys:\n",
    "                print(f\"Skipping already completed: {key}\")\n",
    "                continue\n",
    "            #model.learn(total_timesteps=1000)  # Minimal, adjust as needed\n",
    "            #mean_reward, std_reward = evaluate_agent(model, env, n_episodes=N_EVAL_EPISODES)\n",
    "            model.learn(total_timesteps=CONFIG[\"total_timesteps\"])  # Minimal, adjust as needed\n",
    "            metrics = evaluate_agent(model, env, n_episodes=N_EVAL_EPISODES)\n",
    "            result = {\"ticker\": ticker, \"agent\": agent_type, \"seed\": seed}\n",
    "            result.update(metrics)\n",
    "            results.append(result)\n",
    "            # Save after every new result\n",
    "            pd.DataFrame(results).to_csv(RESULTS_PATH, index=False)\n",
    "            # Log all metrics to experiment tracker\n",
    "            for k, v in metrics.items():\n",
    "                print(f\"{agent_type}_{k}\", v)\n",
    "            #results.append(result)\n",
    "            print(\"--------------------------------------\")\n",
    "            print('')\n",
    "\n",
    "# ========== SAVE RESULTS ==========\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(RESULTS_PATH, index=False)\n",
    "results_df.groupby(['agent'])[['mean_reward', 'std_reward']].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d01734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f886df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
