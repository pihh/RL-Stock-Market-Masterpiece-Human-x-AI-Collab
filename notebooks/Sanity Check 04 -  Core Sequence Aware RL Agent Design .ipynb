{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33beca1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "import jupyter\n",
    "import warnings\n",
    "\n",
    "from src.utils.system import boot, Notify\n",
    "\n",
    "boot()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# PACKAGES ================================\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "# FRAMEWORK STUFF =========================\n",
    "from src.defaults import TOP2_STOCK_BY_SECTOR, FEATURE_COLS,EPISODE_LENGTH\n",
    "from src.data.feature_pipeline import load_base_dataframe\n",
    "from src.experiments.experiment_tracker import ExperimentTracker\n",
    "from src.env.base_timeseries_trading_env import BaseSequenceAwareTradingEnv,SequenceAwareAlphaTradingEnv,SequenceAwareBaselineTradingAgent,SequenceAwareCalmarTradingEnv,SequenceAwareCumulativeTradingEnv,SequenceAwareDrawdownTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareHybridTradingEnv,SequenceAwareSharpeTradingEnv,SequenceAwareSortinoTradingEnv\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f477d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "EXPERIENCE_NAME = \"core_sequence_aware_agent_design\"\n",
    "RESULTS_PATH = f\"data/experiments/{EXPERIENCE_NAME}_barebones_results.csv\"\n",
    "N_EPISODES = 20\n",
    "N_SEEDS = 3\n",
    "N_EVAL_EPISODES = 3\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "WINDOW_LENGTH = 10  # or any value you want\n",
    "TOTAL_TIMESTEPS = EPISODE_LENGTH * 150\n",
    "N_STEPS = EPISODE_LENGTH * 2\n",
    "\n",
    "TRANSACTION_COST = 0\n",
    "\n",
    "CONFIG = {\n",
    "    \"batch_size\": EPISODE_LENGTH,\n",
    "    \"n_steps\": 128,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,   \n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "features_extractor_kwargs={\n",
    "    'window_length': WINDOW_LENGTH,\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'd_model': 32,\n",
    "    'nhead': ...,\n",
    "    'num_layers': ...,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Load data ---\n",
    "ohlcv_df = load_base_dataframe()\n",
    "\n",
    "# --- Experiment tracker ---\n",
    "experiment_tracker = ExperimentTracker(EXPERIENCE_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cbed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(df, ticker, feature_cols, episode_length, window_length):\n",
    "    df_ticker = df[df['symbol'] == ticker].copy()\n",
    "    return CumulativeTradingEnv(\n",
    "        df=df_ticker,\n",
    "        feature_cols=feature_cols,\n",
    "        episode_length=episode_length,\n",
    "        transaction_cost=TRANSACTION_COST,\n",
    "        window_length=window_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d1bb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, window_length, n_features, d_model=32, nhead=1, num_layers=1):\n",
    "        super().__init__(observation_space, features_dim=d_model)\n",
    "        self.window_length = window_length\n",
    "        self.n_features = n_features\n",
    "        self.embedding = nn.Linear(n_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: [batch, window_length * n_features]\n",
    "        batch = obs.shape[0]\n",
    "        # reshape flat vector to (batch, window_length, n_features)\n",
    "        x = obs.view(batch, self.window_length, self.n_features)\n",
    "        x = self.embedding(x)      # (batch, window_length, d_model)\n",
    "        x = x.permute(1, 0, 2)    # (window_length, batch, d_model)\n",
    "        x = self.transformer(x)    # (window_length, batch, d_model)\n",
    "        # Use last token as pooled output\n",
    "        return x[-1]              # (batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f0470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, nhead=1, num_layers=1, window_length=WINDOW_LENGTH, n_features=2, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            features_extractor_class=TransformerExtractor,\n",
    "            features_extractor_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': n_features,\n",
    "                'd_model': 32,\n",
    "                'nhead': nhead,\n",
    "                'num_layers': num_layers,\n",
    "            },\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893c093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D window shape: (10, 25)\n",
      "Flat window shape: (250,)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Output Shapes\n",
    "\n",
    "# Test windowed obs shape (flat vs. 2D)\n",
    "df = ohlcv_df.copy()\n",
    "feature_cols = FEATURE_COLS\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "print(\"2D window shape:\", obs.shape)  # Expect (5, obs_dim)\n",
    "\n",
    "env_flat = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs_flat, _ = env_flat.reset()\n",
    "print(\"Flat window shape:\", obs_flat.shape)  # Expect (5*obs_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26dab814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    obs,reward,done,_,info = env.step(1)\n",
    "    i+=1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f4be2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info[\"returns\"]),env.episode_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92aa112",
   "metadata": {},
   "source": [
    "# Unit tests:\n",
    "1. Output Shapes\n",
    "2. Window Consistency (Padding at Episode Start)\n",
    "3. Step Through Environment\n",
    "4. SB3 Policy Compatibility\n",
    "5. Transformer Policy Compatibility\n",
    "6. Action Space and Reward Consistency\n",
    "7. Episode Generator\n",
    "8. Is able to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d5ba3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D window shape: (10, 25)\n",
      "Flat window shape: (250,)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Output Shapes\n",
    "\n",
    "# Test windowed obs shape (flat vs. 2D)\n",
    "df = ohlcv_df.copy()\n",
    "feature_cols = FEATURE_COLS\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "print(\"2D window shape:\", obs.shape)  # Expect (5, obs_dim)\n",
    "\n",
    "env_flat = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs_flat, _ = env_flat.reset()\n",
    "print(\"Flat window shape:\", obs_flat.shape)  # Expect (5*obs_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b9230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding and shape OK\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Window consistency\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "assert np.allclose(obs[0], obs[1]), \"Padding at start should repeat first row\"\n",
    "assert obs.shape == (WINDOW_LENGTH, len(feature_cols) + len(env.internal_features))\n",
    "print(\"Padding and shape OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83f575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Obs shape: (10, 25) | Reward: -0.00000\n",
      "Step 1 | Obs shape: (10, 25) | Reward: -0.00168\n",
      "Step 2 | Obs shape: (10, 25) | Reward: -0.00290\n",
      "Step 3 | Obs shape: (10, 25) | Reward: -0.00425\n",
      "Step 4 | Obs shape: (10, 25) | Reward: 0.00380\n",
      "Step 5 | Obs shape: (10, 25) | Reward: 0.00045\n",
      "Step 6 | Obs shape: (10, 25) | Reward: -0.00605\n",
      "Step 7 | Obs shape: (10, 25) | Reward: 0.00463\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Step Through Environment\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=True\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "for i in range(8):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    print(f\"Step {i} | Obs shape: {obs.shape} | Reward: {reward:.5f}\")\n",
    "    if done:\n",
    "        print(\"Episode done:\", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad45531b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, n_steps\u001b[38;5;241m=\u001b[39mEPISODE_LENGTH, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mTOTAL_TIMESTEPS)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSB3 PPO MLP works!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    312\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    313\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    314\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    315\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    316\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    317\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    318\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:223\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines3\\common\\callbacks.py:134\u001b[0m, in \u001b[0;36mBaseCallback.update_locals\u001b[1;34m(self, locals_)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_locals\u001b[39m(\u001b[38;5;28mself\u001b[39m, locals_: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Update the references to the local variables.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    :param locals_: the local variables during rollout collection\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocals\u001b[38;5;241m.\u001b[39mupdate(locals_)\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_child_locals(locals_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SB3 Policy Compatibility\n",
    "# Train an MLP agent on env with return_sequences=False (flat). \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, n_steps=, batch_size=4, verbose=0)\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "print(\"SB3 PPO MLP works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Transformer Policy Compatibility\n",
    "# Make sure custom transformer can process the 2D obs by running a forward pass \n",
    "# through the extractor to check for shape errors\n",
    "\n",
    "\n",
    "obs = np.random.randn(2, 5*8).astype(np.float32)  # batch=2, window_length=WINDOW_LENGTH, n_features=8\n",
    "# Extractor expects (batch, window_length*n_features), will reshape internally.\n",
    "extractor = TransformerExtractor(\n",
    "    gym.spaces.Box(-np.inf, np.inf, shape=(5*8,), dtype=np.float32), 5, 8\n",
    ")\n",
    "with torch.no_grad():\n",
    "    torch_out = extractor(torch.from_numpy(obs))\n",
    "print(\"Transformer output shape:\", torch_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5205e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Action Space and Reward Consistency\n",
    "# mini-episode ti check action output and cumulative reward:\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, return_sequences=False\n",
    ")\n",
    "obs, _ = env.reset()\n",
    "cumulative = 0\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    cumulative += reward\n",
    "    if done:\n",
    "        print(\"Episode finished | Cumulative reward:\", cumulative)\n",
    "        print(\"Info dict:\", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Episode Generator\n",
    "# Check that the same seed produces the same episode list across runs.\n",
    "\n",
    "env = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH)\n",
    "seq1 = env.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "env2 = BaseSequenceAwareTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH)\n",
    "seq2 = env2.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "assert seq1 == seq2, \"Episode sequences should be the same for same seed!\"\n",
    "print(\"Episode generator determinism OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8: Learnability\n",
    "from src.env.realistic_synthetic_environment import realistic_synthetic_market_sample\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return self.env.action_space.sample(), {}\n",
    "\n",
    "class AlwaysLongAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def predict(self, obs, *args, **kwargs):\n",
    "        return 1, {}  # Always go long\n",
    "    \n",
    "def evaluate_baseline_agent(env, agent, n_episodes=20, episode_sequence=None):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = SequenceAwareCumulativeTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "env.set_episode_sequence(seq)\n",
    "\n",
    "\n",
    "# Evaluate PPO agent\n",
    "def evaluate_sb3_agent(env, model, n_episodes=10, episode_sequence=None):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "def make_agent(agent_type, env, window_length, feature_cols, **kwargs):\n",
    "    if agent_type == 'mlp':\n",
    "        return PPO(\"MlpPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type == 'lstm':\n",
    "        return RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "        #return PPO(\"MlpLstmPolicy\", env, verbose=0, **kwargs)\n",
    "    elif agent_type.startswith('transformer'):\n",
    "        \n",
    "        n_features = len(feature_cols)\n",
    "       \n",
    "        return PPO(\n",
    "            TransformerPolicy,\n",
    "            env,\n",
    "            verbose=0,\n",
    "            policy_kwargs={\n",
    "                'window_length': window_length,\n",
    "                'n_features': env.observation_space.shape[1],\n",
    "                'nhead': 2,        # set as desired\n",
    "                'num_layers': 2,   # set as desired\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_episodes=10, episode_sequence=None, is_sb3=False):\n",
    "    rewards = []\n",
    "    if episode_sequence:\n",
    "        env.set_episode_sequence(episode_sequence)\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            if is_sb3:\n",
    "                action, _ = agent.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28380f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT AND SEQUENCES ======================\n",
    "df = realistic_synthetic_market_sample(n=200)\n",
    "feature_cols = FEATURE_COLS\n",
    "env = SequenceAwareCumulativeTradingEnv(\n",
    "    df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "seq = env.generate_episode_sequences(train_steps=TOTAL_TIMESTEPS)\n",
    "\n",
    "# Baseline agents =================================\n",
    "random_agent = RandomAgent(env)\n",
    "always_long_agent = AlwaysLongAgent(env)\n",
    "mean_rand, std_rand = evaluate_agent(env, random_agent, n_episodes=20, episode_sequence=seq)\n",
    "mean_long, std_long = evaluate_agent(env, always_long_agent, n_episodes=20, episode_sequence=seq)\n",
    "print(f\"Random: mean {mean_rand:.4f}, std {std_rand:.4f}\")\n",
    "print(f\"Always Long: mean {mean_long:.4f}, std {std_long:.4f}\")\n",
    "\n",
    "# RL agents =======================================\n",
    "AGENT_TYPES = ['mlp', 'lstm', 'transformer_single', 'transformer_multi']\n",
    "#AGENT_TYPES = ['transformer_single', 'transformer_multi']\n",
    "for agent_type in AGENT_TYPES:\n",
    "    print(f\"\\nTraining {agent_type} agent...\")\n",
    "    env = SequenceAwareCumulativeTradingEnv(\n",
    "        df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "    env.set_episode_sequence(seq)\n",
    "    model = make_agent(agent_type, env, window_length=WINDOW_LENGTH, feature_cols=feature_cols, n_steps=EPISODE_LENGTH, batch_size=4)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "    mean_rl, std_rl = evaluate_agent(env, model, n_episodes=10, episode_sequence=seq, is_sb3=True)\n",
    "    print(f\"{agent_type} agent: mean {mean_rl:.4f}, std {std_rl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c47228",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "058f1476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training: 10 episodes, updating every 4 episodes.\n",
      "Episode 4: last episode reward 0.1170\n",
      "Episode 8: last episode reward 0.1489\n",
      "Episode 12: last episode reward 0.2073\n",
      "Eval: mean reward -0.0627, std 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.062725686, 7.450581e-09)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS TEST - Full Episode Learning \n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class EpisodicPPOTrainer:\n",
    "    def __init__(self, \n",
    "                 env_factory,           # callable, returns fresh env instance\n",
    "                 policy=\"MlpPolicy\",    # or custom policy class\n",
    "                 episode_length=100,\n",
    "                 episodes_per_update=4, # number of full episodes per PPO update\n",
    "                 total_episodes=1000,\n",
    "                 verbose=1,\n",
    "                 agent_kwargs=None):\n",
    "        self.env_factory = env_factory\n",
    "        self.episode_length = episode_length\n",
    "        self.episodes_per_update = episodes_per_update\n",
    "        self.total_episodes = total_episodes\n",
    "        self.verbose = verbose\n",
    "        self.policy = policy\n",
    "        #self.agent_kwargs = agent_kwargs or {}\n",
    "\n",
    "        # Build env and agent\n",
    "        self.env = DummyVecEnv([self.env_factory])\n",
    "        self.env = DummyVecEnv([self.env_factory])\n",
    "        steps_per_update = self.episode_length * self.episodes_per_update\n",
    "        self.agent = PPO(\n",
    "            policy, \n",
    "            self.env,\n",
    "            n_steps=steps_per_update,\n",
    "            batch_size=steps_per_update, # so update only at episode boundary\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def train(self, log_callback=None):\n",
    "        episode_count = 0\n",
    "        rewards_log = []\n",
    "        if self.verbose:\n",
    "            print(f\"Training: {self.total_episodes} episodes, updating every {self.episodes_per_update} episodes.\")\n",
    "        while episode_count < self.total_episodes:\n",
    "            all_obs, all_actions, all_rewards, all_dones, all_values, all_logprobs = [], [], [], [], [], []\n",
    "            for ep in range(self.episodes_per_update):\n",
    "                obs = self.env.reset()        # <-- FIX: initialize obs\n",
    "                done = False\n",
    "\n",
    "                ep_obs, ep_actions, ep_rewards, ep_dones, ep_values, ep_logprobs = [], [], [], [], [], []\n",
    "                while not done:\n",
    "                    # Vectorized env returns obs shape (1, obs_dim)\n",
    "                    obs_tensor = torch.from_numpy(obs).float()\n",
    "                    action, _ = self.agent.predict(obs, deterministic=False)\n",
    "                    action_tensor = torch.from_numpy(action).long()\n",
    "                    value = self.agent.policy.predict_values(torch.as_tensor(obs)).detach()\n",
    "                    logprob = self.agent.policy.evaluate_actions(\n",
    "                        torch.as_tensor(obs), torch.as_tensor(action)\n",
    "                    )[1].detach()\n",
    "                    next_obs, reward, done_arr, info = self.env.step(action)\n",
    "                    done = done_arr[0] if isinstance(done_arr, np.ndarray) else done_arr\n",
    "                    reward = reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "                    ep_obs.append(obs)\n",
    "                    ep_actions.append(action)\n",
    "                    ep_rewards.append(reward)\n",
    "                    ep_dones.append(done)\n",
    "                    ep_values.append(value)\n",
    "                    ep_logprobs.append(logprob)\n",
    "                    obs = next_obs\n",
    "                all_obs.extend(ep_obs)\n",
    "                all_actions.extend(ep_actions)\n",
    "                all_rewards.extend(ep_rewards)\n",
    "                all_dones.extend(ep_dones)\n",
    "                all_values.extend(ep_values)\n",
    "                all_logprobs.extend(ep_logprobs)\n",
    "                rewards_log.append(np.sum(ep_rewards))\n",
    "                episode_count += 1\n",
    "\n",
    "                if log_callback is not None:\n",
    "                    log_callback(episode_count, rewards_log)\n",
    "\n",
    "            # Fill rollout buffer with these episodes\n",
    "            self.agent.rollout_buffer.reset()\n",
    "          \n",
    "            for i in range(len(all_obs)):\n",
    "                self.agent.rollout_buffer.add(\n",
    "                    all_obs[i], all_actions[i], all_rewards[i], all_dones[i], all_values[i], all_logprobs[i]\n",
    "                )\n",
    "            # --- Fix logger bug (SB3 expects setup_learn called at least once) ---\n",
    "            if not hasattr(self.agent, \"_logger\"):\n",
    "                self.agent._setup_learn(1)\n",
    "            self.agent.train()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Episode {episode_count}: last episode reward {rewards_log[-1]:.4f}\")\n",
    "\n",
    "        return rewards_log\n",
    "\n",
    "    def evaluate(self, n_episodes=10):\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action, _ = self.agent.predict(obs, deterministic=True)\n",
    "                obs, reward, done,info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "            rewards.append(total_reward)\n",
    "        mean_r = np.mean(rewards)\n",
    "        std_r = np.std(rewards)\n",
    "        if self.verbose:\n",
    "            print(f\"Eval: mean reward {mean_r:.4f}, std {std_r:.4f}\")\n",
    "        return mean_r, std_r\n",
    "\n",
    "# ========== Usage Example ==========\n",
    "\n",
    "# Example env_factory for your env:\n",
    "class EnvFactory:\n",
    "    def __init__(self,\n",
    "                 env_class,\n",
    "                 df,\n",
    "                 feature_cols=None,\n",
    "                 internal_features=None,\n",
    "                 episode_length=100,\n",
    "                 transaction_cost=0.0001,\n",
    "                 seed=314, \n",
    "                 window_length=10,\n",
    "                 return_sequences=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.env_class = env_class\n",
    "        self.feature_cols = feature_cols\n",
    "        self.internal_features=internal_features\n",
    "        self.episode_length=episode_length\n",
    "        self.transaction_cost=transaction_cost\n",
    "        self.seed=seed\n",
    "        self.window_length=window_length\n",
    "        self.return_sequences=return_sequences\n",
    "    \n",
    "    def generate(self):\n",
    "        return self.env_class(self.df.copy(),\n",
    "                              feature_cols=self.feature_cols, \n",
    "                              internal_features=self.internal_features,\n",
    "                              episode_length=self.episode_length, \n",
    "                              window_length=self.window_length, \n",
    "                              transaction_cost=self.transaction_cost,\n",
    "                              seed=self.seed, \n",
    "                 \n",
    "                              return_sequences=self.return_sequences)\n",
    "\n",
    "SEED = 314\n",
    "env_factory = EnvFactory(\n",
    "    SequenceAwareCumulativeTradingEnv,\n",
    "    df, \n",
    "    feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=SEED,return_sequences=True)\n",
    "\n",
    "trainer = EpisodicPPOTrainer(\n",
    "    env_factory=env_factory.generate,\n",
    "    policy=\"MlpPolicy\",              # Can use custom\n",
    "    episode_length=100,\n",
    "    episodes_per_update=4,\n",
    "    total_episodes=10,\n",
    "    #agent_kwargs=dict(n_epochs=2, learning_rate=2e-4)  # as you wish\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SequenceAwareCumulativeTradingEnv(df, feature_cols=feature_cols, episode_length=EPISODE_LENGTH, window_length=WINDOW_LENGTH, seed=314)\n",
    "env.set_episode_sequence([[df['symbol'].unique()[0],0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ba644f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00]], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d50ae365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 5.4095829e-01, -4.6153846e-03,  4.0000000e+00,  1.0000000e+00,\n",
       "          1.4064915e-01, -6.2961789e-04,  1.7500000e+00, -4.0172415e+00,\n",
       "          4.1633849e-03,  1.4009227e-02,  1.4009227e-02,  3.1394106e-01,\n",
       "          3.1839257e-01,  1.8674700e-02,  2.4124569e-01,  1.4954310e-02,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  0.0000000e+00,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00],\n",
       "        [ 4.9372384e-01,  4.7758886e-01,  5.0000000e+00,  2.0000000e+00,\n",
       "          2.7196653e-02, -1.9392766e-02,  2.3599999e+00,  3.4857142e-01,\n",
       "         -1.7200245e-02, -4.1058646e-03, -4.1058646e-03,  1.3501452e-01,\n",
       "          4.7907948e-01,  1.6676523e-01,  1.6708440e-01, -2.4991194e-03,\n",
       "          1.0000000e+00,  1.0000000e+00, -4.1058646e-03,  9.9999998e-03,\n",
       "          4.1058646e-03,  1.5286901e-02, -4.1058646e-03,  1.8023000e+02,\n",
       "          0.0000000e+00]], dtype=float32),\n",
       " -0.004105864728402464,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197faf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
